{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-01T10:24:56.257526Z",
     "start_time": "2024-11-01T10:24:56.223997Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from backtesting import Backtest\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import os\n",
    "from datetime import datetime\n",
    "from lumibot.brokers import Alpaca\n",
    "import matplotlib.pyplot as plt\n",
    "from lumibot.backtesting import YahooDataBacktesting\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"../Models/best_model.pt\"\n",
    "\n",
    "# Delete current model\n",
    "if os.path.exists(file_path):\n",
    "    os.remove(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-01T10:17:07.914668Z",
     "start_time": "2024-11-01T10:17:07.913939Z"
    }
   },
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "device = \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-01T10:17:07.916204Z",
     "start_time": "2024-11-01T10:17:07.914992Z"
    }
   },
   "outputs": [],
   "source": [
    "# Model parameter\n",
    "input_size = 20\n",
    "output_size = 1\n",
    "hidden_size = 10\n",
    "num_layers = 4\n",
    "dropout = 0.2\n",
    "\n",
    "# Training parameter\n",
    "batch_size = 1\n",
    "num_epochs = 10\n",
    "learning_rate = 0.0001\n",
    "seq_size = 30\n",
    "\n",
    "train_data_path = \"../Data/train_dax_data.pkl\"\n",
    "test_data_path = \"../Data/test_dax_data.pkl\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-11-01T10:17:07.916045Z"
    }
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_size, num_layers, dropout=0.2):\n",
    "        super(Net, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, _ = self.lstm(x)\n",
    "        x = self.fc(x[:, -1, :]) \n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FinanceDataset(Dataset):\n",
    "    def __init__(self, input, output, seq_size):\n",
    "        self.seq_size = seq_size\n",
    "        \n",
    "        self.inputs = input\n",
    "        self.labels = output\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.inputs) - self.seq_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.inputs[idx:idx + self.seq_size]\n",
    "        y = self.labels[idx + self.seq_size] \n",
    "\n",
    "        # Convert to tensors\n",
    "        x = torch.tensor(x, dtype=torch.float32)\n",
    "        y = torch.tensor(y, dtype=torch.float32)\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-11-01T10:17:07.918078Z"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize model, loss function, optimizer\n",
    "net = Net(input_size, output_size, hidden_size, num_layers)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-01T10:17:07.919235Z",
     "start_time": "2024-11-01T10:17:07.919001Z"
    }
   },
   "outputs": [],
   "source": [
    "# Train\n",
    "df = pd.read_pickle(train_data_path)\n",
    "\n",
    "df = df[['^GDAXI_Open', '^GDAXI_High', '^GDAXI_Low', '^GDAXI_Close',\n",
    "                                '^GDAXI_Adj Close', '^GDAXI_Volume', '^GDAXI_month', '^GDAXI_weekday',\n",
    "                                'GC=F_Open', 'GC=F_High', 'GC=F_Low', 'GC=F_Close',\n",
    "                                'GC=F_Adj Close', 'GC=F_Volume',\n",
    "                                'BZ=F_Open', 'BZ=F_High', 'BZ=F_Low', 'BZ=F_Close',\n",
    "                                'BZ=F_Adj Close', 'BZ=F_Volume',\n",
    "                                'Y']]\n",
    "\n",
    "display(df)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaler_y = MinMaxScaler()\n",
    "\n",
    "scaled_train_inputs = scaler.fit_transform(df.iloc[:, :-1].values)  \n",
    "scaled_train_labels = scaler_y.fit_transform(df.iloc[:, -1].values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "test_df = pd.read_pickle(test_data_path)\n",
    "\n",
    "test_df = test_df[['^GDAXI_Open', '^GDAXI_High', '^GDAXI_Low', '^GDAXI_Close',\n",
    "                                '^GDAXI_Adj Close', '^GDAXI_Volume', '^GDAXI_month', '^GDAXI_weekday',\n",
    "                                'GC=F_Open', 'GC=F_High', 'GC=F_Low', 'GC=F_Close',\n",
    "                                'GC=F_Adj Close', 'GC=F_Volume',\n",
    "                                'BZ=F_Open', 'BZ=F_High', 'BZ=F_Low', 'BZ=F_Close',\n",
    "                                'BZ=F_Adj Close', 'BZ=F_Volume',\n",
    "                                'Y']]\n",
    "\n",
    "\n",
    "display(test_df)\n",
    "\n",
    "scaled_test_inputs = scaler.transform(test_df.iloc[:,:-1].values) \n",
    "scaled_test_labels = scaler_y.transform(test_df.iloc[:, -1].values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize dataset and dataloader\n",
    "dataset = FinanceDataset(scaled_train_inputs, scaled_train_labels, seq_size=seq_size)\n",
    "train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_dataset = FinanceDataset(scaled_test_inputs, scaled_test_labels, seq_size=seq_size)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-01T10:17:07.922014Z",
     "start_time": "2024-11-01T10:17:07.920173Z"
    }
   },
   "outputs": [],
   "source": [
    "os.makedirs(\"../Models\", exist_ok=True)\n",
    "\n",
    "# Modellpfad festlegen\n",
    "best_model_path = \"../Models/best_model.pt\"\n",
    "\n",
    "# Modell laden, falls es bereits existiert\n",
    "if os.path.exists(best_model_path):\n",
    "    net.load_state_dict(torch.load(best_model_path))\n",
    "    print(f\"Modell erfolgreich geladen von {best_model_path}\")\n",
    "\n",
    "# Parameter für Early Stopping und Modell-Speicherung\n",
    "patience = 8\n",
    "best_test_loss = float('inf')\n",
    "early_stopping_counter = 0\n",
    "\n",
    "losses = []\n",
    "test_loss_vals = []\n",
    "\n",
    "# Training loop mit Early Stopping\n",
    "for epoch in range(num_epochs):\n",
    "    net.train()\n",
    "    epoch_loss = 0\n",
    "    epoch_output = []\n",
    "    # Training\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        epoch_output.append(outputs.detach().numpy())\n",
    "        loss = criterion(outputs.squeeze(-1), labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    epoch_loss /= len(train_loader)\n",
    "    avg_train_loss = epoch_loss / len(train_loader)\n",
    "    losses.append(epoch_loss)\n",
    "\n",
    "    # Modell auswerten auf dem Testset\n",
    "    net.eval()\n",
    "    running_test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            test_outputs = net(inputs)\n",
    "            test_loss = criterion(test_outputs.squeeze(-1), labels)\n",
    "            running_test_loss += test_loss.item()\n",
    "\n",
    "        avg_test_loss = running_test_loss / len(test_loader)\n",
    "        test_loss_vals.append(avg_test_loss)\n",
    "\n",
    "        # Check for Early Stopping und Speichern des besten Modells\n",
    "        if avg_test_loss < best_test_loss:\n",
    "            best_epoch = epoch\n",
    "            best_test_loss = avg_test_loss\n",
    "            torch.save(net.state_dict(), best_model_path)\n",
    "            print(f\"Bestes Modell gespeichert mit Test Loss: {best_test_loss:.4f}\")\n",
    "            early_stopping_counter = 0\n",
    "        else:\n",
    "            early_stopping_counter += 1\n",
    "\n",
    "        # Ausgabe der Verluste\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Train Loss: {avg_train_loss:.4f}, Test Loss: {avg_test_loss:.4f}\")\n",
    "\n",
    "        # Early Stopping-Kriterium prüfen\n",
    "        if early_stopping_counter >= patience:\n",
    "            print(f\"Early stopping nach {epoch + 1} Epochen. Test loss verbesserte sich nicht in den letzten {patience} Epochen.\")\n",
    "            break\n",
    "    # print(f\"Std Epoch Out: {np.std(epoch_output)}\")\n",
    "    # print(f\"Mean Epoch Out: {np.mean(epoch_output)}\")\n",
    "\n",
    "# Plot der Trainings- und Testverluste\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(losses, label='Training Loss')\n",
    "plt.plot(test_loss_vals, label='Test Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Training und Test Loss')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Training abgeschlossen. Bestes Modell gespeichert unter: {best_model_path}\")\n",
    "print(f\"Std Training Loss: {np.std(losses)}\")\n",
    "print(f\"Std Test Loss: {np.std(test_loss_vals)}\")\n",
    "print(f\"Min Training Loss: {np.min(losses)}\")\n",
    "print(f\"Min Test Loss: {np.min(test_loss_vals)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backtesting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "scaler_y = MinMaxScaler()\n",
    "\n",
    "df = pd.read_pickle(train_data_path)\n",
    "df = df[['^GDAXI_Open', '^GDAXI_High', '^GDAXI_Low', '^GDAXI_Close',\n",
    "                                '^GDAXI_Adj Close', '^GDAXI_Volume', '^GDAXI_month', '^GDAXI_weekday',\n",
    "                                'GC=F_Open', 'GC=F_High', 'GC=F_Low', 'GC=F_Close',\n",
    "                                'GC=F_Adj Close', 'GC=F_Volume',\n",
    "                                'BZ=F_Open', 'BZ=F_High', 'BZ=F_Low', 'BZ=F_Close',\n",
    "                                'BZ=F_Adj Close', 'BZ=F_Volume',\n",
    "                                'Y']]\n",
    "\n",
    "display(df)\n",
    "\n",
    "scaled_train_inputs = scaler.fit(df.iloc[:, :-1].values)  \n",
    "scaled_train_labels = scaler_y.fit(df.iloc[:, -1].values.reshape(-1, 1))\n",
    "\n",
    "model_path = \"../Models/best_model.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-11-01T10:17:07.921380Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "\n",
    "model = Net(input_size, output_size, hidden_size, num_layers)\n",
    "\n",
    "# Load state_dict only\n",
    "model.load_state_dict(torch.load(model_path)) \n",
    "model.eval()\n",
    "\n",
    "df = pd.read_pickle(test_data_path)\n",
    "df = df[['^GDAXI_Open', '^GDAXI_High', '^GDAXI_Low', '^GDAXI_Close',\n",
    "                                '^GDAXI_Adj Close', '^GDAXI_Volume', '^GDAXI_month', '^GDAXI_weekday',\n",
    "                                'GC=F_Open', 'GC=F_High', 'GC=F_Low', 'GC=F_Close',\n",
    "                                'GC=F_Adj Close', 'GC=F_Volume',\n",
    "                                'BZ=F_Open', 'BZ=F_High', 'BZ=F_Low', 'BZ=F_Close',\n",
    "                                'BZ=F_Adj Close', 'BZ=F_Volume',\n",
    "                                'Y']]\n",
    "display(df)\n",
    "scaled_test_inputs = scaler.transform(df.iloc[:, :-1].values)  \n",
    "scaled_test_labels = scaler_y.transform(df.iloc[:, -1].values.reshape(-1, 1))\n",
    "\n",
    "test_data = FinanceDataset(scaled_test_inputs, scaled_test_labels, seq_size=seq_size)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        out = model(inputs) \n",
    "        \n",
    "        all_predictions.append(out.numpy())  \n",
    "        all_labels.append(labels.numpy())\n",
    "\n",
    "all_predictions = np.concatenate(all_predictions)\n",
    "all_labels = np.concatenate(all_labels)\n",
    "\n",
    "print(f'Predicted values: {all_predictions.flatten()}')\n",
    "print(f'Actual values: {all_labels.flatten()}')\n",
    "\n",
    "output_df = pd.DataFrame({'Predicted': all_predictions.flatten(), 'Actual': all_labels.flatten()})\n",
    "display(output_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = scaler_y.inverse_transform(all_predictions).reshape(-1, 1)\n",
    "actual = scaler_y.inverse_transform(all_labels.reshape(-1, 1)).reshape(-1, 1)\n",
    "\n",
    "# DataFrame erstellen\n",
    "df_results = pd.DataFrame({\n",
    "    \"pred\": pred.flatten(),\n",
    "    \"actual\": actual.flatten(),\n",
    "})\n",
    "\n",
    "# DataFrame anzeigen\n",
    "print(df_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"logs\", exist_ok=True)\n",
    "os.makedirs(\"results\", exist_ok=True)\n",
    "\n",
    "test_data = pd.read_pickle(test_data_path)\n",
    "train_data = pd.read_pickle(train_data_path)\n",
    "\n",
    "test_data = test_data[['Date','^GDAXI_Open', '^GDAXI_High', '^GDAXI_Low', '^GDAXI_Close',\n",
    "                                '^GDAXI_Adj Close', '^GDAXI_Volume', '^GDAXI_month', '^GDAXI_weekday',\n",
    "                                'GC=F_Open', 'GC=F_High', 'GC=F_Low', 'GC=F_Close',\n",
    "                                'GC=F_Adj Close', 'GC=F_Volume',\n",
    "                                'BZ=F_Open', 'BZ=F_High', 'BZ=F_Low', 'BZ=F_Close',\n",
    "                                'BZ=F_Adj Close', 'BZ=F_Volume',\n",
    "                                'Y']]\n",
    "train_data = train_data[['^GDAXI_Open', '^GDAXI_High', '^GDAXI_Low', '^GDAXI_Close',\n",
    "                                '^GDAXI_Adj Close', '^GDAXI_Volume', '^GDAXI_month', '^GDAXI_weekday',\n",
    "                                'GC=F_Open', 'GC=F_High', 'GC=F_Low', 'GC=F_Close',\n",
    "                                'GC=F_Adj Close', 'GC=F_Volume',\n",
    "                                'BZ=F_Open', 'BZ=F_High', 'BZ=F_Low', 'BZ=F_Close',\n",
    "                                'BZ=F_Adj Close', 'BZ=F_Volume',\n",
    "                                'Y']]\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(train_data.iloc[:, :-1].values)\n",
    "\n",
    "scaler_y = MinMaxScaler()\n",
    "scaler_y.fit(train_data.iloc[:, -1].values.reshape(-1, 1))\n",
    "\n",
    "model = Net(input_size, output_size, hidden_size, num_layers)\n",
    "model_path = \"../Models/best_model.pt\"\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "model.eval()\n",
    "\n",
    "ALPACA_CREDS = {\n",
    "    \"API_KEY\": os.getenv(\"ALPACA_API_KEY\"),\n",
    "    \"API_SECRET\": os.getenv(\"ALPACA_API_SECRET\"),\n",
    "    \"PAPER\": True,\n",
    "}\n",
    "\n",
    "# Strategy setup\n",
    "start_date = datetime(2023, 1, 1)\n",
    "end_date = datetime(2023, 12, 31)\n",
    "broker = Alpaca(ALPACA_CREDS)\n",
    "\n",
    "strategy = Backtest(\n",
    "    name=\"Test4-Raw Material\",\n",
    "    broker=broker,\n",
    "    parameters={\n",
    "        \"symbol\": \"^GDAXI\",\n",
    "        \"cash_at_risk\": 0.5,\n",
    "        \"model\": model,\n",
    "        \"num_prior_days\": 30,\n",
    "        \"dataset\": test_data,\n",
    "        \"scaler\": scaler,\n",
    "        \"scaler_y\": scaler_y,\n",
    "    },\n",
    ")\n",
    "\n",
    "# Run backtest\n",
    "backtest_results = strategy.backtest(\n",
    "    YahooDataBacktesting,\n",
    "    start_date,\n",
    "    end_date,\n",
    "    name=\"Test4-Raw Material\",\n",
    "    parameters={\n",
    "        \"symbol\": \"^GDAXI\",\n",
    "        \"cash_at_risk\": 0.5,\n",
    "        \"model\": model,\n",
    "        \"dataset\": test_data,\n",
    "        \"num_prior_days\": 30,\n",
    "        \"scaler\": scaler,\n",
    "        \"scaler_y\": scaler_y,\n",
    "    },\n",
    "    benchmark_asset=\"^GDAXI\",\n",
    "    show_plot=True,\n",
    "    show_tearsheet=True,\n",
    ")\n",
    "\n",
    "# Save results\n",
    "pd.DataFrame(backtest_results).to_csv(\"results/backtest_results.csv.gz\", index=False, compression=\"gzip\")\n",
    "\n",
    "print(\"Backtesting complete. Results saved to backtest_results.csv.gz.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backtesting-Funktion\n",
    "def backtest_model(model, dataloader, scaler_y):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in dataloader:\n",
    "            # Vorhersage\n",
    "            output = model(X_batch)\n",
    "            predictions.extend(output.numpy())\n",
    "            actuals.extend(y_batch.numpy())\n",
    "\n",
    "    # Rücktransformation der Vorhersagen und tatsächlichen Werte\n",
    "    predictions = scaler_y.inverse_transform(np.array(predictions).reshape(-1, 1))\n",
    "    actuals = scaler_y.inverse_transform(np.array(actuals).reshape(-1, 1))\n",
    "\n",
    "    return predictions.flatten(), actuals.flatten()\n",
    "\n",
    "# Backtesting starten\n",
    "def run_backtest(test_df, model_path, seq_size):\n",
    "    # Daten vorverarbeiten\n",
    "    scaler_X = MinMaxScaler()\n",
    "    scaler_y = MinMaxScaler()\n",
    "    train_data = pd.read_pickle(train_data_path)\n",
    "    train_data = train_data[['^GDAXI_Open', '^GDAXI_High', '^GDAXI_Low', '^GDAXI_Close',\n",
    "                                '^GDAXI_Adj Close', '^GDAXI_Volume', '^GDAXI_month', '^GDAXI_weekday',\n",
    "                                'GC=F_Open', 'GC=F_High', 'GC=F_Low', 'GC=F_Close',\n",
    "                                'GC=F_Adj Close', 'GC=F_Volume',\n",
    "                                'BZ=F_Open', 'BZ=F_High', 'BZ=F_Low', 'BZ=F_Close',\n",
    "                                'BZ=F_Adj Close', 'BZ=F_Volume',\n",
    "                                'Y']]    \n",
    "    scaler_X.fit(train_data.iloc[:, 1:-1].values)\n",
    "    scaler_y.fit(train_data.iloc[:, -1].values.reshape(-1, 1))\n",
    "\n",
    "    X_test = test_df.iloc[:, 1:-1]\n",
    "    y_test = test_df.iloc[:, -1]\n",
    "\n",
    "    X_test_scaled = scaler_X.transform(X_test.values)\n",
    "    y_test_scaled = scaler_y.transform(y_test.values.reshape(-1, 1))\n",
    "\n",
    "    # Sequenzen erstellen\n",
    "    test_dataset = FinanceDataset(X_test_scaled, y_test_scaled, seq_size)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "    # Modell laden\n",
    "    model = Net(input_size=input_size, output_size=output_size, hidden_size=hidden_size, num_layers=num_layers)\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.eval()\n",
    "\n",
    "    # Backtesting durchführen\n",
    "    predictions, actuals = backtest_model(model, test_dataloader, scaler_y)\n",
    "\n",
    "    # Ergebnisse visualisieren\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    plt.plot(predictions, label=\"Predicted\", color=\"blue\")\n",
    "    plt.plot(actuals, label=\"Actual\", color=\"orange\")\n",
    "    plt.title(\"Backtesting Results\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Statistiken berechnen\n",
    "    df_results = pd.DataFrame({\"Actual\": actuals, \"Predicted\": predictions})\n",
    "    mse = ((df_results[\"Actual\"] - df_results[\"Predicted\"]) ** 2).mean()\n",
    "    mae = np.abs(df_results[\"Actual\"] - df_results[\"Predicted\"]).mean()\n",
    "    print(f\"Mean Squared Error (MSE): {mse:.2f}\")\n",
    "    print(f\"Mean Absolute Error (MAE): {mae:.2f}\")\n",
    "\n",
    "    return df_results\n",
    "\n",
    "# Anwendung der Backtesting-Funktion\n",
    "test_data = pd.read_pickle(test_data_path)  # Testdatensatz laden\n",
    "test_data = test_data[['^GDAXI_Open', '^GDAXI_High', '^GDAXI_Low', '^GDAXI_Close',\n",
    "                                '^GDAXI_Adj Close', '^GDAXI_Volume', '^GDAXI_month', '^GDAXI_weekday',\n",
    "                                'GC=F_Open', 'GC=F_High', 'GC=F_Low', 'GC=F_Close',\n",
    "                                'GC=F_Adj Close', 'GC=F_Volume',\n",
    "                                'BZ=F_Open', 'BZ=F_High', 'BZ=F_Low', 'BZ=F_Close',\n",
    "                                'BZ=F_Adj Close', 'BZ=F_Volume',\n",
    "                                'Y']]\n",
    "model_path = \"../Models/best_model.pt\"  # Pfad zum gespeicherten Modell\n",
    "\n",
    "results = run_backtest(test_data, model_path, seq_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
