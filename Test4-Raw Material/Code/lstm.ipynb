{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-01T10:24:56.257526Z",
     "start_time": "2024-11-01T10:24:56.223997Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danielstorch/Desktop/HTW/6 Semester/AKI-Trading-Model/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/Users/danielstorch/Desktop/HTW/6 Semester/AKI-Trading-Model/.venv/lib/python3.11/site-packages/backtesting/_plotting.py:50: UserWarning: Jupyter Notebook detected. Setting Bokeh output to notebook. This may not work in Jupyter clients without JavaScript support (e.g. PyCharm, Spyder IDE). Reset with `backtesting.set_bokeh_output(notebook=False)`.\n",
      "  warnings.warn('Jupyter Notebook detected. '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "    <style>\n",
       "        .bk-notebook-logo {\n",
       "            display: block;\n",
       "            width: 20px;\n",
       "            height: 20px;\n",
       "            background-image: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAUCAYAAACNiR0NAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAABx0RVh0U29mdHdhcmUAQWRvYmUgRmlyZXdvcmtzIENTNui8sowAAAOkSURBVDiNjZRtaJVlGMd/1/08zzln5zjP1LWcU9N0NkN8m2CYjpgQYQXqSs0I84OLIC0hkEKoPtiH3gmKoiJDU7QpLgoLjLIQCpEsNJ1vqUOdO7ppbuec5+V+rj4ctwzd8IIbbi6u+8f1539dt3A78eXC7QizUF7gyV1fD1Yqg4JWz84yffhm0qkFqBogB9rM8tZdtwVsPUhWhGcFJngGeWrPzHm5oaMmkfEg1usvLFyc8jLRqDOMru7AyC8saQr7GG7f5fvDeH7Ej8CM66nIF+8yngt6HWaKh7k49Soy9nXurCi1o3qUbS3zWfrYeQDTB/Qj6kX6Ybhw4B+bOYoLKCC9H3Nu/leUTZ1JdRWkkn2ldcCamzrcf47KKXdAJllSlxAOkRgyHsGC/zRday5Qld9DyoM4/q/rUoy/CXh3jzOu3bHUVZeU+DEn8FInkPBFlu3+nW3Nw0mk6vCDiWg8CeJaxEwuHS3+z5RgY+YBR6V1Z1nxSOfoaPa4LASWxxdNp+VWTk7+4vzaou8v8PN+xo+KY2xsw6une2frhw05CTYOmQvsEhjhWjn0bmXPjpE1+kplmmkP3suftwTubK9Vq22qKmrBhpY4jvd5afdRA3wGjFAgcnTK2s4hY0/GPNIb0nErGMCRxWOOX64Z8RAC4oCXdklmEvcL8o0BfkNK4lUg9HTl+oPlQxdNo3Mg4Nv175e/1LDGzZen30MEjRUtmXSfiTVu1kK8W4txyV6BMKlbgk3lMwYCiusNy9fVfvvwMxv8Ynl6vxoByANLTWplvuj/nF9m2+PDtt1eiHPBr1oIfhCChQMBw6Aw0UulqTKZdfVvfG7VcfIqLG9bcldL/+pdWTLxLUy8Qq38heUIjh4XlzZxzQm19lLFlr8vdQ97rjZVOLf8nclzckbcD4wxXMidpX30sFd37Fv/GtwwhzhxGVAprjbg0gCAEeIgwCZyTV2Z1REEW8O4py0wsjeloKoMr6iCY6dP92H6Vw/oTyICIthibxjm/DfN9lVz8IqtqKYLUXfoKVMVQVVJOElGjrnnUt9T9wbgp8AyYKaGlqingHZU/uG2NTZSVqwHQTWkx9hxjkpWDaCg6Ckj5qebgBVbT3V3NNXMSiWSDdGV3hrtzla7J+duwPOToIg42ChPQOQjspnSlp1V+Gjdged7+8UN5CRAV7a5EdFNwCjEaBR27b3W890TE7g24NAP/mMDXRWrGoFPQI9ls/MWO2dWFAar/xcOIImbbpA3zgAAAABJRU5ErkJggg==);\n",
       "        }\n",
       "    </style>\n",
       "    <div>\n",
       "        <a href=\"https://bokeh.org\" target=\"_blank\" class=\"bk-notebook-logo\"></a>\n",
       "        <span id=\"af33f8be-a455-4d66-aa53-2d8af83bbe6e\">Loading BokehJS ...</span>\n",
       "    </div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": "'use strict';\n(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  const force = true;\n\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\nconst JS_MIME_TYPE = 'application/javascript';\n  const HTML_MIME_TYPE = 'text/html';\n  const EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n  const CLASS_NAME = 'output_bokeh rendered_html';\n\n  /**\n   * Render data to the DOM node\n   */\n  function render(props, node) {\n    const script = document.createElement(\"script\");\n    node.appendChild(script);\n  }\n\n  /**\n   * Handle when an output is cleared or removed\n   */\n  function handleClearOutput(event, handle) {\n    function drop(id) {\n      const view = Bokeh.index.get_by_id(id)\n      if (view != null) {\n        view.model.document.clear()\n        Bokeh.index.delete(view)\n      }\n    }\n\n    const cell = handle.cell;\n\n    const id = cell.output_area._bokeh_element_id;\n    const server_id = cell.output_area._bokeh_server_id;\n\n    // Clean up Bokeh references\n    if (id != null) {\n      drop(id)\n    }\n\n    if (server_id !== undefined) {\n      // Clean up Bokeh references\n      const cmd_clean = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n      cell.notebook.kernel.execute(cmd_clean, {\n        iopub: {\n          output: function(msg) {\n            const id = msg.content.text.trim()\n            drop(id)\n          }\n        }\n      });\n      // Destroy server and session\n      const cmd_destroy = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n      cell.notebook.kernel.execute(cmd_destroy);\n    }\n  }\n\n  /**\n   * Handle when a new output is added\n   */\n  function handleAddOutput(event, handle) {\n    const output_area = handle.output_area;\n    const output = handle.output;\n\n    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n    if ((output.output_type != \"display_data\") || (!Object.prototype.hasOwnProperty.call(output.data, EXEC_MIME_TYPE))) {\n      return\n    }\n\n    const toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n\n    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\n      // store reference to embed id on output_area\n      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n    }\n    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n      const bk_div = document.createElement(\"div\");\n      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n      const script_attrs = bk_div.children[0].attributes;\n      for (let i = 0; i < script_attrs.length; i++) {\n        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n        toinsert[toinsert.length - 1].firstChild.textContent = bk_div.children[0].textContent\n      }\n      // store reference to server id on output_area\n      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n    }\n  }\n\n  function register_renderer(events, OutputArea) {\n\n    function append_mime(data, metadata, element) {\n      // create a DOM node to render to\n      const toinsert = this.create_output_subarea(\n        metadata,\n        CLASS_NAME,\n        EXEC_MIME_TYPE\n      );\n      this.keyboard_manager.register_events(toinsert);\n      // Render to node\n      const props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n      render(props, toinsert[toinsert.length - 1]);\n      element.append(toinsert);\n      return toinsert\n    }\n\n    /* Handle when an output is cleared or removed */\n    events.on('clear_output.CodeCell', handleClearOutput);\n    events.on('delete.Cell', handleClearOutput);\n\n    /* Handle when a new output is added */\n    events.on('output_added.OutputArea', handleAddOutput);\n\n    /**\n     * Register the mime type and append_mime function with output_area\n     */\n    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n      /* Is output safe? */\n      safe: true,\n      /* Index of renderer in `output_area.display_order` */\n      index: 0\n    });\n  }\n\n  // register the mime type if in Jupyter Notebook environment and previously unregistered\n  if (root.Jupyter !== undefined) {\n    const events = require('base/js/events');\n    const OutputArea = require('notebook/js/outputarea').OutputArea;\n\n    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n      register_renderer(events, OutputArea);\n    }\n  }\n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  const NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded(error = null) {\n    const el = document.getElementById(\"af33f8be-a455-4d66-aa53-2d8af83bbe6e\");\n    if (el != null) {\n      const html = (() => {\n        if (typeof root.Bokeh === \"undefined\") {\n          if (error == null) {\n            return \"BokehJS is loading ...\";\n          } else {\n            return \"BokehJS failed to load.\";\n          }\n        } else {\n          const prefix = `BokehJS ${root.Bokeh.version}`;\n          if (error == null) {\n            return `${prefix} successfully loaded.`;\n          } else {\n            return `${prefix} <b>encountered errors</b> while loading and may not function as expected.`;\n          }\n        }\n      })();\n      el.innerHTML = html;\n\n      if (error != null) {\n        const wrapper = document.createElement(\"div\");\n        wrapper.style.overflow = \"auto\";\n        wrapper.style.height = \"5em\";\n        wrapper.style.resize = \"vertical\";\n        const content = document.createElement(\"div\");\n        content.style.fontFamily = \"monospace\";\n        content.style.whiteSpace = \"pre-wrap\";\n        content.style.backgroundColor = \"rgb(255, 221, 221)\";\n        content.textContent = error.stack ?? error.toString();\n        wrapper.append(content);\n        el.append(wrapper);\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(() => display_loaded(error), 100);\n    }\n  }\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = css_urls.length + js_urls.length;\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n\n    function on_error(url) {\n      console.error(\"failed to load \" + url);\n    }\n\n    for (let i = 0; i < css_urls.length; i++) {\n      const url = css_urls[i];\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }\n\n    for (let i = 0; i < js_urls.length; i++) {\n      const url = js_urls[i];\n      const element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.async = false;\n      element.src = url;\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n  };\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  const js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-3.6.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-3.6.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-3.6.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-3.6.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-mathjax-3.6.1.min.js\"];\n  const css_urls = [];\n\n  const inline_js = [    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\nfunction(Bokeh) {\n    }\n  ];\n\n  function run_inline_js() {\n    if (root.Bokeh !== undefined || force === true) {\n      try {\n            for (let i = 0; i < inline_js.length; i++) {\n      inline_js[i].call(root, root.Bokeh);\n    }\n\n      } catch (error) {display_loaded(error);throw error;\n      }if (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      const cell = $(document.getElementById(\"af33f8be-a455-4d66-aa53-2d8af83bbe6e\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(css_urls, js_urls, function() {\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));",
      "application/vnd.bokehjs_load.v0+json": ""
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from backtesting import Backtest\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import os\n",
    "from datetime import datetime\n",
    "from lumibot.brokers import Alpaca\n",
    "import matplotlib.pyplot as plt\n",
    "from lumibot.backtesting import YahooDataBacktesting\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"../Models/best_model.pt\"\n",
    "\n",
    "# Delete current model\n",
    "if os.path.exists(file_path):\n",
    "    os.remove(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-01T10:17:07.914668Z",
     "start_time": "2024-11-01T10:17:07.913939Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "device = \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-01T10:17:07.916204Z",
     "start_time": "2024-11-01T10:17:07.914992Z"
    }
   },
   "outputs": [],
   "source": [
    "# Model parameter\n",
    "input_size = 20\n",
    "output_size = 1\n",
    "hidden_size = 10\n",
    "num_layers = 4\n",
    "dropout = 0.2\n",
    "\n",
    "# Training parameter\n",
    "batch_size = 1\n",
    "num_epochs = 10\n",
    "learning_rate = 0.0001\n",
    "seq_size = 30\n",
    "\n",
    "train_data_path = \"../Data/train_dax_data.pkl\"\n",
    "test_data_path = \"../Data/test_dax_data.pkl\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-11-01T10:17:07.916045Z"
    }
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_size, num_layers, dropout=0.2):\n",
    "        super(Net, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, _ = self.lstm(x)\n",
    "        x = self.fc(x[:, -1, :]) \n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FinanceDataset(Dataset):\n",
    "    def __init__(self, input, output, seq_size):\n",
    "        self.seq_size = seq_size\n",
    "        \n",
    "        self.inputs = input\n",
    "        self.labels = output\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.inputs) - self.seq_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.inputs[idx:idx + self.seq_size]\n",
    "        y = self.labels[idx + self.seq_size] \n",
    "\n",
    "        # Convert to tensors\n",
    "        x = torch.tensor(x, dtype=torch.float32)\n",
    "        y = torch.tensor(y, dtype=torch.float32)\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-11-01T10:17:07.918078Z"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize model, loss function, optimizer\n",
    "net = Net(input_size, output_size, hidden_size, num_layers)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-01T10:17:07.919235Z",
     "start_time": "2024-11-01T10:17:07.919001Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>^GDAXI_Open</th>\n",
       "      <th>^GDAXI_High</th>\n",
       "      <th>^GDAXI_Low</th>\n",
       "      <th>^GDAXI_Close</th>\n",
       "      <th>^GDAXI_Adj Close</th>\n",
       "      <th>^GDAXI_Volume</th>\n",
       "      <th>^GDAXI_month</th>\n",
       "      <th>^GDAXI_weekday</th>\n",
       "      <th>GC=F_Open</th>\n",
       "      <th>GC=F_High</th>\n",
       "      <th>...</th>\n",
       "      <th>GC=F_Close</th>\n",
       "      <th>GC=F_Adj Close</th>\n",
       "      <th>GC=F_Volume</th>\n",
       "      <th>BZ=F_Open</th>\n",
       "      <th>BZ=F_High</th>\n",
       "      <th>BZ=F_Low</th>\n",
       "      <th>BZ=F_Close</th>\n",
       "      <th>BZ=F_Adj Close</th>\n",
       "      <th>BZ=F_Volume</th>\n",
       "      <th>Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9869.129883</td>\n",
       "      <td>9879.530273</td>\n",
       "      <td>9687.259766</td>\n",
       "      <td>9764.730469</td>\n",
       "      <td>9764.730469</td>\n",
       "      <td>67673900</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1184.000000</td>\n",
       "      <td>1194.500000</td>\n",
       "      <td>...</td>\n",
       "      <td>1186.000000</td>\n",
       "      <td>1186.000000</td>\n",
       "      <td>138</td>\n",
       "      <td>57.630001</td>\n",
       "      <td>58.220001</td>\n",
       "      <td>55.520000</td>\n",
       "      <td>56.419998</td>\n",
       "      <td>56.419998</td>\n",
       "      <td>16707</td>\n",
       "      <td>9473.160156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9735.650391</td>\n",
       "      <td>9790.269531</td>\n",
       "      <td>9468.580078</td>\n",
       "      <td>9473.160156</td>\n",
       "      <td>9473.160156</td>\n",
       "      <td>105538300</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1180.300049</td>\n",
       "      <td>1206.900024</td>\n",
       "      <td>...</td>\n",
       "      <td>1203.900024</td>\n",
       "      <td>1203.900024</td>\n",
       "      <td>470</td>\n",
       "      <td>56.290001</td>\n",
       "      <td>56.290001</td>\n",
       "      <td>52.669998</td>\n",
       "      <td>53.110001</td>\n",
       "      <td>53.110001</td>\n",
       "      <td>30065</td>\n",
       "      <td>9469.660156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9484.250000</td>\n",
       "      <td>9624.650391</td>\n",
       "      <td>9382.820312</td>\n",
       "      <td>9469.660156</td>\n",
       "      <td>9469.660156</td>\n",
       "      <td>96812300</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1203.500000</td>\n",
       "      <td>1220.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1219.300049</td>\n",
       "      <td>1219.300049</td>\n",
       "      <td>97</td>\n",
       "      <td>53.230000</td>\n",
       "      <td>53.520000</td>\n",
       "      <td>50.529999</td>\n",
       "      <td>51.099998</td>\n",
       "      <td>51.099998</td>\n",
       "      <td>35494</td>\n",
       "      <td>9518.179688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9510.339844</td>\n",
       "      <td>9592.370117</td>\n",
       "      <td>9459.179688</td>\n",
       "      <td>9518.179688</td>\n",
       "      <td>9518.179688</td>\n",
       "      <td>82466600</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1219.199951</td>\n",
       "      <td>1219.199951</td>\n",
       "      <td>...</td>\n",
       "      <td>1210.599976</td>\n",
       "      <td>1210.599976</td>\n",
       "      <td>29</td>\n",
       "      <td>51.060001</td>\n",
       "      <td>51.840000</td>\n",
       "      <td>49.680000</td>\n",
       "      <td>51.150002</td>\n",
       "      <td>51.150002</td>\n",
       "      <td>37082</td>\n",
       "      <td>9837.610352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9643.769531</td>\n",
       "      <td>9855.429688</td>\n",
       "      <td>9607.900391</td>\n",
       "      <td>9837.610352</td>\n",
       "      <td>9837.610352</td>\n",
       "      <td>114825000</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1207.000000</td>\n",
       "      <td>1215.699951</td>\n",
       "      <td>...</td>\n",
       "      <td>1208.400024</td>\n",
       "      <td>1208.400024</td>\n",
       "      <td>92</td>\n",
       "      <td>51.000000</td>\n",
       "      <td>51.889999</td>\n",
       "      <td>49.820000</td>\n",
       "      <td>50.959999</td>\n",
       "      <td>50.959999</td>\n",
       "      <td>29469</td>\n",
       "      <td>9648.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1969</th>\n",
       "      <td>14113.009766</td>\n",
       "      <td>14160.870117</td>\n",
       "      <td>13890.540039</td>\n",
       "      <td>13914.070312</td>\n",
       "      <td>13914.070312</td>\n",
       "      <td>42893400</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>1818.099976</td>\n",
       "      <td>1818.099976</td>\n",
       "      <td>...</td>\n",
       "      <td>1787.000000</td>\n",
       "      <td>1787.000000</td>\n",
       "      <td>411</td>\n",
       "      <td>82.370003</td>\n",
       "      <td>83.860001</td>\n",
       "      <td>80.680000</td>\n",
       "      <td>80.980003</td>\n",
       "      <td>80.980003</td>\n",
       "      <td>16647</td>\n",
       "      <td>13940.929688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1970</th>\n",
       "      <td>13945.589844</td>\n",
       "      <td>14000.679688</td>\n",
       "      <td>13874.500000</td>\n",
       "      <td>13940.929688</td>\n",
       "      <td>13940.929688</td>\n",
       "      <td>28738700</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>1794.300049</td>\n",
       "      <td>1802.800049</td>\n",
       "      <td>...</td>\n",
       "      <td>1795.900024</td>\n",
       "      <td>1795.900024</td>\n",
       "      <td>49</td>\n",
       "      <td>81.730003</td>\n",
       "      <td>84.370003</td>\n",
       "      <td>81.339996</td>\n",
       "      <td>83.919998</td>\n",
       "      <td>83.919998</td>\n",
       "      <td>8621</td>\n",
       "      <td>13995.099609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1971</th>\n",
       "      <td>14047.419922</td>\n",
       "      <td>14063.139648</td>\n",
       "      <td>13966.349609</td>\n",
       "      <td>13995.099609</td>\n",
       "      <td>13995.099609</td>\n",
       "      <td>22975000</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>1803.400024</td>\n",
       "      <td>1826.300049</td>\n",
       "      <td>...</td>\n",
       "      <td>1814.800049</td>\n",
       "      <td>1814.800049</td>\n",
       "      <td>69</td>\n",
       "      <td>84.459999</td>\n",
       "      <td>85.669998</td>\n",
       "      <td>83.660004</td>\n",
       "      <td>84.330002</td>\n",
       "      <td>84.330002</td>\n",
       "      <td>7512</td>\n",
       "      <td>13925.599609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1972</th>\n",
       "      <td>14013.719727</td>\n",
       "      <td>14018.469727</td>\n",
       "      <td>13914.620117</td>\n",
       "      <td>13925.599609</td>\n",
       "      <td>13925.599609</td>\n",
       "      <td>27583800</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>1803.199951</td>\n",
       "      <td>1807.900024</td>\n",
       "      <td>...</td>\n",
       "      <td>1807.900024</td>\n",
       "      <td>1807.900024</td>\n",
       "      <td>434</td>\n",
       "      <td>84.599998</td>\n",
       "      <td>84.639999</td>\n",
       "      <td>81.949997</td>\n",
       "      <td>83.260002</td>\n",
       "      <td>83.260002</td>\n",
       "      <td>5384</td>\n",
       "      <td>14071.719727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1973</th>\n",
       "      <td>13890.809570</td>\n",
       "      <td>14071.719727</td>\n",
       "      <td>13871.320312</td>\n",
       "      <td>14071.719727</td>\n",
       "      <td>14071.719727</td>\n",
       "      <td>30727400</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>1805.800049</td>\n",
       "      <td>1819.500000</td>\n",
       "      <td>...</td>\n",
       "      <td>1819.500000</td>\n",
       "      <td>1819.500000</td>\n",
       "      <td>277</td>\n",
       "      <td>82.860001</td>\n",
       "      <td>82.910004</td>\n",
       "      <td>81.300003</td>\n",
       "      <td>82.260002</td>\n",
       "      <td>82.260002</td>\n",
       "      <td>20599</td>\n",
       "      <td>13923.589844</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1974 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       ^GDAXI_Open   ^GDAXI_High    ^GDAXI_Low  ^GDAXI_Close  \\\n",
       "0      9869.129883   9879.530273   9687.259766   9764.730469   \n",
       "1      9735.650391   9790.269531   9468.580078   9473.160156   \n",
       "2      9484.250000   9624.650391   9382.820312   9469.660156   \n",
       "3      9510.339844   9592.370117   9459.179688   9518.179688   \n",
       "4      9643.769531   9855.429688   9607.900391   9837.610352   \n",
       "...            ...           ...           ...           ...   \n",
       "1969  14113.009766  14160.870117  13890.540039  13914.070312   \n",
       "1970  13945.589844  14000.679688  13874.500000  13940.929688   \n",
       "1971  14047.419922  14063.139648  13966.349609  13995.099609   \n",
       "1972  14013.719727  14018.469727  13914.620117  13925.599609   \n",
       "1973  13890.809570  14071.719727  13871.320312  14071.719727   \n",
       "\n",
       "      ^GDAXI_Adj Close  ^GDAXI_Volume  ^GDAXI_month  ^GDAXI_weekday  \\\n",
       "0          9764.730469       67673900             1               4   \n",
       "1          9473.160156      105538300             1               0   \n",
       "2          9469.660156       96812300             1               1   \n",
       "3          9518.179688       82466600             1               2   \n",
       "4          9837.610352      114825000             1               3   \n",
       "...                ...            ...           ...             ...   \n",
       "1969      13914.070312       42893400            12               3   \n",
       "1970      13940.929688       28738700            12               4   \n",
       "1971      13995.099609       22975000            12               1   \n",
       "1972      13925.599609       27583800            12               2   \n",
       "1973      14071.719727       30727400            12               3   \n",
       "\n",
       "        GC=F_Open    GC=F_High  ...   GC=F_Close  GC=F_Adj Close  GC=F_Volume  \\\n",
       "0     1184.000000  1194.500000  ...  1186.000000     1186.000000          138   \n",
       "1     1180.300049  1206.900024  ...  1203.900024     1203.900024          470   \n",
       "2     1203.500000  1220.000000  ...  1219.300049     1219.300049           97   \n",
       "3     1219.199951  1219.199951  ...  1210.599976     1210.599976           29   \n",
       "4     1207.000000  1215.699951  ...  1208.400024     1208.400024           92   \n",
       "...           ...          ...  ...          ...             ...          ...   \n",
       "1969  1818.099976  1818.099976  ...  1787.000000     1787.000000          411   \n",
       "1970  1794.300049  1802.800049  ...  1795.900024     1795.900024           49   \n",
       "1971  1803.400024  1826.300049  ...  1814.800049     1814.800049           69   \n",
       "1972  1803.199951  1807.900024  ...  1807.900024     1807.900024          434   \n",
       "1973  1805.800049  1819.500000  ...  1819.500000     1819.500000          277   \n",
       "\n",
       "      BZ=F_Open  BZ=F_High   BZ=F_Low  BZ=F_Close  BZ=F_Adj Close  \\\n",
       "0     57.630001  58.220001  55.520000   56.419998       56.419998   \n",
       "1     56.290001  56.290001  52.669998   53.110001       53.110001   \n",
       "2     53.230000  53.520000  50.529999   51.099998       51.099998   \n",
       "3     51.060001  51.840000  49.680000   51.150002       51.150002   \n",
       "4     51.000000  51.889999  49.820000   50.959999       50.959999   \n",
       "...         ...        ...        ...         ...             ...   \n",
       "1969  82.370003  83.860001  80.680000   80.980003       80.980003   \n",
       "1970  81.730003  84.370003  81.339996   83.919998       83.919998   \n",
       "1971  84.459999  85.669998  83.660004   84.330002       84.330002   \n",
       "1972  84.599998  84.639999  81.949997   83.260002       83.260002   \n",
       "1973  82.860001  82.910004  81.300003   82.260002       82.260002   \n",
       "\n",
       "      BZ=F_Volume             Y  \n",
       "0           16707   9473.160156  \n",
       "1           30065   9469.660156  \n",
       "2           35494   9518.179688  \n",
       "3           37082   9837.610352  \n",
       "4           29469        9648.5  \n",
       "...           ...           ...  \n",
       "1969        16647  13940.929688  \n",
       "1970         8621  13995.099609  \n",
       "1971         7512  13925.599609  \n",
       "1972         5384  14071.719727  \n",
       "1973        20599  13923.589844  \n",
       "\n",
       "[1974 rows x 21 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train\n",
    "df = pd.read_pickle(train_data_path)\n",
    "\n",
    "df = df[['^GDAXI_Open', '^GDAXI_High', '^GDAXI_Low', '^GDAXI_Close',\n",
    "                                '^GDAXI_Adj Close', '^GDAXI_Volume', '^GDAXI_month', '^GDAXI_weekday',\n",
    "                                'GC=F_Open', 'GC=F_High', 'GC=F_Low', 'GC=F_Close',\n",
    "                                'GC=F_Adj Close', 'GC=F_Volume',\n",
    "                                'BZ=F_Open', 'BZ=F_High', 'BZ=F_Low', 'BZ=F_Close',\n",
    "                                'BZ=F_Adj Close', 'BZ=F_Volume',\n",
    "                                'Y']]\n",
    "\n",
    "display(df)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaler_y = MinMaxScaler()\n",
    "\n",
    "scaled_train_inputs = scaler.fit_transform(df.iloc[:, :-1].values)  \n",
    "scaled_train_labels = scaler_y.fit_transform(df.iloc[:, -1].values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>^GDAXI_Open</th>\n",
       "      <th>^GDAXI_High</th>\n",
       "      <th>^GDAXI_Low</th>\n",
       "      <th>^GDAXI_Close</th>\n",
       "      <th>^GDAXI_Adj Close</th>\n",
       "      <th>^GDAXI_Volume</th>\n",
       "      <th>^GDAXI_month</th>\n",
       "      <th>^GDAXI_weekday</th>\n",
       "      <th>GC=F_Open</th>\n",
       "      <th>GC=F_High</th>\n",
       "      <th>...</th>\n",
       "      <th>GC=F_Close</th>\n",
       "      <th>GC=F_Adj Close</th>\n",
       "      <th>GC=F_Volume</th>\n",
       "      <th>BZ=F_Open</th>\n",
       "      <th>BZ=F_High</th>\n",
       "      <th>BZ=F_Low</th>\n",
       "      <th>BZ=F_Close</th>\n",
       "      <th>BZ=F_Adj Close</th>\n",
       "      <th>BZ=F_Volume</th>\n",
       "      <th>Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11951.839844</td>\n",
       "      <td>12227.870117</td>\n",
       "      <td>11893.940430</td>\n",
       "      <td>12209.480469</td>\n",
       "      <td>12209.480469</td>\n",
       "      <td>63674200</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>1667.199951</td>\n",
       "      <td>1700.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1692.900024</td>\n",
       "      <td>1692.900024</td>\n",
       "      <td>410</td>\n",
       "      <td>86.510002</td>\n",
       "      <td>89.830002</td>\n",
       "      <td>86.510002</td>\n",
       "      <td>88.860001</td>\n",
       "      <td>88.860001</td>\n",
       "      <td>38251</td>\n",
       "      <td>12670.480469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12360.719727</td>\n",
       "      <td>12673.349609</td>\n",
       "      <td>12358.980469</td>\n",
       "      <td>12670.480469</td>\n",
       "      <td>12670.480469</td>\n",
       "      <td>80240300</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>1701.199951</td>\n",
       "      <td>1728.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1721.099976</td>\n",
       "      <td>1721.099976</td>\n",
       "      <td>291</td>\n",
       "      <td>88.889999</td>\n",
       "      <td>92.389999</td>\n",
       "      <td>88.690002</td>\n",
       "      <td>91.800003</td>\n",
       "      <td>91.800003</td>\n",
       "      <td>38971</td>\n",
       "      <td>12517.179688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12611.070312</td>\n",
       "      <td>12661.879883</td>\n",
       "      <td>12455.360352</td>\n",
       "      <td>12517.179688</td>\n",
       "      <td>12517.179688</td>\n",
       "      <td>62392400</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>1724.099976</td>\n",
       "      <td>1726.599976</td>\n",
       "      <td>...</td>\n",
       "      <td>1711.400024</td>\n",
       "      <td>1711.400024</td>\n",
       "      <td>418</td>\n",
       "      <td>91.650002</td>\n",
       "      <td>93.970001</td>\n",
       "      <td>90.900002</td>\n",
       "      <td>93.370003</td>\n",
       "      <td>93.370003</td>\n",
       "      <td>39563</td>\n",
       "      <td>12470.780273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12593.410156</td>\n",
       "      <td>12643.219727</td>\n",
       "      <td>12421.889648</td>\n",
       "      <td>12470.780273</td>\n",
       "      <td>12470.780273</td>\n",
       "      <td>53529600</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>1721.000000</td>\n",
       "      <td>1723.300049</td>\n",
       "      <td>...</td>\n",
       "      <td>1711.699951</td>\n",
       "      <td>1711.699951</td>\n",
       "      <td>133</td>\n",
       "      <td>93.830002</td>\n",
       "      <td>95.010002</td>\n",
       "      <td>92.730003</td>\n",
       "      <td>94.419998</td>\n",
       "      <td>94.419998</td>\n",
       "      <td>26200</td>\n",
       "      <td>12273.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12414.780273</td>\n",
       "      <td>12497.519531</td>\n",
       "      <td>12272.589844</td>\n",
       "      <td>12273.000000</td>\n",
       "      <td>12273.000000</td>\n",
       "      <td>56973200</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>1710.099976</td>\n",
       "      <td>1710.099976</td>\n",
       "      <td>...</td>\n",
       "      <td>1700.500000</td>\n",
       "      <td>1700.500000</td>\n",
       "      <td>179</td>\n",
       "      <td>94.989998</td>\n",
       "      <td>98.589996</td>\n",
       "      <td>93.930000</td>\n",
       "      <td>97.919998</td>\n",
       "      <td>97.919998</td>\n",
       "      <td>36871</td>\n",
       "      <td>12272.94043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>304</th>\n",
       "      <td>16779.410156</td>\n",
       "      <td>16789.960938</td>\n",
       "      <td>16694.560547</td>\n",
       "      <td>16733.050781</td>\n",
       "      <td>16733.050781</td>\n",
       "      <td>63366400</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>2037.099976</td>\n",
       "      <td>2037.099976</td>\n",
       "      <td>...</td>\n",
       "      <td>2034.500000</td>\n",
       "      <td>2034.500000</td>\n",
       "      <td>258</td>\n",
       "      <td>79.250000</td>\n",
       "      <td>80.610001</td>\n",
       "      <td>79.080002</td>\n",
       "      <td>79.699997</td>\n",
       "      <td>79.699997</td>\n",
       "      <td>26913</td>\n",
       "      <td>16687.419922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>305</th>\n",
       "      <td>16667.310547</td>\n",
       "      <td>16708.349609</td>\n",
       "      <td>16624.160156</td>\n",
       "      <td>16687.419922</td>\n",
       "      <td>16687.419922</td>\n",
       "      <td>57871300</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>2035.800049</td>\n",
       "      <td>2044.500000</td>\n",
       "      <td>...</td>\n",
       "      <td>2039.099976</td>\n",
       "      <td>2039.099976</td>\n",
       "      <td>228</td>\n",
       "      <td>79.139999</td>\n",
       "      <td>80.120003</td>\n",
       "      <td>77.849998</td>\n",
       "      <td>79.389999</td>\n",
       "      <td>79.389999</td>\n",
       "      <td>22237</td>\n",
       "      <td>16706.179688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306</th>\n",
       "      <td>16673.300781</td>\n",
       "      <td>16735.320312</td>\n",
       "      <td>16651.779297</td>\n",
       "      <td>16706.179688</td>\n",
       "      <td>16706.179688</td>\n",
       "      <td>46295300</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>2055.699951</td>\n",
       "      <td>2068.699951</td>\n",
       "      <td>...</td>\n",
       "      <td>2057.100098</td>\n",
       "      <td>2057.100098</td>\n",
       "      <td>202</td>\n",
       "      <td>79.440002</td>\n",
       "      <td>80.370003</td>\n",
       "      <td>78.830002</td>\n",
       "      <td>79.070000</td>\n",
       "      <td>79.070000</td>\n",
       "      <td>12334</td>\n",
       "      <td>16742.070312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307</th>\n",
       "      <td>16727.769531</td>\n",
       "      <td>16775.710938</td>\n",
       "      <td>16697.580078</td>\n",
       "      <td>16742.070312</td>\n",
       "      <td>16742.070312</td>\n",
       "      <td>37678900</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>2067.300049</td>\n",
       "      <td>2081.899902</td>\n",
       "      <td>...</td>\n",
       "      <td>2081.899902</td>\n",
       "      <td>2081.899902</td>\n",
       "      <td>586</td>\n",
       "      <td>80.739998</td>\n",
       "      <td>81.320000</td>\n",
       "      <td>79.489998</td>\n",
       "      <td>79.650002</td>\n",
       "      <td>79.650002</td>\n",
       "      <td>8282</td>\n",
       "      <td>16701.550781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>308</th>\n",
       "      <td>16780.949219</td>\n",
       "      <td>16783.789062</td>\n",
       "      <td>16688.519531</td>\n",
       "      <td>16701.550781</td>\n",
       "      <td>16701.550781</td>\n",
       "      <td>36091600</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>2081.600098</td>\n",
       "      <td>2087.300049</td>\n",
       "      <td>...</td>\n",
       "      <td>2073.899902</td>\n",
       "      <td>2073.899902</td>\n",
       "      <td>338</td>\n",
       "      <td>79.839996</td>\n",
       "      <td>79.959999</td>\n",
       "      <td>78.339996</td>\n",
       "      <td>78.389999</td>\n",
       "      <td>78.389999</td>\n",
       "      <td>24301</td>\n",
       "      <td>16751.640625</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>309 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      ^GDAXI_Open   ^GDAXI_High    ^GDAXI_Low  ^GDAXI_Close  ^GDAXI_Adj Close  \\\n",
       "0    11951.839844  12227.870117  11893.940430  12209.480469      12209.480469   \n",
       "1    12360.719727  12673.349609  12358.980469  12670.480469      12670.480469   \n",
       "2    12611.070312  12661.879883  12455.360352  12517.179688      12517.179688   \n",
       "3    12593.410156  12643.219727  12421.889648  12470.780273      12470.780273   \n",
       "4    12414.780273  12497.519531  12272.589844  12273.000000      12273.000000   \n",
       "..            ...           ...           ...           ...               ...   \n",
       "304  16779.410156  16789.960938  16694.560547  16733.050781      16733.050781   \n",
       "305  16667.310547  16708.349609  16624.160156  16687.419922      16687.419922   \n",
       "306  16673.300781  16735.320312  16651.779297  16706.179688      16706.179688   \n",
       "307  16727.769531  16775.710938  16697.580078  16742.070312      16742.070312   \n",
       "308  16780.949219  16783.789062  16688.519531  16701.550781      16701.550781   \n",
       "\n",
       "     ^GDAXI_Volume  ^GDAXI_month  ^GDAXI_weekday    GC=F_Open    GC=F_High  \\\n",
       "0         63674200            10               0  1667.199951  1700.000000   \n",
       "1         80240300            10               1  1701.199951  1728.000000   \n",
       "2         62392400            10               2  1724.099976  1726.599976   \n",
       "3         53529600            10               3  1721.000000  1723.300049   \n",
       "4         56973200            10               4  1710.099976  1710.099976   \n",
       "..             ...           ...             ...          ...          ...   \n",
       "304       63366400            12               2  2037.099976  2037.099976   \n",
       "305       57871300            12               3  2035.800049  2044.500000   \n",
       "306       46295300            12               4  2055.699951  2068.699951   \n",
       "307       37678900            12               2  2067.300049  2081.899902   \n",
       "308       36091600            12               3  2081.600098  2087.300049   \n",
       "\n",
       "     ...   GC=F_Close  GC=F_Adj Close  GC=F_Volume  BZ=F_Open  BZ=F_High  \\\n",
       "0    ...  1692.900024     1692.900024          410  86.510002  89.830002   \n",
       "1    ...  1721.099976     1721.099976          291  88.889999  92.389999   \n",
       "2    ...  1711.400024     1711.400024          418  91.650002  93.970001   \n",
       "3    ...  1711.699951     1711.699951          133  93.830002  95.010002   \n",
       "4    ...  1700.500000     1700.500000          179  94.989998  98.589996   \n",
       "..   ...          ...             ...          ...        ...        ...   \n",
       "304  ...  2034.500000     2034.500000          258  79.250000  80.610001   \n",
       "305  ...  2039.099976     2039.099976          228  79.139999  80.120003   \n",
       "306  ...  2057.100098     2057.100098          202  79.440002  80.370003   \n",
       "307  ...  2081.899902     2081.899902          586  80.739998  81.320000   \n",
       "308  ...  2073.899902     2073.899902          338  79.839996  79.959999   \n",
       "\n",
       "      BZ=F_Low  BZ=F_Close  BZ=F_Adj Close  BZ=F_Volume             Y  \n",
       "0    86.510002   88.860001       88.860001        38251  12670.480469  \n",
       "1    88.690002   91.800003       91.800003        38971  12517.179688  \n",
       "2    90.900002   93.370003       93.370003        39563  12470.780273  \n",
       "3    92.730003   94.419998       94.419998        26200       12273.0  \n",
       "4    93.930000   97.919998       97.919998        36871   12272.94043  \n",
       "..         ...         ...             ...          ...           ...  \n",
       "304  79.080002   79.699997       79.699997        26913  16687.419922  \n",
       "305  77.849998   79.389999       79.389999        22237  16706.179688  \n",
       "306  78.830002   79.070000       79.070000        12334  16742.070312  \n",
       "307  79.489998   79.650002       79.650002         8282  16701.550781  \n",
       "308  78.339996   78.389999       78.389999        24301  16751.640625  \n",
       "\n",
       "[309 rows x 21 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test\n",
    "test_df = pd.read_pickle(test_data_path)\n",
    "\n",
    "test_df = test_df[['^GDAXI_Open', '^GDAXI_High', '^GDAXI_Low', '^GDAXI_Close',\n",
    "                                '^GDAXI_Adj Close', '^GDAXI_Volume', '^GDAXI_month', '^GDAXI_weekday',\n",
    "                                'GC=F_Open', 'GC=F_High', 'GC=F_Low', 'GC=F_Close',\n",
    "                                'GC=F_Adj Close', 'GC=F_Volume',\n",
    "                                'BZ=F_Open', 'BZ=F_High', 'BZ=F_Low', 'BZ=F_Close',\n",
    "                                'BZ=F_Adj Close', 'BZ=F_Volume',\n",
    "                                'Y']]\n",
    "\n",
    "\n",
    "display(test_df)\n",
    "\n",
    "scaled_test_inputs = scaler.transform(test_df.iloc[:,:-1].values) \n",
    "scaled_test_labels = scaler_y.transform(test_df.iloc[:, -1].values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize dataset and dataloader\n",
    "dataset = FinanceDataset(scaled_train_inputs, scaled_train_labels, seq_size=seq_size)\n",
    "train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_dataset = FinanceDataset(scaled_test_inputs, scaled_test_labels, seq_size=seq_size)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-01T10:17:07.922014Z",
     "start_time": "2024-11-01T10:17:07.920173Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danielstorch/Desktop/HTW/6 Semester/AKI-Trading-Model/.venv/lib/python3.11/site-packages/torch/nn/modules/loss.py:608: UserWarning: Using a target size (torch.Size([1, 1])) that is different to the input size (torch.Size([1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bestes Modell gespeichert mit Test Loss: 0.0157\n",
      "Epoch 1/10, Train Loss: 0.0000, Test Loss: 0.0157\n",
      "Bestes Modell gespeichert mit Test Loss: 0.0044\n",
      "Epoch 2/10, Train Loss: 0.0000, Test Loss: 0.0044\n",
      "Epoch 3/10, Train Loss: 0.0000, Test Loss: 0.0051\n",
      "Bestes Modell gespeichert mit Test Loss: 0.0038\n",
      "Epoch 4/10, Train Loss: 0.0000, Test Loss: 0.0038\n",
      "Bestes Modell gespeichert mit Test Loss: 0.0037\n",
      "Epoch 5/10, Train Loss: 0.0000, Test Loss: 0.0037\n",
      "Bestes Modell gespeichert mit Test Loss: 0.0037\n",
      "Epoch 6/10, Train Loss: 0.0000, Test Loss: 0.0037\n",
      "Epoch 7/10, Train Loss: 0.0000, Test Loss: 0.0045\n",
      "Bestes Modell gespeichert mit Test Loss: 0.0034\n",
      "Epoch 8/10, Train Loss: 0.0000, Test Loss: 0.0034\n",
      "Epoch 9/10, Train Loss: 0.0000, Test Loss: 0.0038\n",
      "Epoch 10/10, Train Loss: 0.0000, Test Loss: 0.0038\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training abgeschlossen. Bestes Modell gespeichert unter: ../Models/best_model.pt\n",
      "Std Training Loss: 0.007347293812843081\n",
      "Std Test Loss: 0.00355377580693575\n",
      "Min Training Loss: 0.0033919817421289946\n",
      "Min Test Loss: 0.003371461274213227\n"
     ]
    }
   ],
   "source": [
    "os.makedirs(\"../Models\", exist_ok=True)\n",
    "\n",
    "# Modellpfad festlegen\n",
    "best_model_path = \"../Models/best_model.pt\"\n",
    "\n",
    "# Modell laden, falls es bereits existiert\n",
    "if os.path.exists(best_model_path):\n",
    "    net.load_state_dict(torch.load(best_model_path))\n",
    "    print(f\"Modell erfolgreich geladen von {best_model_path}\")\n",
    "\n",
    "# Parameter für Early Stopping und Modell-Speicherung\n",
    "patience = 8\n",
    "best_test_loss = float('inf')\n",
    "early_stopping_counter = 0\n",
    "\n",
    "losses = []\n",
    "test_loss_vals = []\n",
    "\n",
    "# Training loop mit Early Stopping\n",
    "for epoch in range(num_epochs):\n",
    "    net.train()\n",
    "    epoch_loss = 0\n",
    "    epoch_output = []\n",
    "    # Training\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        epoch_output.append(outputs.detach().numpy())\n",
    "        loss = criterion(outputs.squeeze(-1), labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    epoch_loss /= len(train_loader)\n",
    "    avg_train_loss = epoch_loss / len(train_loader)\n",
    "    losses.append(epoch_loss)\n",
    "\n",
    "    # Modell auswerten auf dem Testset\n",
    "    net.eval()\n",
    "    running_test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            test_outputs = net(inputs)\n",
    "            test_loss = criterion(test_outputs.squeeze(-1), labels)\n",
    "            running_test_loss += test_loss.item()\n",
    "\n",
    "        avg_test_loss = running_test_loss / len(test_loader)\n",
    "        test_loss_vals.append(avg_test_loss)\n",
    "\n",
    "        # Check for Early Stopping und Speichern des besten Modells\n",
    "        if avg_test_loss < best_test_loss:\n",
    "            best_epoch = epoch\n",
    "            best_test_loss = avg_test_loss\n",
    "            torch.save(net.state_dict(), best_model_path)\n",
    "            print(f\"Bestes Modell gespeichert mit Test Loss: {best_test_loss:.4f}\")\n",
    "            early_stopping_counter = 0\n",
    "        else:\n",
    "            early_stopping_counter += 1\n",
    "\n",
    "        # Ausgabe der Verluste\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Train Loss: {avg_train_loss:.4f}, Test Loss: {avg_test_loss:.4f}\")\n",
    "\n",
    "        # Early Stopping-Kriterium prüfen\n",
    "        if early_stopping_counter >= patience:\n",
    "            print(f\"Early stopping nach {epoch + 1} Epochen. Test loss verbesserte sich nicht in den letzten {patience} Epochen.\")\n",
    "            break\n",
    "    # print(f\"Std Epoch Out: {np.std(epoch_output)}\")\n",
    "    # print(f\"Mean Epoch Out: {np.mean(epoch_output)}\")\n",
    "\n",
    "# Plot der Trainings- und Testverluste\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(losses, label='Training Loss')\n",
    "plt.plot(test_loss_vals, label='Test Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Training und Test Loss')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Training abgeschlossen. Bestes Modell gespeichert unter: {best_model_path}\")\n",
    "print(f\"Std Training Loss: {np.std(losses)}\")\n",
    "print(f\"Std Test Loss: {np.std(test_loss_vals)}\")\n",
    "print(f\"Min Training Loss: {np.min(losses)}\")\n",
    "print(f\"Min Test Loss: {np.min(test_loss_vals)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backtesting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>^GDAXI_Open</th>\n",
       "      <th>^GDAXI_High</th>\n",
       "      <th>^GDAXI_Low</th>\n",
       "      <th>^GDAXI_Close</th>\n",
       "      <th>^GDAXI_Adj Close</th>\n",
       "      <th>^GDAXI_Volume</th>\n",
       "      <th>^GDAXI_month</th>\n",
       "      <th>^GDAXI_weekday</th>\n",
       "      <th>GC=F_Open</th>\n",
       "      <th>GC=F_High</th>\n",
       "      <th>...</th>\n",
       "      <th>GC=F_Close</th>\n",
       "      <th>GC=F_Adj Close</th>\n",
       "      <th>GC=F_Volume</th>\n",
       "      <th>BZ=F_Open</th>\n",
       "      <th>BZ=F_High</th>\n",
       "      <th>BZ=F_Low</th>\n",
       "      <th>BZ=F_Close</th>\n",
       "      <th>BZ=F_Adj Close</th>\n",
       "      <th>BZ=F_Volume</th>\n",
       "      <th>Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9869.129883</td>\n",
       "      <td>9879.530273</td>\n",
       "      <td>9687.259766</td>\n",
       "      <td>9764.730469</td>\n",
       "      <td>9764.730469</td>\n",
       "      <td>67673900</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1184.000000</td>\n",
       "      <td>1194.500000</td>\n",
       "      <td>...</td>\n",
       "      <td>1186.000000</td>\n",
       "      <td>1186.000000</td>\n",
       "      <td>138</td>\n",
       "      <td>57.630001</td>\n",
       "      <td>58.220001</td>\n",
       "      <td>55.520000</td>\n",
       "      <td>56.419998</td>\n",
       "      <td>56.419998</td>\n",
       "      <td>16707</td>\n",
       "      <td>9473.160156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9735.650391</td>\n",
       "      <td>9790.269531</td>\n",
       "      <td>9468.580078</td>\n",
       "      <td>9473.160156</td>\n",
       "      <td>9473.160156</td>\n",
       "      <td>105538300</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1180.300049</td>\n",
       "      <td>1206.900024</td>\n",
       "      <td>...</td>\n",
       "      <td>1203.900024</td>\n",
       "      <td>1203.900024</td>\n",
       "      <td>470</td>\n",
       "      <td>56.290001</td>\n",
       "      <td>56.290001</td>\n",
       "      <td>52.669998</td>\n",
       "      <td>53.110001</td>\n",
       "      <td>53.110001</td>\n",
       "      <td>30065</td>\n",
       "      <td>9469.660156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9484.250000</td>\n",
       "      <td>9624.650391</td>\n",
       "      <td>9382.820312</td>\n",
       "      <td>9469.660156</td>\n",
       "      <td>9469.660156</td>\n",
       "      <td>96812300</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1203.500000</td>\n",
       "      <td>1220.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1219.300049</td>\n",
       "      <td>1219.300049</td>\n",
       "      <td>97</td>\n",
       "      <td>53.230000</td>\n",
       "      <td>53.520000</td>\n",
       "      <td>50.529999</td>\n",
       "      <td>51.099998</td>\n",
       "      <td>51.099998</td>\n",
       "      <td>35494</td>\n",
       "      <td>9518.179688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9510.339844</td>\n",
       "      <td>9592.370117</td>\n",
       "      <td>9459.179688</td>\n",
       "      <td>9518.179688</td>\n",
       "      <td>9518.179688</td>\n",
       "      <td>82466600</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1219.199951</td>\n",
       "      <td>1219.199951</td>\n",
       "      <td>...</td>\n",
       "      <td>1210.599976</td>\n",
       "      <td>1210.599976</td>\n",
       "      <td>29</td>\n",
       "      <td>51.060001</td>\n",
       "      <td>51.840000</td>\n",
       "      <td>49.680000</td>\n",
       "      <td>51.150002</td>\n",
       "      <td>51.150002</td>\n",
       "      <td>37082</td>\n",
       "      <td>9837.610352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9643.769531</td>\n",
       "      <td>9855.429688</td>\n",
       "      <td>9607.900391</td>\n",
       "      <td>9837.610352</td>\n",
       "      <td>9837.610352</td>\n",
       "      <td>114825000</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1207.000000</td>\n",
       "      <td>1215.699951</td>\n",
       "      <td>...</td>\n",
       "      <td>1208.400024</td>\n",
       "      <td>1208.400024</td>\n",
       "      <td>92</td>\n",
       "      <td>51.000000</td>\n",
       "      <td>51.889999</td>\n",
       "      <td>49.820000</td>\n",
       "      <td>50.959999</td>\n",
       "      <td>50.959999</td>\n",
       "      <td>29469</td>\n",
       "      <td>9648.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1969</th>\n",
       "      <td>14113.009766</td>\n",
       "      <td>14160.870117</td>\n",
       "      <td>13890.540039</td>\n",
       "      <td>13914.070312</td>\n",
       "      <td>13914.070312</td>\n",
       "      <td>42893400</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>1818.099976</td>\n",
       "      <td>1818.099976</td>\n",
       "      <td>...</td>\n",
       "      <td>1787.000000</td>\n",
       "      <td>1787.000000</td>\n",
       "      <td>411</td>\n",
       "      <td>82.370003</td>\n",
       "      <td>83.860001</td>\n",
       "      <td>80.680000</td>\n",
       "      <td>80.980003</td>\n",
       "      <td>80.980003</td>\n",
       "      <td>16647</td>\n",
       "      <td>13940.929688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1970</th>\n",
       "      <td>13945.589844</td>\n",
       "      <td>14000.679688</td>\n",
       "      <td>13874.500000</td>\n",
       "      <td>13940.929688</td>\n",
       "      <td>13940.929688</td>\n",
       "      <td>28738700</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>1794.300049</td>\n",
       "      <td>1802.800049</td>\n",
       "      <td>...</td>\n",
       "      <td>1795.900024</td>\n",
       "      <td>1795.900024</td>\n",
       "      <td>49</td>\n",
       "      <td>81.730003</td>\n",
       "      <td>84.370003</td>\n",
       "      <td>81.339996</td>\n",
       "      <td>83.919998</td>\n",
       "      <td>83.919998</td>\n",
       "      <td>8621</td>\n",
       "      <td>13995.099609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1971</th>\n",
       "      <td>14047.419922</td>\n",
       "      <td>14063.139648</td>\n",
       "      <td>13966.349609</td>\n",
       "      <td>13995.099609</td>\n",
       "      <td>13995.099609</td>\n",
       "      <td>22975000</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>1803.400024</td>\n",
       "      <td>1826.300049</td>\n",
       "      <td>...</td>\n",
       "      <td>1814.800049</td>\n",
       "      <td>1814.800049</td>\n",
       "      <td>69</td>\n",
       "      <td>84.459999</td>\n",
       "      <td>85.669998</td>\n",
       "      <td>83.660004</td>\n",
       "      <td>84.330002</td>\n",
       "      <td>84.330002</td>\n",
       "      <td>7512</td>\n",
       "      <td>13925.599609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1972</th>\n",
       "      <td>14013.719727</td>\n",
       "      <td>14018.469727</td>\n",
       "      <td>13914.620117</td>\n",
       "      <td>13925.599609</td>\n",
       "      <td>13925.599609</td>\n",
       "      <td>27583800</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>1803.199951</td>\n",
       "      <td>1807.900024</td>\n",
       "      <td>...</td>\n",
       "      <td>1807.900024</td>\n",
       "      <td>1807.900024</td>\n",
       "      <td>434</td>\n",
       "      <td>84.599998</td>\n",
       "      <td>84.639999</td>\n",
       "      <td>81.949997</td>\n",
       "      <td>83.260002</td>\n",
       "      <td>83.260002</td>\n",
       "      <td>5384</td>\n",
       "      <td>14071.719727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1973</th>\n",
       "      <td>13890.809570</td>\n",
       "      <td>14071.719727</td>\n",
       "      <td>13871.320312</td>\n",
       "      <td>14071.719727</td>\n",
       "      <td>14071.719727</td>\n",
       "      <td>30727400</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>1805.800049</td>\n",
       "      <td>1819.500000</td>\n",
       "      <td>...</td>\n",
       "      <td>1819.500000</td>\n",
       "      <td>1819.500000</td>\n",
       "      <td>277</td>\n",
       "      <td>82.860001</td>\n",
       "      <td>82.910004</td>\n",
       "      <td>81.300003</td>\n",
       "      <td>82.260002</td>\n",
       "      <td>82.260002</td>\n",
       "      <td>20599</td>\n",
       "      <td>13923.589844</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1974 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       ^GDAXI_Open   ^GDAXI_High    ^GDAXI_Low  ^GDAXI_Close  \\\n",
       "0      9869.129883   9879.530273   9687.259766   9764.730469   \n",
       "1      9735.650391   9790.269531   9468.580078   9473.160156   \n",
       "2      9484.250000   9624.650391   9382.820312   9469.660156   \n",
       "3      9510.339844   9592.370117   9459.179688   9518.179688   \n",
       "4      9643.769531   9855.429688   9607.900391   9837.610352   \n",
       "...            ...           ...           ...           ...   \n",
       "1969  14113.009766  14160.870117  13890.540039  13914.070312   \n",
       "1970  13945.589844  14000.679688  13874.500000  13940.929688   \n",
       "1971  14047.419922  14063.139648  13966.349609  13995.099609   \n",
       "1972  14013.719727  14018.469727  13914.620117  13925.599609   \n",
       "1973  13890.809570  14071.719727  13871.320312  14071.719727   \n",
       "\n",
       "      ^GDAXI_Adj Close  ^GDAXI_Volume  ^GDAXI_month  ^GDAXI_weekday  \\\n",
       "0          9764.730469       67673900             1               4   \n",
       "1          9473.160156      105538300             1               0   \n",
       "2          9469.660156       96812300             1               1   \n",
       "3          9518.179688       82466600             1               2   \n",
       "4          9837.610352      114825000             1               3   \n",
       "...                ...            ...           ...             ...   \n",
       "1969      13914.070312       42893400            12               3   \n",
       "1970      13940.929688       28738700            12               4   \n",
       "1971      13995.099609       22975000            12               1   \n",
       "1972      13925.599609       27583800            12               2   \n",
       "1973      14071.719727       30727400            12               3   \n",
       "\n",
       "        GC=F_Open    GC=F_High  ...   GC=F_Close  GC=F_Adj Close  GC=F_Volume  \\\n",
       "0     1184.000000  1194.500000  ...  1186.000000     1186.000000          138   \n",
       "1     1180.300049  1206.900024  ...  1203.900024     1203.900024          470   \n",
       "2     1203.500000  1220.000000  ...  1219.300049     1219.300049           97   \n",
       "3     1219.199951  1219.199951  ...  1210.599976     1210.599976           29   \n",
       "4     1207.000000  1215.699951  ...  1208.400024     1208.400024           92   \n",
       "...           ...          ...  ...          ...             ...          ...   \n",
       "1969  1818.099976  1818.099976  ...  1787.000000     1787.000000          411   \n",
       "1970  1794.300049  1802.800049  ...  1795.900024     1795.900024           49   \n",
       "1971  1803.400024  1826.300049  ...  1814.800049     1814.800049           69   \n",
       "1972  1803.199951  1807.900024  ...  1807.900024     1807.900024          434   \n",
       "1973  1805.800049  1819.500000  ...  1819.500000     1819.500000          277   \n",
       "\n",
       "      BZ=F_Open  BZ=F_High   BZ=F_Low  BZ=F_Close  BZ=F_Adj Close  \\\n",
       "0     57.630001  58.220001  55.520000   56.419998       56.419998   \n",
       "1     56.290001  56.290001  52.669998   53.110001       53.110001   \n",
       "2     53.230000  53.520000  50.529999   51.099998       51.099998   \n",
       "3     51.060001  51.840000  49.680000   51.150002       51.150002   \n",
       "4     51.000000  51.889999  49.820000   50.959999       50.959999   \n",
       "...         ...        ...        ...         ...             ...   \n",
       "1969  82.370003  83.860001  80.680000   80.980003       80.980003   \n",
       "1970  81.730003  84.370003  81.339996   83.919998       83.919998   \n",
       "1971  84.459999  85.669998  83.660004   84.330002       84.330002   \n",
       "1972  84.599998  84.639999  81.949997   83.260002       83.260002   \n",
       "1973  82.860001  82.910004  81.300003   82.260002       82.260002   \n",
       "\n",
       "      BZ=F_Volume             Y  \n",
       "0           16707   9473.160156  \n",
       "1           30065   9469.660156  \n",
       "2           35494   9518.179688  \n",
       "3           37082   9837.610352  \n",
       "4           29469        9648.5  \n",
       "...           ...           ...  \n",
       "1969        16647  13940.929688  \n",
       "1970         8621  13995.099609  \n",
       "1971         7512  13925.599609  \n",
       "1972         5384  14071.719727  \n",
       "1973        20599  13923.589844  \n",
       "\n",
       "[1974 rows x 21 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "scaler = MinMaxScaler()\n",
    "scaler_y = MinMaxScaler()\n",
    "\n",
    "df = pd.read_pickle(train_data_path)\n",
    "df = df[['^GDAXI_Open', '^GDAXI_High', '^GDAXI_Low', '^GDAXI_Close',\n",
    "                                '^GDAXI_Adj Close', '^GDAXI_Volume', '^GDAXI_month', '^GDAXI_weekday',\n",
    "                                'GC=F_Open', 'GC=F_High', 'GC=F_Low', 'GC=F_Close',\n",
    "                                'GC=F_Adj Close', 'GC=F_Volume',\n",
    "                                'BZ=F_Open', 'BZ=F_High', 'BZ=F_Low', 'BZ=F_Close',\n",
    "                                'BZ=F_Adj Close', 'BZ=F_Volume',\n",
    "                                'Y']]\n",
    "\n",
    "display(df)\n",
    "\n",
    "scaled_train_inputs = scaler.fit(df.iloc[:, :-1].values)  \n",
    "scaled_train_labels = scaler_y.fit(df.iloc[:, -1].values.reshape(-1, 1))\n",
    "\n",
    "model_path = \"../Models/best_model.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-11-01T10:17:07.921380Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fd/rkz1xz5d2fzgd9_trr31m_200000gn/T/ipykernel_7390/3780326204.py:10: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>^GDAXI_Open</th>\n",
       "      <th>^GDAXI_High</th>\n",
       "      <th>^GDAXI_Low</th>\n",
       "      <th>^GDAXI_Close</th>\n",
       "      <th>^GDAXI_Adj Close</th>\n",
       "      <th>^GDAXI_Volume</th>\n",
       "      <th>^GDAXI_month</th>\n",
       "      <th>^GDAXI_weekday</th>\n",
       "      <th>GC=F_Open</th>\n",
       "      <th>GC=F_High</th>\n",
       "      <th>...</th>\n",
       "      <th>GC=F_Close</th>\n",
       "      <th>GC=F_Adj Close</th>\n",
       "      <th>GC=F_Volume</th>\n",
       "      <th>BZ=F_Open</th>\n",
       "      <th>BZ=F_High</th>\n",
       "      <th>BZ=F_Low</th>\n",
       "      <th>BZ=F_Close</th>\n",
       "      <th>BZ=F_Adj Close</th>\n",
       "      <th>BZ=F_Volume</th>\n",
       "      <th>Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11951.839844</td>\n",
       "      <td>12227.870117</td>\n",
       "      <td>11893.940430</td>\n",
       "      <td>12209.480469</td>\n",
       "      <td>12209.480469</td>\n",
       "      <td>63674200</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>1667.199951</td>\n",
       "      <td>1700.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1692.900024</td>\n",
       "      <td>1692.900024</td>\n",
       "      <td>410</td>\n",
       "      <td>86.510002</td>\n",
       "      <td>89.830002</td>\n",
       "      <td>86.510002</td>\n",
       "      <td>88.860001</td>\n",
       "      <td>88.860001</td>\n",
       "      <td>38251</td>\n",
       "      <td>12670.480469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12360.719727</td>\n",
       "      <td>12673.349609</td>\n",
       "      <td>12358.980469</td>\n",
       "      <td>12670.480469</td>\n",
       "      <td>12670.480469</td>\n",
       "      <td>80240300</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>1701.199951</td>\n",
       "      <td>1728.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1721.099976</td>\n",
       "      <td>1721.099976</td>\n",
       "      <td>291</td>\n",
       "      <td>88.889999</td>\n",
       "      <td>92.389999</td>\n",
       "      <td>88.690002</td>\n",
       "      <td>91.800003</td>\n",
       "      <td>91.800003</td>\n",
       "      <td>38971</td>\n",
       "      <td>12517.179688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12611.070312</td>\n",
       "      <td>12661.879883</td>\n",
       "      <td>12455.360352</td>\n",
       "      <td>12517.179688</td>\n",
       "      <td>12517.179688</td>\n",
       "      <td>62392400</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>1724.099976</td>\n",
       "      <td>1726.599976</td>\n",
       "      <td>...</td>\n",
       "      <td>1711.400024</td>\n",
       "      <td>1711.400024</td>\n",
       "      <td>418</td>\n",
       "      <td>91.650002</td>\n",
       "      <td>93.970001</td>\n",
       "      <td>90.900002</td>\n",
       "      <td>93.370003</td>\n",
       "      <td>93.370003</td>\n",
       "      <td>39563</td>\n",
       "      <td>12470.780273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12593.410156</td>\n",
       "      <td>12643.219727</td>\n",
       "      <td>12421.889648</td>\n",
       "      <td>12470.780273</td>\n",
       "      <td>12470.780273</td>\n",
       "      <td>53529600</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>1721.000000</td>\n",
       "      <td>1723.300049</td>\n",
       "      <td>...</td>\n",
       "      <td>1711.699951</td>\n",
       "      <td>1711.699951</td>\n",
       "      <td>133</td>\n",
       "      <td>93.830002</td>\n",
       "      <td>95.010002</td>\n",
       "      <td>92.730003</td>\n",
       "      <td>94.419998</td>\n",
       "      <td>94.419998</td>\n",
       "      <td>26200</td>\n",
       "      <td>12273.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12414.780273</td>\n",
       "      <td>12497.519531</td>\n",
       "      <td>12272.589844</td>\n",
       "      <td>12273.000000</td>\n",
       "      <td>12273.000000</td>\n",
       "      <td>56973200</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>1710.099976</td>\n",
       "      <td>1710.099976</td>\n",
       "      <td>...</td>\n",
       "      <td>1700.500000</td>\n",
       "      <td>1700.500000</td>\n",
       "      <td>179</td>\n",
       "      <td>94.989998</td>\n",
       "      <td>98.589996</td>\n",
       "      <td>93.930000</td>\n",
       "      <td>97.919998</td>\n",
       "      <td>97.919998</td>\n",
       "      <td>36871</td>\n",
       "      <td>12272.94043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>304</th>\n",
       "      <td>16779.410156</td>\n",
       "      <td>16789.960938</td>\n",
       "      <td>16694.560547</td>\n",
       "      <td>16733.050781</td>\n",
       "      <td>16733.050781</td>\n",
       "      <td>63366400</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>2037.099976</td>\n",
       "      <td>2037.099976</td>\n",
       "      <td>...</td>\n",
       "      <td>2034.500000</td>\n",
       "      <td>2034.500000</td>\n",
       "      <td>258</td>\n",
       "      <td>79.250000</td>\n",
       "      <td>80.610001</td>\n",
       "      <td>79.080002</td>\n",
       "      <td>79.699997</td>\n",
       "      <td>79.699997</td>\n",
       "      <td>26913</td>\n",
       "      <td>16687.419922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>305</th>\n",
       "      <td>16667.310547</td>\n",
       "      <td>16708.349609</td>\n",
       "      <td>16624.160156</td>\n",
       "      <td>16687.419922</td>\n",
       "      <td>16687.419922</td>\n",
       "      <td>57871300</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>2035.800049</td>\n",
       "      <td>2044.500000</td>\n",
       "      <td>...</td>\n",
       "      <td>2039.099976</td>\n",
       "      <td>2039.099976</td>\n",
       "      <td>228</td>\n",
       "      <td>79.139999</td>\n",
       "      <td>80.120003</td>\n",
       "      <td>77.849998</td>\n",
       "      <td>79.389999</td>\n",
       "      <td>79.389999</td>\n",
       "      <td>22237</td>\n",
       "      <td>16706.179688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306</th>\n",
       "      <td>16673.300781</td>\n",
       "      <td>16735.320312</td>\n",
       "      <td>16651.779297</td>\n",
       "      <td>16706.179688</td>\n",
       "      <td>16706.179688</td>\n",
       "      <td>46295300</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>2055.699951</td>\n",
       "      <td>2068.699951</td>\n",
       "      <td>...</td>\n",
       "      <td>2057.100098</td>\n",
       "      <td>2057.100098</td>\n",
       "      <td>202</td>\n",
       "      <td>79.440002</td>\n",
       "      <td>80.370003</td>\n",
       "      <td>78.830002</td>\n",
       "      <td>79.070000</td>\n",
       "      <td>79.070000</td>\n",
       "      <td>12334</td>\n",
       "      <td>16742.070312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307</th>\n",
       "      <td>16727.769531</td>\n",
       "      <td>16775.710938</td>\n",
       "      <td>16697.580078</td>\n",
       "      <td>16742.070312</td>\n",
       "      <td>16742.070312</td>\n",
       "      <td>37678900</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>2067.300049</td>\n",
       "      <td>2081.899902</td>\n",
       "      <td>...</td>\n",
       "      <td>2081.899902</td>\n",
       "      <td>2081.899902</td>\n",
       "      <td>586</td>\n",
       "      <td>80.739998</td>\n",
       "      <td>81.320000</td>\n",
       "      <td>79.489998</td>\n",
       "      <td>79.650002</td>\n",
       "      <td>79.650002</td>\n",
       "      <td>8282</td>\n",
       "      <td>16701.550781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>308</th>\n",
       "      <td>16780.949219</td>\n",
       "      <td>16783.789062</td>\n",
       "      <td>16688.519531</td>\n",
       "      <td>16701.550781</td>\n",
       "      <td>16701.550781</td>\n",
       "      <td>36091600</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>2081.600098</td>\n",
       "      <td>2087.300049</td>\n",
       "      <td>...</td>\n",
       "      <td>2073.899902</td>\n",
       "      <td>2073.899902</td>\n",
       "      <td>338</td>\n",
       "      <td>79.839996</td>\n",
       "      <td>79.959999</td>\n",
       "      <td>78.339996</td>\n",
       "      <td>78.389999</td>\n",
       "      <td>78.389999</td>\n",
       "      <td>24301</td>\n",
       "      <td>16751.640625</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>309 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      ^GDAXI_Open   ^GDAXI_High    ^GDAXI_Low  ^GDAXI_Close  ^GDAXI_Adj Close  \\\n",
       "0    11951.839844  12227.870117  11893.940430  12209.480469      12209.480469   \n",
       "1    12360.719727  12673.349609  12358.980469  12670.480469      12670.480469   \n",
       "2    12611.070312  12661.879883  12455.360352  12517.179688      12517.179688   \n",
       "3    12593.410156  12643.219727  12421.889648  12470.780273      12470.780273   \n",
       "4    12414.780273  12497.519531  12272.589844  12273.000000      12273.000000   \n",
       "..            ...           ...           ...           ...               ...   \n",
       "304  16779.410156  16789.960938  16694.560547  16733.050781      16733.050781   \n",
       "305  16667.310547  16708.349609  16624.160156  16687.419922      16687.419922   \n",
       "306  16673.300781  16735.320312  16651.779297  16706.179688      16706.179688   \n",
       "307  16727.769531  16775.710938  16697.580078  16742.070312      16742.070312   \n",
       "308  16780.949219  16783.789062  16688.519531  16701.550781      16701.550781   \n",
       "\n",
       "     ^GDAXI_Volume  ^GDAXI_month  ^GDAXI_weekday    GC=F_Open    GC=F_High  \\\n",
       "0         63674200            10               0  1667.199951  1700.000000   \n",
       "1         80240300            10               1  1701.199951  1728.000000   \n",
       "2         62392400            10               2  1724.099976  1726.599976   \n",
       "3         53529600            10               3  1721.000000  1723.300049   \n",
       "4         56973200            10               4  1710.099976  1710.099976   \n",
       "..             ...           ...             ...          ...          ...   \n",
       "304       63366400            12               2  2037.099976  2037.099976   \n",
       "305       57871300            12               3  2035.800049  2044.500000   \n",
       "306       46295300            12               4  2055.699951  2068.699951   \n",
       "307       37678900            12               2  2067.300049  2081.899902   \n",
       "308       36091600            12               3  2081.600098  2087.300049   \n",
       "\n",
       "     ...   GC=F_Close  GC=F_Adj Close  GC=F_Volume  BZ=F_Open  BZ=F_High  \\\n",
       "0    ...  1692.900024     1692.900024          410  86.510002  89.830002   \n",
       "1    ...  1721.099976     1721.099976          291  88.889999  92.389999   \n",
       "2    ...  1711.400024     1711.400024          418  91.650002  93.970001   \n",
       "3    ...  1711.699951     1711.699951          133  93.830002  95.010002   \n",
       "4    ...  1700.500000     1700.500000          179  94.989998  98.589996   \n",
       "..   ...          ...             ...          ...        ...        ...   \n",
       "304  ...  2034.500000     2034.500000          258  79.250000  80.610001   \n",
       "305  ...  2039.099976     2039.099976          228  79.139999  80.120003   \n",
       "306  ...  2057.100098     2057.100098          202  79.440002  80.370003   \n",
       "307  ...  2081.899902     2081.899902          586  80.739998  81.320000   \n",
       "308  ...  2073.899902     2073.899902          338  79.839996  79.959999   \n",
       "\n",
       "      BZ=F_Low  BZ=F_Close  BZ=F_Adj Close  BZ=F_Volume             Y  \n",
       "0    86.510002   88.860001       88.860001        38251  12670.480469  \n",
       "1    88.690002   91.800003       91.800003        38971  12517.179688  \n",
       "2    90.900002   93.370003       93.370003        39563  12470.780273  \n",
       "3    92.730003   94.419998       94.419998        26200       12273.0  \n",
       "4    93.930000   97.919998       97.919998        36871   12272.94043  \n",
       "..         ...         ...             ...          ...           ...  \n",
       "304  79.080002   79.699997       79.699997        26913  16687.419922  \n",
       "305  77.849998   79.389999       79.389999        22237  16706.179688  \n",
       "306  78.830002   79.070000       79.070000        12334  16742.070312  \n",
       "307  79.489998   79.650002       79.650002         8282  16701.550781  \n",
       "308  78.339996   78.389999       78.389999        24301  16751.640625  \n",
       "\n",
       "[309 rows x 21 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted values: [0.6686028  0.67494726 0.6816493  0.6885419  0.69573426 0.7033571\n",
      " 0.7104021  0.7170713  0.7234162  0.7294277  0.7350105  0.7389651\n",
      " 0.7425835  0.7462071  0.75013393 0.7546163  0.75721276 0.76011026\n",
      " 0.76359177 0.7677993  0.7739286  0.7752104  0.77806866 0.7823608\n",
      " 0.789636   0.80157745 0.7986945  0.7977536  0.800068   0.80698436\n",
      " 0.81912565 0.8052725  0.79495215 0.7917019  0.7919992  0.7987639\n",
      " 0.781008   0.771894   0.76764405 0.77392066 0.76156867 0.75810647\n",
      " 0.7591718  0.7648232  0.77567345 0.7759027  0.78091854 0.78934085\n",
      " 0.7995152  0.8125367  0.816993   0.82314533 0.830564   0.8396815\n",
      " 0.8519301  0.85668546 0.86335635 0.8708062  0.8789018  0.88781184\n",
      " 0.8927078  0.89826834 0.9036895  0.90898114 0.9105416  0.9125015\n",
      " 0.91455597 0.91701734 0.9167553  0.9169051  0.9170653  0.9172072\n",
      " 0.91754395 0.9157026  0.91451234 0.9140329  0.91462106 0.9132708\n",
      " 0.9125523  0.9128192  0.9140536  0.91575474 0.9153324  0.9149274\n",
      " 0.9151196  0.9159661  0.9170296  0.9148831  0.91344076 0.91267014\n",
      " 0.912762   0.91359204 0.9121825  0.91150105 0.91165906 0.912857\n",
      " 0.9153361  0.9165373  0.9190622  0.92213047 0.92575896 0.92784566\n",
      " 0.9301855  0.9328013  0.935875   0.9387994  0.940373   0.94204724\n",
      " 0.9437337  0.94542855 0.9471623  0.9477152  0.94828635 0.9489487\n",
      " 0.94996816 0.951143   0.95158464 0.9520876  0.9527627  0.95356923\n",
      " 0.95454884 0.9549237  0.95531327 0.95584375 0.9564438  0.9569034\n",
      " 0.95691866 0.95703554 0.957353   0.9579268  0.95812774 0.9583284\n",
      " 0.9585306  0.9588173  0.95870405 0.9585901  0.9585513  0.95872706\n",
      " 0.959033   0.9590226  0.95914954 0.9593585  0.95974725 0.96013224\n",
      " 0.9602851  0.9604994  0.9607989  0.9611699  0.9612089  0.9612132\n",
      " 0.96113545 0.96106744 0.96095043 0.9604525  0.95994216 0.95954865\n",
      " 0.9594148  0.9594759  0.9592646  0.9590778  0.95902467 0.9590536\n",
      " 0.95916957 0.95916027 0.959307   0.95962614 0.9601281  0.96031016\n",
      " 0.9605878  0.9609292  0.9614148  0.9620524  0.9622191  0.9623613\n",
      " 0.9625758  0.96304804 0.963726   0.96396506 0.9642988  0.9646504\n",
      " 0.965143   0.96512777 0.9651095  0.965113   0.9651738  0.9652496\n",
      " 0.96499306 0.96485734 0.9648283  0.96485025 0.9646215  0.9644105\n",
      " 0.9642314  0.96406555 0.9639645  0.9636393  0.9633711  0.963133\n",
      " 0.96310383 0.9632492  0.9632515  0.96335286 0.96349937 0.9636318\n",
      " 0.9639774  0.96387064 0.96375024 0.9637523  0.963905   0.96420735\n",
      " 0.96410143 0.9640504  0.964055   0.964152   0.9643025  0.9639557\n",
      " 0.9635002  0.9629963  0.9626108  0.96245253 0.9618312  0.96124595\n",
      " 0.9606798  0.96024615 0.960142   0.9596031  0.9592101  0.95900357\n",
      " 0.95893925 0.9591313  0.958914   0.95898336 0.9591976  0.9594174\n",
      " 0.9590407  0.95864356 0.9583126  0.95792407 0.9575843  0.9564001\n",
      " 0.9551739  0.9541225  0.953528   0.9534951  0.95287514 0.9527196\n",
      " 0.9528447  0.95336336 0.9537204  0.9535019  0.9535278  0.9539597\n",
      " 0.95466787 0.95570135 0.9561891  0.95690304 0.95776945 0.9587167\n",
      " 0.9599559  0.96059155 0.9613401  0.9622764  0.9634279  0.96482164\n",
      " 0.9655718  0.9663758  0.96723694 0.9683295  0.969433   0.9701316\n",
      " 0.9707653  0.97141844 0.972058   0.97271115 0.97307616 0.97343767\n",
      " 0.9738291  0.97418857 0.97462696]\n",
      "Actual values: [0.7582081  0.73975617 0.7438876  0.7650217  0.7583895  0.76380706\n",
      " 0.7644763  0.7790088  0.75882757 0.7552631  0.76057464 0.7724852\n",
      " 0.77747744 0.76703316 0.75369734 0.74322486 0.74365515 0.75721323\n",
      " 0.7490281  0.7734545  0.76864105 0.70810884 0.69621104 0.70257115\n",
      " 0.69513696 0.7223603  0.698893   0.7023233  0.7092415  0.7003655\n",
      " 0.7190269  0.70010877 0.73306906 0.7725465  0.7655899  0.787775\n",
      " 0.8111223  0.808794   0.8309281  0.8450263  0.84863037 0.861472\n",
      " 0.8607989  0.8274096  0.8418667  0.85072875 0.8494721  0.84800714\n",
      " 0.8545473  0.8567415  0.85368276 0.85396236 0.8606636  0.902611\n",
      " 0.89842707 0.881758   0.87856126 0.8902049  0.9044283  0.87691385\n",
      " 0.8883262  0.88618314 0.90224695 0.9057335  0.89913845 0.888362\n",
      " 0.88865185 0.89833266 0.86436725 0.8862943  0.8842138  0.8765357\n",
      " 0.8794246  0.91144866 0.9210515  0.9090401  0.9182789  0.91844994\n",
      " 0.8922381  0.8324044  0.8673161  0.8037698  0.83337885 0.8079767\n",
      " 0.82907236 0.86252815 0.86519104 0.8644502  0.8321184  0.853887\n",
      " 0.85571843 0.87957025 0.9042981  0.91789186 0.91177183 0.91465175\n",
      " 0.9040133  0.9139391  0.9212545  0.92743963 0.9307424  0.9407091\n",
      " 0.93841416 0.95030934 0.9519096  0.9392365  0.95018035 0.94791853\n",
      " 0.9489632  0.939206   0.9398088  0.9553808  0.9304206  0.9416746\n",
      " 0.93135285 0.9603156  0.9592697  0.9596082  0.95204115 0.94420975\n",
      " 0.95428765 0.9547244  0.95225817 0.95907426 0.9861572  1.0004636\n",
      " 0.9939004  0.9848162  0.94513184 0.9389594  0.96324664 0.95366055\n",
      " 0.92238474 0.9466044  0.97183675 0.96068215 0.96432847 0.9602569\n",
      " 0.96401167 0.9588878  0.9777932  0.9947548  1.0049859  1.0023462\n",
      " 1.010968   0.979511   0.9682479  0.96378183 0.9435751  0.9414191\n",
      " 0.94573593 0.9587805  0.9584893  0.9841828  0.9756438  0.95732206\n",
      " 0.9050822  0.9146429  0.92355216 0.9385175  0.9682313  0.98330534\n",
      " 0.9787128  0.9740615  0.9813207  0.97920567 0.9913755  0.98792726\n",
      " 0.98968077 0.9923167  0.9820831  1.0171492  1.0252873  1.0223601\n",
      " 0.99599624 0.9678507  0.95167714 0.95914584 0.9590053  0.9365494\n",
      " 0.94646645 0.9648494  0.9438598  0.9530654  0.9355725  0.9384039\n",
      " 0.9240298  0.9109212  0.91462755 0.9276977  0.9306083  0.9169532\n",
      " 0.9182725  0.9388075  0.9564664  0.9514919  0.9585353  0.9449032\n",
      " 0.9361383  0.93226343 0.9293631  0.93212676 0.93987775 0.92896336\n",
      " 0.921109   0.9404269  0.95169634 0.9304435  0.9224436  0.93740004\n",
      " 0.9106148  0.908754   0.88936716 0.8702587  0.8653519  0.8788959\n",
      " 0.88695204 0.86915267 0.8484631  0.85034174 0.8465486  0.8669253\n",
      " 0.85394204 0.8916697  0.89633    0.89186263 0.8614196  0.8679752\n",
      " 0.86972487 0.84970194 0.8433572  0.81184256 0.8121299  0.82224745\n",
      " 0.8238106  0.80323213 0.7976588  0.801379   0.81335855 0.82778114\n",
      " 0.85592026 0.86175036 0.8549458  0.85707474 0.86690354 0.8826047\n",
      " 0.8675153  0.88164175 0.91605145 0.9331319  0.9380412  0.9549696\n",
      " 0.95269245 0.9525903  0.959907   0.96906024 0.96099895 0.9643578\n",
      " 0.9865518  0.99280715 1.0160624  1.0169871  1.0333791  1.04913\n",
      " 1.0456244  1.0622565  1.0667531  1.0664097  1.0631287  1.0613637\n",
      " 1.0612627  1.0483779  1.060365   1.0589142  1.0530865  1.0554824\n",
      " 1.0600661  1.0548912  1.0612884 ]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predicted</th>\n",
       "      <th>Actual</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.668603</td>\n",
       "      <td>0.758208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.674947</td>\n",
       "      <td>0.739756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.681649</td>\n",
       "      <td>0.743888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.688542</td>\n",
       "      <td>0.765022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.695734</td>\n",
       "      <td>0.758389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274</th>\n",
       "      <td>0.973076</td>\n",
       "      <td>1.053087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275</th>\n",
       "      <td>0.973438</td>\n",
       "      <td>1.055482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>276</th>\n",
       "      <td>0.973829</td>\n",
       "      <td>1.060066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>277</th>\n",
       "      <td>0.974189</td>\n",
       "      <td>1.054891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>278</th>\n",
       "      <td>0.974627</td>\n",
       "      <td>1.061288</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>279 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Predicted    Actual\n",
       "0     0.668603  0.758208\n",
       "1     0.674947  0.739756\n",
       "2     0.681649  0.743888\n",
       "3     0.688542  0.765022\n",
       "4     0.695734  0.758389\n",
       "..         ...       ...\n",
       "274   0.973076  1.053087\n",
       "275   0.973438  1.055482\n",
       "276   0.973829  1.060066\n",
       "277   0.974189  1.054891\n",
       "278   0.974627  1.061288\n",
       "\n",
       "[279 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "\n",
    "model = Net(input_size, output_size, hidden_size, num_layers)\n",
    "\n",
    "# Load state_dict only\n",
    "model.load_state_dict(torch.load(model_path)) \n",
    "model.eval()\n",
    "\n",
    "df = pd.read_pickle(test_data_path)\n",
    "df = df[['^GDAXI_Open', '^GDAXI_High', '^GDAXI_Low', '^GDAXI_Close',\n",
    "                                '^GDAXI_Adj Close', '^GDAXI_Volume', '^GDAXI_month', '^GDAXI_weekday',\n",
    "                                'GC=F_Open', 'GC=F_High', 'GC=F_Low', 'GC=F_Close',\n",
    "                                'GC=F_Adj Close', 'GC=F_Volume',\n",
    "                                'BZ=F_Open', 'BZ=F_High', 'BZ=F_Low', 'BZ=F_Close',\n",
    "                                'BZ=F_Adj Close', 'BZ=F_Volume',\n",
    "                                'Y']]\n",
    "display(df)\n",
    "scaled_test_inputs = scaler.transform(df.iloc[:, :-1].values)  \n",
    "scaled_test_labels = scaler_y.transform(df.iloc[:, -1].values.reshape(-1, 1))\n",
    "\n",
    "test_data = FinanceDataset(scaled_test_inputs, scaled_test_labels, seq_size=seq_size)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        out = model(inputs) \n",
    "        \n",
    "        all_predictions.append(out.numpy())  \n",
    "        all_labels.append(labels.numpy())\n",
    "\n",
    "all_predictions = np.concatenate(all_predictions)\n",
    "all_labels = np.concatenate(all_labels)\n",
    "\n",
    "print(f'Predicted values: {all_predictions.flatten()}')\n",
    "print(f'Actual values: {all_labels.flatten()}')\n",
    "\n",
    "output_df = pd.DataFrame({'Predicted': all_predictions.flatten(), 'Actual': all_labels.flatten()})\n",
    "display(output_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             pred        actual\n",
      "0    13676.896484  14378.509766\n",
      "1    13726.573242  14234.030273\n",
      "2    13779.050781  14266.379883\n",
      "3    13833.020508  14431.860352\n",
      "4    13889.336914  14379.929688\n",
      "..            ...           ...\n",
      "274  16060.934570  16687.419922\n",
      "275  16063.766602  16706.179688\n",
      "276  16066.830078  16742.070312\n",
      "277  16069.645508  16701.550781\n",
      "278  16073.078125  16751.640625\n",
      "\n",
      "[279 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "pred = scaler_y.inverse_transform(all_predictions).reshape(-1, 1)\n",
    "actual = scaler_y.inverse_transform(all_labels.reshape(-1, 1)).reshape(-1, 1)\n",
    "\n",
    "# DataFrame erstellen\n",
    "df_results = pd.DataFrame({\n",
    "    \"pred\": pred.flatten(),\n",
    "    \"actual\": actual.flatten(),\n",
    "})\n",
    "\n",
    "# DataFrame anzeigen\n",
    "print(df_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fd/rkz1xz5d2fzgd9_trr31m_200000gn/T/ipykernel_7390/800740207.py:30: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path))\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Backtest.__init__() got an unexpected keyword argument 'name'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 44\u001b[0m\n\u001b[1;32m     41\u001b[0m end_date \u001b[38;5;241m=\u001b[39m datetime(\u001b[38;5;241m2023\u001b[39m, \u001b[38;5;241m12\u001b[39m, \u001b[38;5;241m31\u001b[39m)\n\u001b[1;32m     42\u001b[0m broker \u001b[38;5;241m=\u001b[39m Alpaca(ALPACA_CREDS)\n\u001b[0;32m---> 44\u001b[0m strategy \u001b[38;5;241m=\u001b[39m \u001b[43mBacktest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTest4-Raw Material\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbroker\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbroker\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msymbol\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m^GDAXI\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcash_at_risk\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnum_prior_days\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdataset\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mscaler\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mscaler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mscaler_y\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mscaler_y\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m# Run backtest\u001b[39;00m\n\u001b[1;32m     59\u001b[0m backtest_results \u001b[38;5;241m=\u001b[39m strategy\u001b[38;5;241m.\u001b[39mbacktest(\n\u001b[1;32m     60\u001b[0m     YahooDataBacktesting,\n\u001b[1;32m     61\u001b[0m     start_date,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     75\u001b[0m     show_tearsheet\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     76\u001b[0m )\n",
      "\u001b[0;31mTypeError\u001b[0m: Backtest.__init__() got an unexpected keyword argument 'name'"
     ]
    }
   ],
   "source": [
    "os.makedirs(\"logs\", exist_ok=True)\n",
    "os.makedirs(\"results\", exist_ok=True)\n",
    "\n",
    "test_data = pd.read_pickle(test_data_path)\n",
    "train_data = pd.read_pickle(train_data_path)\n",
    "\n",
    "test_data = test_data[['Date','^GDAXI_Open', '^GDAXI_High', '^GDAXI_Low', '^GDAXI_Close',\n",
    "                                '^GDAXI_Adj Close', '^GDAXI_Volume', '^GDAXI_month', '^GDAXI_weekday',\n",
    "                                'GC=F_Open', 'GC=F_High', 'GC=F_Low', 'GC=F_Close',\n",
    "                                'GC=F_Adj Close', 'GC=F_Volume',\n",
    "                                'BZ=F_Open', 'BZ=F_High', 'BZ=F_Low', 'BZ=F_Close',\n",
    "                                'BZ=F_Adj Close', 'BZ=F_Volume',\n",
    "                                'Y']]\n",
    "train_data = train_data[['^GDAXI_Open', '^GDAXI_High', '^GDAXI_Low', '^GDAXI_Close',\n",
    "                                '^GDAXI_Adj Close', '^GDAXI_Volume', '^GDAXI_month', '^GDAXI_weekday',\n",
    "                                'GC=F_Open', 'GC=F_High', 'GC=F_Low', 'GC=F_Close',\n",
    "                                'GC=F_Adj Close', 'GC=F_Volume',\n",
    "                                'BZ=F_Open', 'BZ=F_High', 'BZ=F_Low', 'BZ=F_Close',\n",
    "                                'BZ=F_Adj Close', 'BZ=F_Volume',\n",
    "                                'Y']]\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(train_data.iloc[:, :-1].values)\n",
    "\n",
    "scaler_y = MinMaxScaler()\n",
    "scaler_y.fit(train_data.iloc[:, -1].values.reshape(-1, 1))\n",
    "\n",
    "model = Net(input_size, output_size, hidden_size, num_layers)\n",
    "model_path = \"../Models/best_model.pt\"\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "model.eval()\n",
    "\n",
    "ALPACA_CREDS = {\n",
    "    \"API_KEY\": os.getenv(\"ALPACA_API_KEY\"),\n",
    "    \"API_SECRET\": os.getenv(\"ALPACA_API_SECRET\"),\n",
    "    \"PAPER\": True,\n",
    "}\n",
    "\n",
    "# Strategy setup\n",
    "start_date = datetime(2023, 1, 1)\n",
    "end_date = datetime(2023, 12, 31)\n",
    "broker = Alpaca(ALPACA_CREDS)\n",
    "\n",
    "strategy = Backtest(\n",
    "    name=\"Test4-Raw Material\",\n",
    "    broker=broker,\n",
    "    parameters={\n",
    "        \"symbol\": \"^GDAXI\",\n",
    "        \"cash_at_risk\": 0.5,\n",
    "        \"model\": model,\n",
    "        \"num_prior_days\": 30,\n",
    "        \"dataset\": test_data,\n",
    "        \"scaler\": scaler,\n",
    "        \"scaler_y\": scaler_y,\n",
    "    },\n",
    ")\n",
    "\n",
    "# Run backtest\n",
    "backtest_results = strategy.backtest(\n",
    "    YahooDataBacktesting,\n",
    "    start_date,\n",
    "    end_date,\n",
    "    name=\"Test4-Raw Material\",\n",
    "    parameters={\n",
    "        \"symbol\": \"^GDAXI\",\n",
    "        \"cash_at_risk\": 0.5,\n",
    "        \"model\": model,\n",
    "        \"dataset\": test_data,\n",
    "        \"num_prior_days\": 30,\n",
    "        \"scaler\": scaler,\n",
    "        \"scaler_y\": scaler_y,\n",
    "    },\n",
    "    benchmark_asset=\"^GDAXI\",\n",
    "    show_plot=True,\n",
    "    show_tearsheet=True,\n",
    ")\n",
    "\n",
    "# Save results\n",
    "pd.DataFrame(backtest_results).to_csv(\"results/backtest_results.csv.gz\", index=False, compression=\"gzip\")\n",
    "\n",
    "print(\"Backtesting complete. Results saved to backtest_results.csv.gz.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wistudent\\AppData\\Local\\Temp\\ipykernel_10684\\1285740814.py:48: FutureWarning:\n",
      "\n",
      "You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "input.size(-1) must be equal to input_size. Expected 20, got 19",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 82\u001b[0m\n\u001b[0;32m     73\u001b[0m test_data \u001b[38;5;241m=\u001b[39m test_data[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m^GDAXI_Open\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m^GDAXI_High\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m^GDAXI_Low\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m^GDAXI_Close\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     74\u001b[0m                                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m^GDAXI_Adj Close\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m^GDAXI_Volume\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m^GDAXI_month\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m^GDAXI_weekday\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     75\u001b[0m                                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGC=F_Open\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGC=F_High\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGC=F_Low\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGC=F_Close\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     78\u001b[0m                                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBZ=F_Adj Close\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBZ=F_Volume\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     79\u001b[0m                                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mY\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n\u001b[0;32m     80\u001b[0m model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../Models/best_model.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# Pfad zum gespeicherten Modell\u001b[39;00m\n\u001b[1;32m---> 82\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mrun_backtest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseq_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[29], line 52\u001b[0m, in \u001b[0;36mrun_backtest\u001b[1;34m(test_df, model_path, seq_size)\u001b[0m\n\u001b[0;32m     49\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m     51\u001b[0m \u001b[38;5;66;03m# Backtesting durchführen\u001b[39;00m\n\u001b[1;32m---> 52\u001b[0m predictions, actuals \u001b[38;5;241m=\u001b[39m \u001b[43mbacktest_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscaler_y\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;66;03m# Ergebnisse visualisieren\u001b[39;00m\n\u001b[0;32m     55\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m14\u001b[39m, \u001b[38;5;241m7\u001b[39m))\n",
      "Cell \u001b[1;32mIn[29], line 10\u001b[0m, in \u001b[0;36mbacktest_model\u001b[1;34m(model, dataloader, scaler_y)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m X_batch, y_batch \u001b[38;5;129;01min\u001b[39;00m dataloader:\n\u001b[0;32m      9\u001b[0m         \u001b[38;5;66;03m# Vorhersage\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m         output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m         predictions\u001b[38;5;241m.\u001b[39mextend(output\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[0;32m     12\u001b[0m         actuals\u001b[38;5;241m.\u001b[39mextend(y_batch\u001b[38;5;241m.\u001b[39mnumpy())\n",
      "File \u001b[1;32mc:\\Users\\wistudent\\Documents\\AKI-Trading-Model-Test4\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\wistudent\\Documents\\AKI-Trading-Model-Test4\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[18], line 8\u001b[0m, in \u001b[0;36mNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m----> 8\u001b[0m     x, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc(x[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :]) \n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32mc:\\Users\\wistudent\\Documents\\AKI-Trading-Model-Test4\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\wistudent\\Documents\\AKI-Trading-Model-Test4\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\wistudent\\Documents\\AKI-Trading-Model-Test4\\.venv\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:1100\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m   1092\u001b[0m     c_zeros \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(\n\u001b[0;32m   1093\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers \u001b[38;5;241m*\u001b[39m num_directions,\n\u001b[0;32m   1094\u001b[0m         max_batch_size,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1097\u001b[0m         device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdevice,\n\u001b[0;32m   1098\u001b[0m     )\n\u001b[0;32m   1099\u001b[0m     hx \u001b[38;5;241m=\u001b[39m (h_zeros, c_zeros)\n\u001b[1;32m-> 1100\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_forward_args\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_sizes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1101\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1102\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_batched:\n",
      "File \u001b[1;32mc:\\Users\\wistudent\\Documents\\AKI-Trading-Model-Test4\\.venv\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:1000\u001b[0m, in \u001b[0;36mLSTM.check_forward_args\u001b[1;34m(self, input, hidden, batch_sizes)\u001b[0m\n\u001b[0;32m    994\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcheck_forward_args\u001b[39m(\n\u001b[0;32m    995\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    996\u001b[0m     \u001b[38;5;28minput\u001b[39m: Tensor,\n\u001b[0;32m    997\u001b[0m     hidden: Tuple[Tensor, Tensor],  \u001b[38;5;66;03m# type: ignore[override]\u001b[39;00m\n\u001b[0;32m    998\u001b[0m     batch_sizes: Optional[Tensor],\n\u001b[0;32m    999\u001b[0m ):\n\u001b[1;32m-> 1000\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_sizes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1001\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_hidden_size(\n\u001b[0;32m   1002\u001b[0m         hidden[\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m   1003\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_expected_hidden_size(\u001b[38;5;28minput\u001b[39m, batch_sizes),\n\u001b[0;32m   1004\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected hidden[0] size \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1005\u001b[0m     )\n\u001b[0;32m   1006\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_hidden_size(\n\u001b[0;32m   1007\u001b[0m         hidden[\u001b[38;5;241m1\u001b[39m],\n\u001b[0;32m   1008\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_expected_cell_size(\u001b[38;5;28minput\u001b[39m, batch_sizes),\n\u001b[0;32m   1009\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected hidden[1] size \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1010\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\wistudent\\Documents\\AKI-Trading-Model-Test4\\.venv\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:312\u001b[0m, in \u001b[0;36mRNNBase.check_input\u001b[1;34m(self, input, batch_sizes)\u001b[0m\n\u001b[0;32m    308\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    309\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput must have \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexpected_input_dim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m dimensions, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    310\u001b[0m     )\n\u001b[0;32m    311\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_size \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m--> 312\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    313\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput.size(-1) must be equal to input_size. Expected \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    314\u001b[0m     )\n",
      "\u001b[1;31mRuntimeError\u001b[0m: input.size(-1) must be equal to input_size. Expected 20, got 19"
     ]
    }
   ],
   "source": [
    "# Backtesting-Funktion\n",
    "def backtest_model(model, dataloader, scaler_y):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in dataloader:\n",
    "            # Vorhersage\n",
    "            output = model(X_batch)\n",
    "            predictions.extend(output.numpy())\n",
    "            actuals.extend(y_batch.numpy())\n",
    "\n",
    "    # Rücktransformation der Vorhersagen und tatsächlichen Werte\n",
    "    predictions = scaler_y.inverse_transform(np.array(predictions).reshape(-1, 1))\n",
    "    actuals = scaler_y.inverse_transform(np.array(actuals).reshape(-1, 1))\n",
    "\n",
    "    return predictions.flatten(), actuals.flatten()\n",
    "\n",
    "# Backtesting starten\n",
    "def run_backtest(test_df, model_path, seq_size):\n",
    "    # Daten vorverarbeiten\n",
    "    scaler_X = MinMaxScaler()\n",
    "    scaler_y = MinMaxScaler()\n",
    "    train_data = pd.read_pickle(train_data_path)\n",
    "    train_data = train_data[['^GDAXI_Open', '^GDAXI_High', '^GDAXI_Low', '^GDAXI_Close',\n",
    "                                '^GDAXI_Adj Close', '^GDAXI_Volume', '^GDAXI_month', '^GDAXI_weekday',\n",
    "                                'GC=F_Open', 'GC=F_High', 'GC=F_Low', 'GC=F_Close',\n",
    "                                'GC=F_Adj Close', 'GC=F_Volume',\n",
    "                                'BZ=F_Open', 'BZ=F_High', 'BZ=F_Low', 'BZ=F_Close',\n",
    "                                'BZ=F_Adj Close', 'BZ=F_Volume',\n",
    "                                'Y']]    \n",
    "    scaler_X.fit(train_data.iloc[:, 1:-1].values)\n",
    "    scaler_y.fit(train_data.iloc[:, -1].values.reshape(-1, 1))\n",
    "\n",
    "    X_test = test_df.iloc[:, 1:-1]\n",
    "    y_test = test_df.iloc[:, -1]\n",
    "\n",
    "    X_test_scaled = scaler_X.transform(X_test.values)\n",
    "    y_test_scaled = scaler_y.transform(y_test.values.reshape(-1, 1))\n",
    "\n",
    "    # Sequenzen erstellen\n",
    "    test_dataset = FinanceDataset(X_test_scaled, y_test_scaled, seq_size)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "    # Modell laden\n",
    "    model = Net(input_size=input_size, output_size=output_size, hidden_size=hidden_size, num_layers=num_layers)\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.eval()\n",
    "\n",
    "    # Backtesting durchführen\n",
    "    predictions, actuals = backtest_model(model, test_dataloader, scaler_y)\n",
    "\n",
    "    # Ergebnisse visualisieren\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    plt.plot(predictions, label=\"Predicted\", color=\"blue\")\n",
    "    plt.plot(actuals, label=\"Actual\", color=\"orange\")\n",
    "    plt.title(\"Backtesting Results\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Statistiken berechnen\n",
    "    df_results = pd.DataFrame({\"Actual\": actuals, \"Predicted\": predictions})\n",
    "    mse = ((df_results[\"Actual\"] - df_results[\"Predicted\"]) ** 2).mean()\n",
    "    mae = np.abs(df_results[\"Actual\"] - df_results[\"Predicted\"]).mean()\n",
    "    print(f\"Mean Squared Error (MSE): {mse:.2f}\")\n",
    "    print(f\"Mean Absolute Error (MAE): {mae:.2f}\")\n",
    "\n",
    "    return df_results\n",
    "\n",
    "# Anwendung der Backtesting-Funktion\n",
    "test_data = pd.read_pickle(test_data_path)  # Testdatensatz laden\n",
    "test_data = test_data[['^GDAXI_Open', '^GDAXI_High', '^GDAXI_Low', '^GDAXI_Close',\n",
    "                                '^GDAXI_Adj Close', '^GDAXI_Volume', '^GDAXI_month', '^GDAXI_weekday',\n",
    "                                'GC=F_Open', 'GC=F_High', 'GC=F_Low', 'GC=F_Close',\n",
    "                                'GC=F_Adj Close', 'GC=F_Volume',\n",
    "                                'BZ=F_Open', 'BZ=F_High', 'BZ=F_Low', 'BZ=F_Close',\n",
    "                                'BZ=F_Adj Close', 'BZ=F_Volume',\n",
    "                                'Y']]\n",
    "model_path = \"../Models/best_model.pt\"  # Pfad zum gespeicherten Modell\n",
    "\n",
    "results = run_backtest(test_data, model_path, seq_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
