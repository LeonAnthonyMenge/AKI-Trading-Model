{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-01T10:24:56.257526Z",
     "start_time": "2024-11-01T10:24:56.223997Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from backtesting import Backtest\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import os\n",
    "from datetime import datetime\n",
    "from lumibot.brokers import Alpaca\n",
    "from lumibot.backtesting import YahooDataBacktesting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-01T10:17:07.914668Z",
     "start_time": "2024-11-01T10:17:07.913939Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-01T10:17:07.916204Z",
     "start_time": "2024-11-01T10:17:07.914992Z"
    }
   },
   "outputs": [],
   "source": [
    "# Model parameter\n",
    "input_size = 25\n",
    "output_size = 1\n",
    "hidden_size = 10\n",
    "num_layers = 2\n",
    "dropout = 0.2\n",
    "\n",
    "# Training parameter\n",
    "batch_size = 16\n",
    "num_epochs = 0\n",
    "learning_rate = 0.001\n",
    "seq_size = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-11-01T10:17:07.916045Z"
    }
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, output_size, hidden_size, num_layers):\n",
    "        super(Net, self).__init__()\n",
    "        self.layer_1 = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "        self.layer_2 = nn.LSTM(hidden_size, hidden_size, batch_first=True, dropout=dropout, num_layers=num_layers)\n",
    "        self.output_layer = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, _ = self.layer_1(x)  \n",
    "        x = torch.relu(x)\n",
    "\n",
    "        x, _ = self.layer_2(x)\n",
    "        x = torch.relu(x)\n",
    "\n",
    "        x = self.output_layer(x[:, -1, :])  \n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-01T10:17:07.918741Z",
     "start_time": "2024-11-01T10:17:07.916968Z"
    }
   },
   "outputs": [],
   "source": [
    "class FinanceDataset(Dataset):\n",
    "    def __init__(self, data_path, seq_size):\n",
    "        self.data = pd.read_pickle(data_path)\n",
    "        print(self.data)\n",
    "        self.seq_size = seq_size\n",
    "        \n",
    "        self.data = self.data[['^GDAXI_Open', '^GDAXI_High', '^GDAXI_Low', '^GDAXI_Close',\n",
    "                                '^GDAXI_Adj Close', '^GDAXI_Volume', '^GDAXI_month', '^GDAXI_weekday',\n",
    "                                'GC=F_Open', 'GC=F_High', 'GC=F_Low', 'GC=F_Close',\n",
    "                                'GC=F_Adj Close', 'GC=F_Volume', 'GC=F_month', 'GC=F_weekday',\n",
    "                                'BZ=F_Open', 'BZ=F_High', 'BZ=F_Low', 'BZ=F_Close',\n",
    "                                'BZ=F_Adj Close', 'BZ=F_Volume', 'BZ=F_month', 'BZ=F_weekday',\n",
    "                                '^GDAXI_trend', 'Invest']]\n",
    "\n",
    "        # Entfernen der 'Date'-Spalte für Inputs\n",
    "        self.inputs = self.data.drop(columns=['Invest']).values\n",
    "        self.labels = self.data['Invest'].values  # Die Label-Spalte ist hier 'Invest'\n",
    "\n",
    "        # Skalieren der Eingabedaten\n",
    "        scaler = MinMaxScaler()\n",
    "        self.inputs = scaler.fit_transform(self.inputs)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs) - self.seq_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Slice für Sequenzeingaben und das entsprechende Label\n",
    "        x = self.inputs[idx:idx + self.seq_size]\n",
    "        y = self.labels[idx + self.seq_size]\n",
    "\n",
    "        # In Tensoren umwandeln\n",
    "        x = torch.tensor(x, dtype=torch.float32)\n",
    "        y = torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-11-01T10:17:07.918078Z"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize model, loss function, optimizer\n",
    "net = Net(input_size, output_size, hidden_size, num_layers)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-01T10:17:07.919235Z",
     "start_time": "2024-11-01T10:17:07.919001Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Date   ^GDAXI_Open   ^GDAXI_High    ^GDAXI_Low  ^GDAXI_Close  \\\n",
      "0    2015-01-02   9869.129883   9879.530273   9687.259766   9764.730469   \n",
      "1    2015-01-05   9735.650391   9790.269531   9468.580078   9473.160156   \n",
      "2    2015-01-06   9484.250000   9624.650391   9382.820312   9469.660156   \n",
      "3    2015-01-07   9510.339844   9592.370117   9459.179688   9518.179688   \n",
      "4    2015-01-08   9643.769531   9855.429688   9607.900391   9837.610352   \n",
      "...         ...           ...           ...           ...           ...   \n",
      "1970 2022-12-23  13945.589844  14000.679688  13874.500000  13940.929688   \n",
      "1971 2022-12-27  14047.419922  14063.139648  13966.349609  13995.099609   \n",
      "1972 2022-12-28  14013.719727  14018.469727  13914.620117  13925.599609   \n",
      "1973 2022-12-29  13890.809570  14071.719727  13871.320312  14071.719727   \n",
      "1974 2022-12-30  14005.839844  14008.969727  13922.549805  13923.589844   \n",
      "\n",
      "      ^GDAXI_Adj Close  ^GDAXI_Volume  ^GDAXI_month  ^GDAXI_weekday Symbol_x  \\\n",
      "0          9764.730469       67673900             1               4   ^GDAXI   \n",
      "1          9473.160156      105538300             1               0   ^GDAXI   \n",
      "2          9469.660156       96812300             1               1   ^GDAXI   \n",
      "3          9518.179688       82466600             1               2   ^GDAXI   \n",
      "4          9837.610352      114825000             1               3   ^GDAXI   \n",
      "...                ...            ...           ...             ...      ...   \n",
      "1970      13940.929688       28738700            12               4   ^GDAXI   \n",
      "1971      13995.099609       22975000            12               1   ^GDAXI   \n",
      "1972      13925.599609       27583800            12               2   ^GDAXI   \n",
      "1973      14071.719727       30727400            12               3   ^GDAXI   \n",
      "1974      13923.589844       24346600            12               4   ^GDAXI   \n",
      "\n",
      "      ...  BZ=F_High   BZ=F_Low  BZ=F_Close  BZ=F_Adj Close  BZ=F_Volume  \\\n",
      "0     ...  58.220001  55.520000   56.419998       56.419998        16707   \n",
      "1     ...  56.290001  52.669998   53.110001       53.110001        30065   \n",
      "2     ...  53.520000  50.529999   51.099998       51.099998        35494   \n",
      "3     ...  51.840000  49.680000   51.150002       51.150002        37082   \n",
      "4     ...  51.889999  49.820000   50.959999       50.959999        29469   \n",
      "...   ...        ...        ...         ...             ...          ...   \n",
      "1970  ...  84.370003  81.339996   83.919998       83.919998         8621   \n",
      "1971  ...  85.669998  83.660004   84.330002       84.330002         7512   \n",
      "1972  ...  84.639999  81.949997   83.260002       83.260002         5384   \n",
      "1973  ...  82.910004  81.300003   82.260002       82.260002        20599   \n",
      "1974  ...  86.279999  82.879997   85.910004       85.910004        19375   \n",
      "\n",
      "      BZ=F_month  BZ=F_weekday  Symbol ^GDAXI_trend  Invest  \n",
      "0              1             4    BZ=F           -1   False  \n",
      "1              1             0    BZ=F           -1   False  \n",
      "2              1             1    BZ=F           -1    True  \n",
      "3              1             2    BZ=F            1    True  \n",
      "4              1             3    BZ=F            1   False  \n",
      "...          ...           ...     ...          ...     ...  \n",
      "1970          12             4    BZ=F           -1   False  \n",
      "1971          12             1    BZ=F           -1   False  \n",
      "1972          12             2    BZ=F           -1    True  \n",
      "1973          12             3    BZ=F            1   False  \n",
      "1974          12             4    BZ=F           -1    True  \n",
      "\n",
      "[1975 rows x 30 columns]\n"
     ]
    }
   ],
   "source": [
    "# Initialize dataset and dataloader\n",
    "dataset = FinanceDataset('../Data/train_dax_data.pkl', seq_size=seq_size)\n",
    "train_loader = DataLoader(dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-01T10:17:07.922014Z",
     "start_time": "2024-11-01T10:17:07.920173Z"
    }
   },
   "outputs": [],
   "source": [
    "os.makedirs(\"../Models\", exist_ok=True)\n",
    "losses = []\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0\n",
    "    for inputs, labels in train_loader:\n",
    "        #print(input)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs.float())\n",
    "        loss = criterion(torch.squeeze(outputs), labels.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # Save model after each epoch\n",
    "    model_path = f'../Models/model-{epoch + 1}.pt'\n",
    "    torch.save(net.state_dict(), model_path)\n",
    "\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        learning_rate *= 0.8\n",
    "        optimizer = optim.SGD(net.parameters(), lr=learning_rate)\n",
    "\n",
    "    print(f'Epoch {epoch + 1}, Loss: {running_loss / len(train_loader)}, {running_loss}, {train_loader}')\n",
    "    losses.append(running_loss / len(train_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backtesting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-01T10:17:07.928733Z",
     "start_time": "2024-11-01T10:17:07.924953Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "0\n"
     ]
    },
    {
     "ename": "UnpicklingError",
     "evalue": "invalid load key, '\\x00'.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnpicklingError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 32\u001b[0m\n\u001b[1;32m     29\u001b[0m model \u001b[38;5;241m=\u001b[39m Net(input_size, output_size, hidden_size, num_layers)\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# Load state_dict only\u001b[39;00m\n\u001b[0;32m---> 32\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m)\n\u001b[1;32m     33\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m     35\u001b[0m ALPACA_CREDS \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAPI_KEY\u001b[39m\u001b[38;5;124m\"\u001b[39m: os\u001b[38;5;241m.\u001b[39mgetenv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mALPACA_API_KEY\u001b[39m\u001b[38;5;124m\"\u001b[39m), \n\u001b[1;32m     37\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAPI_SECRET\u001b[39m\u001b[38;5;124m\"\u001b[39m: os\u001b[38;5;241m.\u001b[39mgetenv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mALPACA_API_SECRET\u001b[39m\u001b[38;5;124m\"\u001b[39m), \n\u001b[1;32m     38\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPAPER\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     39\u001b[0m }\n",
      "File \u001b[0;32m~/Desktop/HTW/6 Semester/AKI-Trading-Model/.venv/lib/python3.11/site-packages/torch/serialization.py:1384\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1382\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1383\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError(_get_wo_message(\u001b[38;5;28mstr\u001b[39m(e))) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1384\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_legacy_load\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1385\u001b[0m \u001b[43m    \u001b[49m\u001b[43mopened_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpickle_load_args\u001b[49m\n\u001b[1;32m   1386\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/HTW/6 Semester/AKI-Trading-Model/.venv/lib/python3.11/site-packages/torch/serialization.py:1628\u001b[0m, in \u001b[0;36m_legacy_load\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1621\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(f, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreadinto\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m sys\u001b[38;5;241m.\u001b[39mversion_info \u001b[38;5;241m<\u001b[39m (\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m2\u001b[39m):\n\u001b[1;32m   1622\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1623\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch.load does not work with file-like objects that do not implement readinto on Python 3.8.0 and 3.8.1. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1624\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mReceived object of type \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(f)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m. Please update to Python 3.8.2 or newer to restore this \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   1625\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunctionality.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1626\u001b[0m     )\n\u001b[0;32m-> 1628\u001b[0m magic_number \u001b[38;5;241m=\u001b[39m \u001b[43mpickle_module\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpickle_load_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1629\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m magic_number \u001b[38;5;241m!=\u001b[39m MAGIC_NUMBER:\n\u001b[1;32m   1630\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid magic number; corrupt file?\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mUnpicklingError\u001b[0m: invalid load key, '\\x00'."
     ]
    }
   ],
   "source": [
    "os.makedirs(\"logs\", exist_ok=True)\n",
    "os.makedirs(\"results\", exist_ok=True)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "df = pd.read_pickle('../Data/train_dax_data.pkl').reset_index()\n",
    "df = df[['^GDAXI_Open', '^GDAXI_High', '^GDAXI_Low', '^GDAXI_Close',\n",
    "                                '^GDAXI_Adj Close', '^GDAXI_Volume', '^GDAXI_month', '^GDAXI_weekday',\n",
    "                                'GC=F_Open', 'GC=F_High', 'GC=F_Low', 'GC=F_Close',\n",
    "                                'GC=F_Adj Close', 'GC=F_Volume', 'GC=F_month', 'GC=F_weekday',\n",
    "                                'BZ=F_Open', 'BZ=F_High', 'BZ=F_Low', 'BZ=F_Close',\n",
    "                                'BZ=F_Adj Close', 'BZ=F_Volume', 'BZ=F_month', 'BZ=F_weekday',\n",
    "                                '^GDAXI_trend']]\n",
    "scaler.fit(df.values)\n",
    "\n",
    "models = [f for f in os.listdir(\"/Users/danielstorch/Desktop/HTW/6 Semester/AKI-Trading-Model/Test4-Raw Material/Code/Models\")]\n",
    "alredy_done = [f.removeprefix(\"backtest-\").removesuffix(\".csv.gz\") for f in os.listdir(\"results/\")]\n",
    "print(len(models))\n",
    "print(len(alredy_done))\n",
    "\n",
    "test_data = pd.read_pickle(\"../Data/test_dax_data.pkl\")\n",
    "\n",
    "for model_name in models:\n",
    "    # Skip Backtesting for Models which already did the Backtesting\n",
    "    if model_name in alredy_done:\n",
    "        print(f\"Skip model: {model_name}\")\n",
    "        continue\n",
    "\n",
    "    model_path = f\"/Users/danielstorch/Desktop/HTW/6 Semester/AKI-Trading-Model/Test4-Raw Material/Code/Models/{model_name}\"\n",
    "    model = Net(input_size, output_size, hidden_size, num_layers)\n",
    "\n",
    "    # Load state_dict only\n",
    "    model.load_state_dict(torch.load(model_path, weights_only=False))\n",
    "    model.eval()\n",
    "\n",
    "    ALPACA_CREDS = {\n",
    "        \"API_KEY\": os.getenv(\"ALPACA_API_KEY\"), \n",
    "        \"API_SECRET\": os.getenv(\"ALPACA_API_SECRET\"), \n",
    "        \"PAPER\": True\n",
    "    }\n",
    "\n",
    "    # Strategy setup\n",
    "    start_date = datetime(2023, 1, 1)\n",
    "    end_date = datetime(2023, 12, 31)\n",
    "    broker = Alpaca(ALPACA_CREDS)\n",
    "\n",
    "    # Instantiate and run the strategy\n",
    "    strategy = Backtest(\n",
    "        name=model_name,  \n",
    "        broker=broker,\n",
    "        parameters={\n",
    "            \"symbol\": \"SPY\",\n",
    "            \"cash_at_risk\": 0.5,\n",
    "            \"model\": model,\n",
    "            \"num_prior_days\": 30,\n",
    "            \"dataset\": test_data,\n",
    "            \"scaler\": scaler,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Run the backtest\n",
    "    backtest_results = strategy.backtest(\n",
    "        YahooDataBacktesting,\n",
    "        start_date,\n",
    "        end_date,\n",
    "        name=model_name,\n",
    "        parameters={\n",
    "            \"symbol\": \"SPY\", \n",
    "            \"cash_at_risk\": 0.5, \n",
    "            \"model\": model,  \n",
    "            \"dataset\": test_data,\n",
    "            \"num_prior_days\": 30,\n",
    "            \"scaler\": scaler,\n",
    "        },\n",
    "        benchmark_asset=\"SPY\",\n",
    "        show_plot=True,\n",
    "        show_tearsheet=True\n",
    "    )\n",
    "\n",
    "    # Convert to DataFrame only if results are non-empty\n",
    "    backtest_results = pd.DataFrame(backtest_results)   \n",
    "    backtest_results[\"model\"] = model_name\n",
    "    \n",
    "    backtest_results.to_csv(f\"results/backtest-{model_name}.csv.gz\", index=False, compression='gzip')\n",
    "    \n",
    "print(\"Backtesting complete. Results saved to backtest_results.csv.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"results.csv\")\n",
    "display(df.sort_values(by=[\"model\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "best_model = df[df[\"total_return\"] == df[\"total_return\"].max()]\n",
    "best_model.reset_index(inplace=True)\n",
    "\n",
    "model_name = best_model.at[0, \"model\"]\n",
    "print(f\"Best model: {model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-11-01T10:17:07.921380Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "\n",
    "model_path = f\"../Models/{model_name}\"\n",
    "model = Net(input_size, output_size, hidden_size, num_layers)\n",
    "\n",
    "# Load state_dict only\n",
    "model.load_state_dict(torch.load(model_path))  # Do not use weights_only\n",
    "model.eval()\n",
    "\n",
    "# Load your DataFrame\n",
    "data = pd.read_pickle('../Data/test_dax_data.pkl')\n",
    "\n",
    "test_data = FinanceDataset('../Data/test_dax_data.pkl', seq_size=seq_size)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size)\n",
    "# Select all rows and columns from index 1 to -1 (exclusive)\n",
    "\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        out = model(inputs) \n",
    "        # Store predictions and labels\n",
    "        all_predictions.append(out.numpy())  \n",
    "        all_labels.append(labels.numpy())\n",
    "\n",
    "# Concatenate results\n",
    "all_predictions = np.concatenate(all_predictions)\n",
    "all_labels = np.concatenate(all_labels)\n",
    "\n",
    "# Print or analyze the predictions\n",
    "print(f'Predicted values: {all_predictions.flatten()}')\n",
    "print(f'Actual values: {all_labels.flatten()}')\n",
    "\n",
    "output_df = pd.DataFrame({'Predicted': all_predictions.flatten(), 'Actual': all_labels.flatten()})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-01T10:17:07.924515Z",
     "start_time": "2024-11-01T10:17:07.922162Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "threshold = 0.5\n",
    "predicted_classes = (all_predictions.flatten() > threshold).astype(int)\n",
    "\n",
    "accuracy = accuracy_score(all_labels.flatten(), predicted_classes)\n",
    "precision = precision_score(all_labels.flatten(), predicted_classes)\n",
    "recall = recall_score(all_labels.flatten(), predicted_classes)\n",
    "f1 = f1_score(all_labels.flatten(), predicted_classes)\n",
    "\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "print(f'Precision: {precision:.2f}')\n",
    "print(f'Recall: {recall:.2f}')\n",
    "print(f'F1 Score: {f1:.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-11-01T10:17:07.922825Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cm = confusion_matrix(all_labels.flatten(), predicted_classes)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-11-01T10:17:07.923504Z"
    }
   },
   "outputs": [],
   "source": [
    "output_df = pd.DataFrame({\n",
    "    'Predicted': all_predictions.flatten(),\n",
    "    'Predicted_Class': predicted_classes,\n",
    "    'Actual': all_labels.flatten()\n",
    "})\n",
    "correlation_matrix = output_df.corr()\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()\n",
    "display(output_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
