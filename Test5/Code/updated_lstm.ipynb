{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-01T10:24:56.257526Z",
     "start_time": "2024-11-01T10:24:56.223997Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Normalize the data\n",
    "scaler_X = MinMaxScaler()\n",
    "scaler_y = MinMaxScaler()\n",
    "\n",
    "df = pd.read_pickle(\"../Data/train_dax_data.pkl\")\n",
    "df[\"Y_scaled\"] = scaler_y.fit_transform(df[\"Y\"].values.reshape(-1, 1))\n",
    "X = scaler_X.fit_transform(df.iloc[:, 2:-2])  # Exclude 'Y' and unnecessary columns\n",
    "\n",
    "# Create sequences\n",
    "def create_sequences(data, labels, seq_length):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - seq_length):\n",
    "        X.append(data[i:i + seq_length])\n",
    "        y.append(labels[i + seq_length])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "seq_size = 30\n",
    "X_sequences, y_sequences = create_sequences(X, df[\"Y_scaled\"].values, seq_size)\n",
    "\n",
    "# Dataset and DataLoader\n",
    "class FinanceDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.X[idx], dtype=torch.float32), torch.tensor(self.y[idx], dtype=torch.float32)\n",
    "\n",
    "dataset = FinanceDataset(X_sequences, y_sequences)\n",
    "train_loader = DataLoader(dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "# Define the LSTM model\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_size, num_layers, dropout=0.2):\n",
    "        super(Net, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, _ = self.lstm(x)\n",
    "        x = self.fc(x[:, -1, :])  # Output from the last timestep\n",
    "        return x\n",
    "\n",
    "# Initialize model, loss function, and optimizer\n",
    "input_size = X.shape[1]\n",
    "output_size = 1\n",
    "hidden_size = 100\n",
    "num_layers = 2\n",
    "\n",
    "model = Net(input_size, output_size, hidden_size, num_layers)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 20\n",
    "losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs.squeeze(-1), labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    epoch_loss /= len(train_loader)\n",
    "    losses.append(epoch_loss)\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "# Plot training loss\n",
    "plt.plot(losses, label='Training Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Save the model\n",
    "torch.save(model.state_dict(), \"../Models/best_model.pt\")\n",
    "\n",
    "# Evaluate the model\n",
    "model.eval()\n",
    "test_df = pd.read_pickle(\"../Data/test_dax_data.pkl\")\n",
    "test_X = scaler_X.transform(test_df.iloc[:, :-2])  # Normalize test data\n",
    "test_y = scaler_y.transform(test_df[\"Y\"].values.reshape(-1, 1))\n",
    "\n",
    "test_sequences, test_labels = create_sequences(test_X, test_y.flatten(), seq_size)\n",
    "test_dataset = FinanceDataset(test_sequences, test_labels)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16)\n",
    "\n",
    "predictions, actuals = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        outputs = model(inputs)\n",
    "        predictions.append(outputs.numpy())\n",
    "        actuals.append(labels.numpy())\n",
    "\n",
    "predictions = scaler_y.inverse_transform(np.concatenate(predictions))\n",
    "actuals = scaler_y.inverse_transform(np.concatenate(actuals))\n",
    "\n",
    "# Plot predictions vs actuals\n",
    "plt.plot(actuals, label='Actual')\n",
    "plt.plot(predictions, label='Predicted')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-01T10:17:07.914668Z",
     "start_time": "2024-11-01T10:17:07.913939Z"
    }
   },
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "device = \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-01T10:17:07.916204Z",
     "start_time": "2024-11-01T10:17:07.914992Z"
    }
   },
   "outputs": [],
   "source": [
    "# Model parameter\n",
    "input_size = 8\n",
    "output_size = 1\n",
    "hidden_size = 100\n",
    "num_layers = 3\n",
    "dropout = 0.2\n",
    "\n",
    "# Training parameter\n",
    "batch_size = 16\n",
    "num_epochs = 10\n",
    "learning_rate = 0.001\n",
    "seq_size = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-11-01T10:17:07.916045Z"
    }
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_size, num_layers, dropout=0.2):\n",
    "        super(Net, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, _ = self.lstm(x)\n",
    "        x = self.fc(x[:, -1, :])  # Output from the last timestep\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-01T10:17:07.918741Z",
     "start_time": "2024-11-01T10:17:07.916968Z"
    }
   },
   "outputs": [],
   "source": [
    "class FinanceDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.X[idx], dtype=torch.float32), torch.tensor(self.y[idx], dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-11-01T10:17:07.918078Z"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize model, loss function, optimizer\n",
    "net = Net(input_size, output_size, hidden_size, num_layers)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-01T10:17:07.919235Z",
     "start_time": "2024-11-01T10:17:07.919001Z"
    }
   },
   "outputs": [],
   "source": [
    "# Train\n",
    "# Normalize the data\n",
    "scaler_X = MinMaxScaler()\n",
    "scaler_y = MinMaxScaler()\n",
    "\n",
    "df = pd.read_pickle(\"../Data/train_dax_data.pkl\")\n",
    "df[\"Y_scaled\"] = scaler_y.fit_transform(df[\"Y\"].values.reshape(-1, 1))\n",
    "X = scaler_X.fit_transform(df.iloc[:, :-2])  # Exclude 'Y' and unnecessary columns\n",
    "\n",
    "\n",
    "\n",
    "df = pd.read_pickle(\"../Data/train_dax_data.pkl\")\n",
    "if \"Date\" in df.columns:\n",
    "    df = df.drop(\"Date\", axis=1)\n",
    "if \"index\" in df.columns:\n",
    "    df = df.drop(\"index\", axis=1)\n",
    "    \n",
    "display(df)\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(df.iloc[:, :-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "test_df = pd.read_pickle(\"../Data/test_dax_data.pkl\")\n",
    "if \"Date\" in test_df.columns:\n",
    "    test_df = test_df.drop(\"Date\", axis=1)\n",
    "if \"index\" in test_df.columns:\n",
    "    test_df = test_df.drop(\"index\", axis=1)\n",
    "\n",
    "display(test_df)\n",
    "    \n",
    "labels_test = test_df.iloc[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize dataset and dataloader\n",
    "dataset = FinanceDataset(df, scaler, seq_size=seq_size)\n",
    "train_loader = DataLoader(dataset, batch_size=batch_size)\n",
    "\n",
    "test_dataset = FinanceDataset(test_df, scaler, seq_size=seq_size)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-01T10:17:07.922014Z",
     "start_time": "2024-11-01T10:17:07.920173Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Sicherstellen, dass das Modellverzeichnis existiert\n",
    "os.makedirs(\"../Models\", exist_ok=True)\n",
    "\n",
    "best_model_path = \"../Models/best_model.pt\"\n",
    "\n",
    "if os.path.exists(best_model_path):\n",
    "    net.load_state_dict(torch.load(best_model_path))\n",
    "    print(f\"Modell erfolgreich geladen von {best_model_path}\")\n",
    "\n",
    "\n",
    "# Parameter für Early Stopping und Modell-Speicherung\n",
    "patience = 5 \n",
    "best_test_loss = float('inf')\n",
    "early_stopping_counter = 0\n",
    "\n",
    "losses = []\n",
    "test_loss_vals = []\n",
    "learning_rate = 0.001  # Initiale Lernrate\n",
    "\n",
    "# Training loop mit Early Stopping\n",
    "for epoch in range(num_epochs):\n",
    "    net.train()\n",
    "    running_loss = 0\n",
    "\n",
    "    # Training\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)  # Auf das Gerät verschieben\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs.float())\n",
    "        loss = criterion(torch.squeeze(outputs), labels.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = running_loss / len(train_loader)\n",
    "    losses.append(avg_train_loss)\n",
    "\n",
    "    # Modell auswerten auf dem Testset\n",
    "    net.eval()\n",
    "    running_test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        # Iteriere über den Testloader\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)  # Auf das Gerät verschieben\n",
    "            test_outputs = net(inputs)\n",
    "            test_outputs = test_outputs.squeeze(-1)  # Entferne unnötige Dimension\n",
    "            test_loss = criterion(test_outputs, labels)\n",
    "            running_test_loss += test_loss.item()\n",
    "        \n",
    "    avg_test_loss = running_test_loss / len(test_loader)\n",
    "    test_loss_vals.append(avg_test_loss)\n",
    "\n",
    "    # Check for Early Stopping und Speichern des besten Modells\n",
    "    if avg_test_loss <= best_test_loss:\n",
    "        best_test_loss = avg_test_loss\n",
    "        best_model_path = '../Models/best_model.pt'\n",
    "        torch.save(net.state_dict(), best_model_path)\n",
    "        early_stopping_counter = 0\n",
    "    else:\n",
    "        early_stopping_counter += 1\n",
    "\n",
    "    # Ausgabe der Verluste\n",
    "    print(f'Epoch {epoch + 1}, Train Loss: {avg_train_loss:.4f}, Test Loss: {avg_test_loss:.4f}')\n",
    "    \n",
    "    # Early Stopping-Kriterium prüfen\n",
    "    if early_stopping_counter >= patience:\n",
    "        print(f\"Early stopping after {epoch + 1} epochs. Test loss did not improve for {patience} epochs.\")\n",
    "        break\n",
    "\n",
    "    # Dynamische Lernratenanpassung alle 10 Epochen\n",
    "    if epoch > 0 and epoch % 10 == 0:\n",
    "        learning_rate *= 0.5\n",
    "        optimizer = optim.SGD(net.parameters(), lr=learning_rate)\n",
    "        print(f\"Reduced learning rate to {learning_rate:.5f}\")\n",
    "\n",
    "print(f\"Training abgeschlossen. Bestes Modell gespeichert unter: {best_model_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backtesting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "df = pd.read_pickle(\"../Data/train_dax_data.pkl\").reset_index().iloc[:, :-1]\n",
    "if \"Date\" in test_df.columns:\n",
    "    test_df = test_df.drop(\"Date\", axis=1)\n",
    "if \"index\" in test_df.columns:\n",
    "    test_df = test_df.drop(\"index\", axis=1)\n",
    "df = df[[\"Open\", \"High\", \"Low\", \"Close\", \"Adj Close\", \"Volume\", \"month\", \"weekday\"]]\n",
    "display(df)\n",
    "scaler.fit(df.values)\n",
    "\n",
    "model_path = \"../Models/best_model.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-11-01T10:17:07.921380Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "\n",
    "model = Net(input_size, output_size, hidden_size, num_layers)\n",
    "\n",
    "# Load state_dict only\n",
    "model.load_state_dict(torch.load(model_path)) \n",
    "model.eval()\n",
    "\n",
    "df = pd.read_pickle('../Data/train_dax_data.pkl')\n",
    "df = df[[\"Open\", \"High\", \"Low\", \"Close\", \"Adj Close\", \"Volume\", \"month\", \"weekday\", \"Y\"]]\n",
    "\n",
    "\n",
    "test_data = FinanceDataset(df, scaler, seq_size=seq_size)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        out = model(inputs) \n",
    "        \n",
    "        all_predictions.append(out.numpy())  \n",
    "        all_labels.append(labels.numpy())\n",
    "\n",
    "all_predictions = np.concatenate(all_predictions)\n",
    "all_labels = np.concatenate(all_labels)\n",
    "\n",
    "print(f'Predicted values: {all_predictions.flatten()}')\n",
    "print(f'Actual values: {all_labels.flatten()}')\n",
    "\n",
    "output_df = pd.DataFrame({'Predicted': all_predictions.flatten(), 'Actual': all_labels.flatten()})\n",
    "display(output_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-01T10:17:07.924515Z",
     "start_time": "2024-11-01T10:17:07.922162Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "threshold = 0.5\n",
    "predicted_classes = (all_predictions.flatten() > threshold).astype(int)\n",
    "\n",
    "accuracy = accuracy_score(all_labels.flatten(), predicted_classes)\n",
    "precision = precision_score(all_labels.flatten(), predicted_classes)\n",
    "recall = recall_score(all_labels.flatten(), predicted_classes)\n",
    "f1 = f1_score(all_labels.flatten(), predicted_classes)\n",
    "\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "print(f'Precision: {precision:.2f}')\n",
    "print(f'Recall: {recall:.2f}')\n",
    "print(f'F1 Score: {f1:.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-11-01T10:17:07.922825Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cm = confusion_matrix(all_labels.flatten(), predicted_classes)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-11-01T10:17:07.923504Z"
    }
   },
   "outputs": [],
   "source": [
    "output_df = pd.DataFrame({\n",
    "    'Predicted': all_predictions.flatten(),\n",
    "    'Predicted_Class': predicted_classes,\n",
    "    'Actual': all_labels.flatten()\n",
    "})\n",
    "correlation_matrix = output_df.corr()\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()\n",
    "display(output_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"logs\", exist_ok=True)\n",
    "os.makedirs(\"results\", exist_ok=True)\n",
    "\n",
    "test_data = pd.read_pickle(\"../Data/test_dax_data.pkl\")\n",
    "\n",
    "print(test_data.dtypes)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(test_data.iloc[:, 2:-1])\n",
    "\n",
    "input_size = 8\n",
    "output_size = 1\n",
    "hidden_size = 100\n",
    "num_layers = 2\n",
    "\n",
    "model = Net(input_size, output_size, hidden_size, num_layers)\n",
    "model_path = \"../Models/best_model.pt\"\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "model.eval()\n",
    "\n",
    "ALPACA_CREDS = {\n",
    "    \"API_KEY\": os.getenv(\"ALPACA_API_KEY\"),\n",
    "    \"API_SECRET\": os.getenv(\"ALPACA_API_SECRET\"),\n",
    "    \"PAPER\": True,\n",
    "}\n",
    "\n",
    "# Strategy setup\n",
    "start_date = datetime(2023, 1, 1)\n",
    "end_date = datetime(2023, 12, 31)\n",
    "broker = Alpaca(ALPACA_CREDS)\n",
    "\n",
    "strategy = Backtest(\n",
    "    name=\"Test2.1\",\n",
    "    broker=broker,\n",
    "    parameters={\n",
    "        \"symbol\": \"^GDAXI\",\n",
    "        \"cash_at_risk\": 0.5,\n",
    "        \"model\": model,\n",
    "        \"num_prior_days\": 30,\n",
    "        \"dataset\": test_data,\n",
    "        \"scaler\": scaler,\n",
    "    },\n",
    ")\n",
    "\n",
    "# Run backtest\n",
    "backtest_results = strategy.backtest(\n",
    "    YahooDataBacktesting,\n",
    "    start_date,\n",
    "    end_date,\n",
    "    name=\"Test2.1\",\n",
    "    parameters={\n",
    "        \"symbol\": \"^GDAXI\",\n",
    "        \"cash_at_risk\": 0.5,\n",
    "        \"model\": model,\n",
    "        \"dataset\": test_data,\n",
    "        \"num_prior_days\": 30,\n",
    "        \"scaler\": scaler,\n",
    "    },\n",
    "    benchmark_asset=\"SPY\",\n",
    "    show_plot=True,\n",
    "    show_tearsheet=True,\n",
    ")\n",
    "\n",
    "# Save results\n",
    "backtest_results.to_csv(\"results/backtest_results.csv.gz\", index=False, compression=\"gzip\")\n",
    "\n",
    "print(\"Backtesting complete. Results saved to backtest_results.csv.gz.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
