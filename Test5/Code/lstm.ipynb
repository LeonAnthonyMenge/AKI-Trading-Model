{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-01T10:24:56.257526Z",
     "start_time": "2024-11-01T10:24:56.223997Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from backtesting import Backtest\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import os\n",
    "from datetime import datetime\n",
    "from lumibot.brokers import Alpaca\n",
    "import matplotlib.pyplot as plt\n",
    "from lumibot.backtesting import YahooDataBacktesting\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-01T10:17:07.914668Z",
     "start_time": "2024-11-01T10:17:07.913939Z"
    }
   },
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "device = \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-01T10:17:07.916204Z",
     "start_time": "2024-11-01T10:17:07.914992Z"
    }
   },
   "outputs": [],
   "source": [
    "# Model parameter\n",
    "input_size = 8\n",
    "output_size = 1\n",
    "hidden_size = 1000\n",
    "num_layers = 6\n",
    "dropout = 0.2\n",
    "\n",
    "# Training parameter\n",
    "batch_size = 16\n",
    "num_epochs = 5\n",
    "learning_rate = 0.001\n",
    "seq_size = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-11-01T10:17:07.916045Z"
    }
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_size, num_layers, dropout=0.2):\n",
    "        super(Net, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, _ = self.lstm(x)\n",
    "        x = self.fc(x[:, -1, :])  # Output from the last timestep\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-01T10:17:07.918741Z",
     "start_time": "2024-11-01T10:17:07.916968Z"
    }
   },
   "outputs": [],
   "source": [
    "class FinanceDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.X[idx], dtype=torch.float32), torch.tensor(self.y[idx], dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(data, labels, seq_length):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - seq_length):\n",
    "        X.append(data[i:i + seq_length])\n",
    "        y.append(labels[i + seq_length])\n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-11-01T10:17:07.918078Z"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize model, loss function, optimizer\n",
    "net = Net(input_size, output_size, hidden_size, num_layers)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-01T10:17:07.919235Z",
     "start_time": "2024-11-01T10:17:07.919001Z"
    }
   },
   "outputs": [],
   "source": [
    "scaler_X = MinMaxScaler()\n",
    "scaler_y = MinMaxScaler()\n",
    "\n",
    "df = pd.read_pickle(\"../Data/train_dax_data.pkl\")\n",
    "df[\"Y_scaled\"] = scaler_y.fit_transform(df[\"Y\"].values.reshape(-1, 1))\n",
    "print(df.dtypes)\n",
    "X = scaler_X.fit_transform(df.iloc[:, 2:-2])\n",
    "\n",
    "seq_size = 30\n",
    "X_sequences, y_sequences = create_sequences(X, df[\"Y_scaled\"].values, seq_size)\n",
    "\n",
    "dataset = FinanceDataset(X_sequences, y_sequences)\n",
    "train_loader = DataLoader(dataset, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-01T10:17:07.922014Z",
     "start_time": "2024-11-01T10:17:07.920173Z"
    }
   },
   "outputs": [],
   "source": [
    "losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    net.train()\n",
    "    epoch_loss = 0\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs.squeeze(-1), labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    epoch_loss /= len(train_loader)\n",
    "    losses.append(epoch_loss)\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "# Plot training loss\n",
    "plt.plot(losses, label='Training Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "torch.save(net.state_dict(), \"../Models/best_model.pt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backtesting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "df = pd.read_pickle(\"../Data/train_dax_data.pkl\").reset_index().iloc[:, :-1]\n",
    "if \"Date\" in df.columns:\n",
    "    df = df.drop(\"Date\", axis=1)\n",
    "if \"index\" in df.columns:\n",
    "    df = df.drop(\"index\", axis=1)\n",
    "df = df[[\"Open\", \"High\", \"Low\", \"Close\", \"Adj Close\", \"Volume\", \"month\", \"weekday\"]]\n",
    "display(df)\n",
    "scaler.fit(df.values)\n",
    "\n",
    "model_path = \"../Models/best_model.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-11-01T10:17:07.921380Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "\n",
    "model = Net(input_size, output_size, hidden_size, num_layers)\n",
    "\n",
    "# Load state_dict only\n",
    "model.load_state_dict(torch.load(model_path)) \n",
    "model.eval()\n",
    "\n",
    "df = pd.read_pickle('../Data/train_dax_data.pkl')\n",
    "df = df[[\"Open\", \"High\", \"Low\", \"Close\", \"Adj Close\", \"Volume\", \"month\", \"weekday\", \"Y\"]]\n",
    "\n",
    "seq_size = 30\n",
    "X_sequences, y_sequences = create_sequences(X, df[\"Y_scaled\"].values, seq_size)\n",
    "\n",
    "dataset = FinanceDataset(X_sequences, y_sequences)\n",
    "\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        out = model(inputs) \n",
    "        \n",
    "        all_predictions.append(out.numpy())  \n",
    "        all_labels.append(labels.numpy())\n",
    "\n",
    "all_predictions = np.concatenate(all_predictions)\n",
    "all_labels = np.concatenate(all_labels)\n",
    "\n",
    "print(f'Predicted values: {all_predictions.flatten()}')\n",
    "print(f'Actual values: {all_labels.flatten()}')\n",
    "\n",
    "output_df = pd.DataFrame({'Predicted': all_predictions.flatten(), 'Actual': all_labels.flatten()})\n",
    "display(output_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"logs\", exist_ok=True)\n",
    "os.makedirs(\"results\", exist_ok=True)\n",
    "\n",
    "test_data = pd.read_pickle(\"../Data/test_dax_data.pkl\")\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(test_data.iloc[:, 2:-1])\n",
    "\n",
    "scaler_y = MinMaxScaler()\n",
    "scaler_y.fit(test_data.iloc[:, -1].values.reshape(-1, 1))\n",
    "\n",
    "model = Net(input_size, output_size, hidden_size, num_layers)\n",
    "model_path = \"../Models/best_model.pt\"\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "model.eval()\n",
    "\n",
    "ALPACA_CREDS = {\n",
    "    \"API_KEY\": os.getenv(\"ALPACA_API_KEY\"),\n",
    "    \"API_SECRET\": os.getenv(\"ALPACA_API_SECRET\"),\n",
    "    \"PAPER\": True,\n",
    "}\n",
    "\n",
    "# Strategy setup\n",
    "start_date = datetime(2023, 1, 1)\n",
    "end_date = datetime(2023, 12, 31)\n",
    "broker = Alpaca(ALPACA_CREDS)\n",
    "\n",
    "strategy = Backtest(\n",
    "    name=\"Test5\",\n",
    "    broker=broker,\n",
    "    parameters={\n",
    "        \"symbol\": \"^GDAXI\",\n",
    "        \"cash_at_risk\": 0.5,\n",
    "        \"model\": model,\n",
    "        \"num_prior_days\": 30,\n",
    "        \"dataset\": test_data,\n",
    "        \"scaler\": scaler,\n",
    "        \"scaler_y\": scaler_y,\n",
    "    },\n",
    ")\n",
    "\n",
    "# Run backtest\n",
    "backtest_results = strategy.backtest(\n",
    "    YahooDataBacktesting,\n",
    "    start_date,\n",
    "    end_date,\n",
    "    name=\"Test5\",\n",
    "    parameters={\n",
    "        \"symbol\": \"^GDAXI\",\n",
    "        \"cash_at_risk\": 0.5,\n",
    "        \"model\": model,\n",
    "        \"dataset\": test_data,\n",
    "        \"num_prior_days\": 30,\n",
    "        \"scaler\": scaler,\n",
    "        \"scaler_y\": scaler_y,\n",
    "    },\n",
    "    benchmark_asset=\"SPY\",\n",
    "    show_plot=True,\n",
    "    show_tearsheet=True,\n",
    ")\n",
    "\n",
    "# Save results\n",
    "pd.DataFrame(backtest_results).to_csv(\"results/backtest_results.csv.gz\", index=False, compression=\"gzip\")\n",
    "\n",
    "print(\"Backtesting complete. Results saved to backtest_results.csv.gz.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Dataset-Klasse für Backtesting\n",
    "class FinanceDataset(Dataset):\n",
    "    def __init__(self, X, y, seq_size):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.seq_size = seq_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X) - self.seq_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (\n",
    "            torch.tensor(self.X[idx:idx + self.seq_size], dtype=torch.float32),\n",
    "            torch.tensor(self.y[idx + self.seq_size], dtype=torch.float32),\n",
    "        )\n",
    "\n",
    "# Backtesting-Funktion\n",
    "def backtest_model(model, dataloader, scaler_y, seq_size):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in dataloader:\n",
    "            # Vorhersage\n",
    "            output = model(X_batch)\n",
    "            predictions.extend(output.numpy())\n",
    "            actuals.extend(y_batch.numpy())\n",
    "\n",
    "    # Rücktransformation der Vorhersagen und tatsächlichen Werte\n",
    "    predictions = scaler_y.inverse_transform(np.array(predictions).reshape(-1, 1))\n",
    "    actuals = scaler_y.inverse_transform(np.array(actuals).reshape(-1, 1))\n",
    "\n",
    "    return predictions.flatten(), actuals.flatten()\n",
    "\n",
    "# Backtesting starten\n",
    "def run_backtest(test_df, model_path, seq_size):\n",
    "    # Daten vorverarbeiten\n",
    "    scaler_X = MinMaxScaler()\n",
    "    scaler_y = MinMaxScaler()\n",
    "\n",
    "    X_test = test_df.iloc[:, 2:-1]\n",
    "    y_test = test_df.iloc[:, -1]\n",
    "\n",
    "    X_test_scaled = scaler_X.fit_transform(X_test)\n",
    "    y_test_scaled = scaler_y.fit_transform(y_test.values.reshape(-1, 1))\n",
    "\n",
    "    # Sequenzen erstellen\n",
    "    test_dataset = FinanceDataset(X_test_scaled, y_test_scaled, seq_size)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "    # Modell laden\n",
    "    model = Net(input_size=X_test.shape[1], output_size=1, hidden_size=100, num_layers=3)\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.eval()\n",
    "\n",
    "    # Backtesting durchführen\n",
    "    predictions, actuals = backtest_model(model, test_dataloader, scaler_y, seq_size)\n",
    "\n",
    "    # Ergebnisse visualisieren\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    plt.plot(predictions, label=\"Predicted\", color=\"blue\")\n",
    "    plt.plot(actuals, label=\"Actual\", color=\"orange\")\n",
    "    plt.title(\"Backtesting Results\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Statistiken berechnen\n",
    "    df_results = pd.DataFrame({\"Actual\": actuals, \"Predicted\": predictions})\n",
    "    mse = ((df_results[\"Actual\"] - df_results[\"Predicted\"]) ** 2).mean()\n",
    "    mae = np.abs(df_results[\"Actual\"] - df_results[\"Predicted\"]).mean()\n",
    "    print(f\"Mean Squared Error (MSE): {mse:.2f}\")\n",
    "    print(f\"Mean Absolute Error (MAE): {mae:.2f}\")\n",
    "\n",
    "    return df_results\n",
    "\n",
    "# Anwendung der Backtesting-Funktion\n",
    "test_data = pd.read_pickle(\"../Data/test_dax_data.pkl\")  # Testdatensatz laden\n",
    "model_path = \"../Models/best_model.pt\"  # Pfad zum gespeicherten Modell\n",
    "seq_size = 30  # Sequenzgröße\n",
    "\n",
    "results = run_backtest(test_data, model_path, seq_size)\n",
    "results.to_csv(\"backtest_results.csv\", index=False)  # Ergebnisse speichern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
