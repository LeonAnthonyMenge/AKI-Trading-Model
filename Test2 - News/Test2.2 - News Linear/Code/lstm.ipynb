{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test 2.2 - News Linear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-01T10:24:56.257526Z",
     "start_time": "2024-11-01T10:24:56.223997Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from backtesting import Backtest\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import os\n",
    "from datetime import datetime\n",
    "from lumibot.brokers import Alpaca\n",
    "from lumibot.backtesting import YahooDataBacktesting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-01T10:17:07.914668Z",
     "start_time": "2024-11-01T10:17:07.913939Z"
    }
   },
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-01T10:17:07.916204Z",
     "start_time": "2024-11-01T10:17:07.914992Z"
    }
   },
   "outputs": [],
   "source": [
    "# Model parameter\n",
    "input_size = 7\n",
    "output_size = 1\n",
    "hidden_size = 2048\n",
    "num_layers = 20\n",
    "dropout = 0.3\n",
    "\n",
    "# Training parameter\n",
    "num_epochs = 0\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-11-01T10:17:07.916045Z"
    }
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_size, num_layers, dropout_rate=0.5):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self.layer_1 = nn.Linear(input_size, hidden_size)\n",
    "        self.dropout = nn.Dropout(p=dropout_rate)  \n",
    "\n",
    "        self.hidden_layers = nn.ModuleList([\n",
    "            nn.Linear(hidden_size, hidden_size) for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        self.output_layer = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.arctan(self.layer_1(x))\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        for layer in self.hidden_layers:\n",
    "            x = torch.arctan(layer(x))\n",
    "            x = self.dropout(x)\n",
    "        \n",
    "        x = self.output_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preperation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-01T10:17:07.918741Z",
     "start_time": "2024-11-01T10:17:07.916968Z"
    }
   },
   "outputs": [],
   "source": [
    "# Train Data\n",
    "df_train = pd.read_pickle(\"../Data/spy_train_data.pkl\")\n",
    "\n",
    "inputs = torch.tensor(df_train.iloc[:, :-1].values)\n",
    "labels = torch.tensor(df_train.iloc[:, -1].values)\n",
    "\n",
    "# Normalize the data\n",
    "# min_max_scaler = MinMaxScaler()\n",
    "\n",
    "# Fit the scaler on the training data and transform the training data\n",
    "# min_max_scaler.fit(inputs)\n",
    "# inputs_scaled = min_max_scaler.transform(inputs)\n",
    "inputs_scaled = torch.tensor(inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Data\n",
    "df_test = pd.read_pickle(\"../Data/spy_test_data.pkl\")\n",
    "\n",
    "df_test[\"news_probability\"] = df_test[\"news_probability\"].apply(lambda x: float(x.removeprefix(\"tensor(\").split(\",\")[0]))\n",
    "\n",
    "inputs_test = torch.tensor(df_test.iloc[:,:-1].values)\n",
    "labels_test = torch.tensor(df_test.iloc[:,-1].values)\n",
    "\n",
    "# Normalize the data\n",
    "# min_max_scaler = MinMaxScaler()\n",
    "\n",
    "# Fit the scaler on the training data and transform the training data\n",
    "# min_max_scaler.fit(inputs_test)\n",
    "# inputs_scaled_test = min_max_scaler.transform(inputs_test)\n",
    "inputs_scaled_test = torch.tensor(inputs_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-11-01T10:17:07.918078Z"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize model, loss function, optimizer\n",
    "net = Net(input_size, output_size, hidden_size, num_layers)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=learning_rate)\n",
    "\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-01T10:17:07.922014Z",
     "start_time": "2024-11-01T10:17:07.920173Z"
    }
   },
   "outputs": [],
   "source": [
    "os.makedirs(\"Models\", exist_ok=True)\n",
    "\n",
    "loss_vals = []\n",
    "test_loss_vals = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    net.train()  \n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    outputs = net(inputs_scaled.float())\n",
    "    outputs = outputs.squeeze(-1)  \n",
    "\n",
    "    loss = criterion(outputs, labels.float())\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    loss_vals.append(loss.item())\n",
    "\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        test_outputs = net(inputs_scaled_test.float())\n",
    "        test_outputs = test_outputs.squeeze(-1) \n",
    "        test_loss = criterion(test_outputs, labels_test.float())\n",
    "        test_loss_vals.append(test_loss.item())\n",
    "    \n",
    "    # Save model after each epoch\n",
    "    model_path = f'Models/model-{epoch + 1}.pt'\n",
    "    torch.save(net.state_dict(), model_path)\n",
    "    \n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        learning_rate *= 0.5\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = learning_rate\n",
    "    \n",
    "    if epoch % 5 == 0:\n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}, Test Loss: {test_loss.item():.4f}')\n",
    "\n",
    "net.eval()\n",
    "with torch.no_grad():\n",
    "    outputs_test = net(inputs_scaled_test.float()).squeeze(-1)\n",
    "    loss_test = criterion(outputs_test, labels_test.float())\n",
    "    print(f'Final Test Loss: {loss_test.item():.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plotting the loss values\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(len(loss_vals)), loss_vals, label='Training Loss', color='red')\n",
    "plt.plot(range(len(test_loss_vals)), test_loss_vals, label='Test Loss', color='blue')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Test Loss over Epochs')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Printing the final loss values\n",
    "print(f'Final Training Loss: {loss_vals[-1]:.4f}')\n",
    "print(f'Final Test Loss: {loss_test.item():.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backtesting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-01T10:17:07.928733Z",
     "start_time": "2024-11-01T10:17:07.924953Z"
    }
   },
   "outputs": [],
   "source": [
    "os.makedirs(\"logs\", exist_ok=True)\n",
    "os.makedirs(\"results\", exist_ok=True)\n",
    "\n",
    "models = [f for f in os.listdir(\"Models/\")]\n",
    "alredy_done = [f.removeprefix(\"backtest-\").removesuffix(\".csv.gz\") for f in os.listdir(\"results/\")]\n",
    "print(len(models))\n",
    "print(len(alredy_done))\n",
    "\n",
    "test_data = pd.read_pickle(\"../../Data/spy_test_data.pkl\")\n",
    "\n",
    "for model_name in models:\n",
    "    # Skip Backtesting for Models which already did the Backtesting\n",
    "    if model_name in alredy_done:\n",
    "        print(f\"Skip model: {model_name}\")\n",
    "        continue\n",
    "\n",
    "    model_path = f\"Models/{model_name}\"\n",
    "    model = Net(input_size, output_size, hidden_size, num_layers)\n",
    "\n",
    "    # Load state_dict only\n",
    "    model.load_state_dict(torch.load(model_path, weights_only=False))\n",
    "    model.eval()\n",
    "\n",
    "    ALPACA_CREDS = {\n",
    "        \"API_KEY\": os.getenv(\"ALPACA_API_KEY\"), \n",
    "        \"API_SECRET\": os.getenv(\"ALPACA_API_SECRET\"), \n",
    "        \"PAPER\": True\n",
    "    }\n",
    "\n",
    "    # Strategy setup\n",
    "    start_date = datetime(2023, 1, 1)\n",
    "    end_date = datetime(2023, 12, 31)\n",
    "    broker = Alpaca(ALPACA_CREDS)\n",
    "\n",
    "    # Instantiate and run the strategy\n",
    "    strategy = Backtest(\n",
    "        name=model_name,  \n",
    "        broker=broker,\n",
    "        parameters={\n",
    "            \"symbol\": \"SPY\",\n",
    "            \"cash_at_risk\": 0.5,\n",
    "            \"model\": model,\n",
    "            \"num_prior_days\": 1,\n",
    "            \"dataset\": test_data,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Run the backtest\n",
    "    backtest_results = strategy.backtest(\n",
    "        YahooDataBacktesting,\n",
    "        start_date,\n",
    "        end_date,\n",
    "        name=model_name,\n",
    "        parameters={\n",
    "            \"symbol\": \"SPY\", \n",
    "            \"cash_at_risk\": 0.5, \n",
    "            \"model\": model,  \n",
    "            \"dataset\": test_data,\n",
    "        },\n",
    "        benchmark_asset=\"SPY\",\n",
    "        show_plot=True,\n",
    "        show_tearsheet=True\n",
    "    )\n",
    "\n",
    "    # Convert to DataFrame only if results are non-empty\n",
    "    backtest_results = pd.DataFrame(backtest_results)   \n",
    "    backtest_results[\"model\"] = model_name\n",
    "    \n",
    "    backtest_results.to_csv(f\"results/backtest-{model_name}.csv.gz\", index=False, compression='gzip')\n",
    "    \n",
    "print(\"Backtesting complete. Results saved to backtest_results.csv.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = [pd.read_csv(f\"results/{f}\", compression=\"gzip\") for f in os.listdir(\"results/\")]\n",
    "df = pd.concat(dfs).sort_values(by=[\"model\"])\n",
    "#df = pd.read_csv(\"results.csv\")\n",
    "display(df.sort_values(by=[\"model\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "best_model = df[df[\"total_return\"] == df[\"total_return\"].max()]\n",
    "best_model.reset_index(inplace=True)\n",
    "\n",
    "model_name = best_model.at[0, \"model\"]\n",
    "print(f\"Best model: {model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-11-01T10:17:07.921380Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "\n",
    "model_path = f\"Models/{model_name}\"\n",
    "model = Net(input_size, output_size, hidden_size, num_layers)\n",
    "\n",
    "# Load state_dict only\n",
    "model.load_state_dict(torch.load(model_path))  # Do not use weights_only\n",
    "model.eval()\n",
    "\n",
    "# Load your DataFrame\n",
    "data = pd.read_pickle('../../Data/spy_train_data.pkl')\n",
    "\n",
    "inputs = torch.tensor(data.iloc[:, :-1].values, dtype=torch.float32)\n",
    "labels = torch.tensor(data.iloc[:, -1].values, dtype=torch.float32)\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "\n",
    "\n",
    "out = model(inputs) \n",
    "# Store predictions and labels\n",
    "all_predictions.append(out.detach().numpy())  \n",
    "all_labels.append(labels.numpy())\n",
    "\n",
    "# Concatenate results\n",
    "all_predictions = np.concatenate(all_predictions)\n",
    "all_labels = np.concatenate(all_labels)\n",
    "\n",
    "# Print or analyze the predictions\n",
    "print(f'Predicted values: {all_predictions.flatten()}')\n",
    "print(f'Actual values: {all_labels.flatten()}')\n",
    "\n",
    "output_df = pd.DataFrame({'Predicted': all_predictions.flatten(), 'Actual': all_labels.flatten()})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-01T10:17:07.924515Z",
     "start_time": "2024-11-01T10:17:07.922162Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "threshold = 0.5\n",
    "predicted_classes = (all_predictions.flatten() > threshold).astype(int)\n",
    "print(predicted_classes)\n",
    "\n",
    "accuracy = accuracy_score(all_labels.flatten(), predicted_classes)\n",
    "precision = precision_score(all_labels.flatten(), predicted_classes)\n",
    "recall = recall_score(all_labels.flatten(), predicted_classes)\n",
    "f1 = f1_score(all_labels.flatten(), predicted_classes)\n",
    "\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "print(f'Precision: {precision:.2f}')\n",
    "print(f'Recall: {recall:.2f}')\n",
    "print(f'F1 Score: {f1:.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-11-01T10:17:07.922825Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cm = confusion_matrix(all_labels.flatten(), predicted_classes)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-11-01T10:17:07.923504Z"
    }
   },
   "outputs": [],
   "source": [
    "output_df = pd.DataFrame({\n",
    "    'Predicted': all_predictions.flatten(),\n",
    "    'Predicted_Class': predicted_classes,\n",
    "    'Actual': all_labels.flatten()\n",
    "})\n",
    "correlation_matrix = output_df.corr()\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()\n",
    "display(output_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
