{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-08T11:48:33.779584Z",
     "start_time": "2024-11-08T11:48:33.774220Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from backtesting import Backtest\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import os\n",
    "from datetime import datetime\n",
    "from lumibot.brokers import Alpaca\n",
    "from lumibot.backtesting import YahooDataBacktesting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-08T11:42:02.136656Z",
     "start_time": "2024-11-08T11:42:02.130517Z"
    }
   },
   "source": [
    "### Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-08T11:48:33.792141Z",
     "start_time": "2024-11-08T11:48:33.788124Z"
    }
   },
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-08T11:48:33.813759Z",
     "start_time": "2024-11-08T11:48:33.808822Z"
    }
   },
   "outputs": [],
   "source": [
    "# Model parameter\n",
    "input_size = 10\n",
    "output_size = 1\n",
    "hidden_size = 1000\n",
    "num_layers = 6\n",
    "dropout = 0.2\n",
    "\n",
    "# Training parameter\n",
    "batch_size = 16\n",
    "num_epochs = 50\n",
    "learning_rate = 0.001\n",
    "seq_size = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-08T11:48:33.835357Z",
     "start_time": "2024-11-08T11:48:33.829437Z"
    }
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, output_size, hidden_size, num_layers):\n",
    "        super(Net, self).__init__()\n",
    "        self.layer_1 = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "        self.layer_2 = nn.LSTM(hidden_size, hidden_size, batch_first=True, dropout=dropout, num_layers=num_layers)\n",
    "        self.output_layer = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, _ = self.layer_1(x)  \n",
    "        x = torch.relu(x)\n",
    "\n",
    "        x, _ = self.layer_2(x)\n",
    "        x = torch.relu(x)\n",
    "\n",
    "        x = self.output_layer(x[:, -1, :])  \n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-08T11:48:33.856451Z",
     "start_time": "2024-11-08T11:48:33.850337Z"
    }
   },
   "outputs": [],
   "source": [
    "class FinanceDataset(Dataset):\n",
    "    def __init__(self, data_path, seq_size):\n",
    "        self.data = pd.read_pickle(data_path)\n",
    "        self.seq_size = seq_size\n",
    "\n",
    "        self.data = self.data[['Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume', 'month', 'weekday', 'INTEREST_RATE', 'trend', 'Invest']]\n",
    "\n",
    "        # Entfernen der 'Date'-Spalte für Inputs\n",
    "        self.inputs = self.data.drop(columns=['Invest']).values\n",
    "        self.labels = self.data['Invest'].values\n",
    "\n",
    "        # Skalieren der Eingabedaten\n",
    "        scaler = MinMaxScaler()\n",
    "        self.inputs = scaler.fit_transform(self.inputs)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs) - self.seq_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Slice für Sequenzeingaben und das entsprechende Label\n",
    "        x = self.inputs[idx:idx + self.seq_size]\n",
    "        y = self.labels[idx + self.seq_size]\n",
    "\n",
    "        # In Tensoren umwandeln\n",
    "        x = torch.tensor(x, dtype=torch.float32)\n",
    "        y = torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "        return x, y\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-08T11:48:34.144385Z",
     "start_time": "2024-11-08T11:48:33.876849Z"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize model, loss function, optimizer\n",
    "net = Net(input_size, output_size, hidden_size, num_layers)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-08T11:48:34.263868Z",
     "start_time": "2024-11-08T11:48:34.159701Z"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize dataset and dataloader\n",
    "dataset = FinanceDataset('../Data/train_dax_data.pkl', seq_size=seq_size)\n",
    "train_loader = DataLoader(dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-08T11:48:34.267756800Z",
     "start_time": "2024-11-01T10:17:07.920173Z"
    }
   },
   "outputs": [],
   "source": [
    "os.makedirs('../Models', exist_ok = True)\n",
    "losses = []\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs.float())\n",
    "        loss = criterion(torch.squeeze(outputs), labels.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # Save model after each epoch\n",
    "    model_path = f'../Models/model-{epoch + 1}.pt'\n",
    "    torch.save(net.state_dict(), model_path)\n",
    "\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        learning_rate *= 0.8\n",
    "        optimizer = optim.SGD(net.parameters(), lr=learning_rate)\n",
    "\n",
    "    print(f'Epoch {epoch + 1}, Loss: {running_loss / len(train_loader)}')\n",
    "    losses.append(running_loss / len(train_loader))\n",
    "\n",
    "\n",
    "\n",
    "os.makedirs(\"../Models\", exist_ok=True)\n",
    "\n",
    "model_path = \"../Models/best_model.pt\"\n",
    "\n",
    "if os.path.exists(model_path):\n",
    "    net.load_state_dict(torch.load(model_path))\n",
    "    print(f\"Modell erfolgreich geladen von {model_path}\")\n",
    "\n",
    "\n",
    "# Parameter für Early Stopping und Modell-Speicherung\n",
    "patience = 5 \n",
    "best_test_loss = float('inf')\n",
    "early_stopping_counter = 0\n",
    "\n",
    "losses = []\n",
    "test_loss_vals = []\n",
    "learning_rate = 0.001  # Initiale Lernrate\n",
    "\n",
    "# Training loop mit Early Stopping\n",
    "for epoch in range(num_epochs):\n",
    "    net.train()\n",
    "    running_loss = 0\n",
    "\n",
    "    # Training\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)  # Auf das Gerät verschieben\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs.float())\n",
    "        loss = criterion(torch.squeeze(outputs), labels.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = running_loss / len(train_loader)\n",
    "    losses.append(avg_train_loss)\n",
    "\n",
    "    # Modell auswerten auf dem Testset\n",
    "    net.eval()\n",
    "    running_test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        # Iteriere über den Testloader\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)  # Auf das Gerät verschieben\n",
    "            test_outputs = net(inputs)\n",
    "            test_outputs = test_outputs.squeeze(-1)  # Entferne unnötige Dimension\n",
    "            test_loss = criterion(test_outputs, labels)\n",
    "            running_test_loss += test_loss.item()\n",
    "        \n",
    "    avg_test_loss = running_test_loss / len(test_loader)\n",
    "    test_loss_vals.append(avg_test_loss)\n",
    "\n",
    "    # Check for Early Stopping und Speichern des besten Modells\n",
    "    if avg_test_loss < best_test_loss:\n",
    "        best_test_loss = avg_test_loss\n",
    "        best_model_path = '../Models/best_model.pt'\n",
    "        torch.save(net.state_dict(), best_model_path)\n",
    "        early_stopping_counter = 0\n",
    "    else:\n",
    "        early_stopping_counter += 1\n",
    "\n",
    "    # Ausgabe der Verluste\n",
    "    print(f'Epoch {epoch + 1}, Train Loss: {avg_train_loss:.4f}, Test Loss: {avg_test_loss:.4f}')\n",
    "    \n",
    "    # Early Stopping-Kriterium prüfen\n",
    "    if early_stopping_counter >= patience:\n",
    "        print(f\"Early stopping after {epoch + 1} epochs. Test loss did not improve for {patience} epochs.\")\n",
    "        break\n",
    "\n",
    "    # Dynamische Lernratenanpassung alle 10 Epochen\n",
    "    if epoch > 0 and epoch % 10 == 0:\n",
    "        learning_rate *= 0.5\n",
    "        optimizer = optim.SGD(net.parameters(), lr=learning_rate)\n",
    "        print(f\"Reduced learning rate to {learning_rate:.5f}\")\n",
    "\n",
    "print(f\"Training abgeschlossen. Bestes Modell gespeichert unter: {best_model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backtesting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-08T11:48:34.267756800Z",
     "start_time": "2024-11-01T10:17:07.924953Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "scaler = MinMaxScaler()\n",
    "df = pd.read_pickle('../Data/train_dax_data.pkl').reset_index()\n",
    "df = df[['Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume', 'month', 'weekday', 'INTEREST_RATE', 'trend']]\n",
    "scaler.fit(df.values)\n",
    "\n",
    "alredy_done = [f.removeprefix(\"backtest-\").removesuffix(\".csv.gz\") for f in os.listdir(\"results/\")]\n",
    "\n",
    "test_data = pd.read_pickle(\"../Data/test_dax_data.pkl\")\n",
    "models = [f for f in os.listdir(\"../Models/\") if os.path.isfile(os.path.join(\"../Models/\", f))]\n",
    "results = []\n",
    "\n",
    "os.makedirs(\"logs\", exist_ok=True)\n",
    "os.makedirs(\"results\", exist_ok=True)\n",
    "\n",
    "for model_name in models:\n",
    "\n",
    "    model_path = f\"../Models/{model_name}\"\n",
    "    model = Net(input_size, output_size, hidden_size, num_layers)\n",
    "\n",
    "    # Load state_dict only\n",
    "    model.load_state_dict(torch.load(model_path, weights_only=False))\n",
    "    model.eval()\n",
    "\n",
    "    ALPACA_CREDS = {\n",
    "        \"API_KEY\": os.getenv(\"ALPACA_API_KEY\"), \n",
    "        \"API_SECRET\": os.getenv(\"ALPACA_API_SECRET\"), \n",
    "        \"PAPER\": True\n",
    "    }\n",
    "\n",
    "    # Strategy setup\n",
    "    start_date = datetime(2023, 1, 1)\n",
    "    end_date = datetime(2023, 12, 31)\n",
    "    broker = Alpaca(ALPACA_CREDS)\n",
    "\n",
    "    # Instantiate and run the strategy\n",
    "    strategy = Backtest(\n",
    "        name=model_name,  \n",
    "        broker=broker,\n",
    "        parameters={\n",
    "            \"symbol\": \"^GDAXI\",\n",
    "            \"cash_at_risk\": 0.5,\n",
    "            \"model\": model,\n",
    "            \"num_prior_days\": 30,\n",
    "            \"dataset\": test_data,\n",
    "            \"scaler\": scaler,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Run the backtest\n",
    "    backtest_results = strategy.backtest(\n",
    "        YahooDataBacktesting,\n",
    "        start_date,\n",
    "        end_date,\n",
    "        parameters={\"symbol\": \"^GDAXI\", \"cash_at_risk\": 0.5, \"model\": model, \"dataset\": test_data, \"scaler\": scaler},\n",
    "        benchmark_asset=\"^GDAXI\",\n",
    "        plot_file_html=f\"results/plot-{str(model_name)}.html\",\n",
    "        tearsheet_file=f\"results/tearsheet-{model_name}.html\",\n",
    "        trades_file=f\"results/plot-{str(model_name)}\",\n",
    "        settings_file=f\"results/plot-{str(model_name)}\",\n",
    "        indicators_file=f\"results/plot-{str(model_name)}\",\n",
    "        show_plot=True,\n",
    "        show_tearsheet=True\n",
    "    )\n",
    "    backtest_results = pd.DataFrame(backtest_results)\n",
    "    backtest_results[\"model\"] = model_name\n",
    "    results.append(backtest_results)\n",
    "\n",
    "# Save the results to a CSV file\n",
    "pd.concat(results, ignore_index=True).to_csv(\"results.csv\", index=False)\n",
    "\n",
    "print(\"Backtesting complete. Results saved to backtest_results.csv.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"results.csv\")\n",
    "display(df.sort_values(by=[\"model\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "best_model = df[df[\"total_return\"] == df[\"total_return\"].max()]\n",
    "best_model.reset_index(inplace=True)\n",
    "\n",
    "model_name = best_model.at[0, \"model\"]\n",
    "print(f\"Best model: {model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-08T11:48:34.267756800Z",
     "start_time": "2024-11-01T10:17:07.921380Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "\n",
    "model_path = f\"../Models/{model_name}\"\n",
    "model = Net(input_size, output_size, hidden_size, num_layers)\n",
    "\n",
    "# Load state_dict only\n",
    "model.load_state_dict(torch.load(model_path))  # Do not use weights_only\n",
    "model.eval()\n",
    "\n",
    "# Load your DataFrame\n",
    "data = pd.read_pickle('../Data/test_dax_data.pkl')\n",
    "\n",
    "test_data = FinanceDataset('../Data/test_dax_data.pkl', seq_size=seq_size)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size)\n",
    "# Select all rows and columns from index 1 to -1 (exclusive)\n",
    "\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        out = model(inputs) \n",
    "        # Store predictions and labels\n",
    "        all_predictions.append(out.numpy())  \n",
    "        all_labels.append(labels.numpy())\n",
    "\n",
    "# Concatenate results\n",
    "all_predictions = np.concatenate(all_predictions)\n",
    "all_labels = np.concatenate(all_labels)\n",
    "\n",
    "# Print or analyze the predictions\n",
    "print(f'Predicted values: {all_predictions.flatten()}')\n",
    "print(f'Actual values: {all_labels.flatten()}')\n",
    "\n",
    "output_df = pd.DataFrame({'Predicted': all_predictions.flatten(), 'Actual': all_labels.flatten()})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-08T11:48:34.267756800Z",
     "start_time": "2024-11-01T10:17:07.922162Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "threshold = 0.5\n",
    "predicted_classes = (all_predictions.flatten() > threshold).astype(int)\n",
    "\n",
    "accuracy = accuracy_score(all_labels.flatten(), predicted_classes)\n",
    "precision = precision_score(all_labels.flatten(), predicted_classes)\n",
    "recall = recall_score(all_labels.flatten(), predicted_classes)\n",
    "f1 = f1_score(all_labels.flatten(), predicted_classes)\n",
    "\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "print(f'Precision: {precision:.2f}')\n",
    "print(f'Recall: {recall:.2f}')\n",
    "print(f'F1 Score: {f1:.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-08T11:48:34.267756800Z",
     "start_time": "2024-11-01T10:17:07.922825Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cm = confusion_matrix(all_labels.flatten(), predicted_classes)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-08T11:48:34.267756800Z",
     "start_time": "2024-11-01T10:17:07.923504Z"
    }
   },
   "outputs": [],
   "source": [
    "output_df = pd.DataFrame({\n",
    "    'Predicted': all_predictions.flatten(),\n",
    "    'Predicted_Class': predicted_classes,\n",
    "    'Actual': all_labels.flatten()\n",
    "})\n",
    "correlation_matrix = output_df.corr()\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()\n",
    "display(output_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
