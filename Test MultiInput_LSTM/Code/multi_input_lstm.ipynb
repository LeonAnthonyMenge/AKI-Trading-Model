{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from lumibot.backtesting import YahooDataBacktesting\n",
    "from lumibot.brokers import Alpaca\n",
    "from datetime import datetime\n",
    "from backtesting import Backtest\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helpful Notes\n",
    "- https://www.geeksforgeeks.org/how-to-use-pytorchs-nnmultiheadattention/\n",
    "- https://github.com/keras-team/tf-keras/issues/139"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available:\n",
    "    device = \"cuda\"\n",
    "elif torch.mps.is_available:\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "device = \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Parameter\n",
    "input_size_news = 17\n",
    "input_size_f_and_g = 14\n",
    "input_size_commodities = 48\n",
    "output_zize = 1\n",
    "hidden_size = 1024\n",
    "num_layers = 1\n",
    "dropout = 0.1\n",
    "embed_dim = 2048\n",
    "num_heads = 512\n",
    "\n",
    "# Training Parameter\n",
    "batch_size = 1\n",
    "num_epochs = 500 # early stopping\n",
    "learning_rate = 0.000001\n",
    "seq_size = 30\n",
    "\n",
    "# Training Data Path\n",
    "train_data_path_commodities = \"../Data/commodities_train.pkl\"\n",
    "train_data_path_f_and_g = \"../Data/train_f_and_g.pkl\"\n",
    "train_data_path_news = \"../Data/train_news.pkl\"\n",
    "# Test Data Path\n",
    "test_data_path_commodities = \"../Data/commodities_test.pkl\"\n",
    "test_data_path_f_and_g = \"../Data/test_f_and_g.pkl\"\n",
    "test_data_path_news = \"../Data/test_news.pkl\"\n",
    "\n",
    "model_path = \"../Models/best_model.pt\"\n",
    "\n",
    "backtest_name = \"Multi Input inkl stop_loss, cash at risk 1, take_profit, careful short, careful buy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(\n",
    "            self, \n",
    "            input_size_commodities: int, \n",
    "            input_size_f_and_g: int, \n",
    "            input_size_news: int,\n",
    "            output_size: int = 1,\n",
    "            hidden_size: int = 10,\n",
    "            embed_dim: int = 3,\n",
    "            num_layers: int = 1,\n",
    "            dropout: float = 0.2,\n",
    "            num_heads: int = 4,\n",
    "        ):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self.lstm_commodities = nn.LSTM(input_size_commodities, hidden_size, num_layers, batch_first=True, dropout=dropout)\n",
    "        self.fc_commodities = nn.Linear(hidden_size, embed_dim)\n",
    "        \n",
    "        self.lstm_f_and_g = nn.LSTM(input_size_f_and_g, hidden_size, num_layers, batch_first=True, dropout=dropout)\n",
    "        self.fc_f_and_g = nn.Linear(hidden_size, embed_dim)\n",
    "        \n",
    "        self.lstm_news = nn.LSTM(input_size_news, hidden_size, num_layers, batch_first=True, dropout=dropout)\n",
    "        self.fc_news = nn.Linear(hidden_size, embed_dim)\n",
    "        \n",
    "        self.attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout)\n",
    "        \n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.fc = nn.Linear(embed_dim, output_size)\n",
    "        \n",
    "    def forward(self, x_commodities, x_f_and_g, x_news):\n",
    "        out_commodities, _ = self.lstm_commodities(x_commodities)\n",
    "        out_commodities = self.fc_commodities(out_commodities[:, -1, :]) \n",
    "        \n",
    "        out_f_and_g, _ = self.lstm_f_and_g(x_f_and_g)\n",
    "        out_f_and_g = self.fc_f_and_g(out_f_and_g[:, -1, :])  \n",
    "        \n",
    "        out_news, _ = self.lstm_news(x_news)\n",
    "        out_news = self.fc_news(out_news[:, -1, :])  \n",
    "        \n",
    "        combined = torch.cat([out_commodities.unsqueeze(1), \n",
    "                              out_f_and_g.unsqueeze(1), \n",
    "                              out_news.unsqueeze(1)], dim=1)\n",
    "\n",
    "        # Apply attention mechanism\n",
    "        attn_output, attn_weights = self.attn(combined, combined, combined)\n",
    "        \n",
    "        # Normalize and pass through the final output layer\n",
    "        attn_output = self.norm1(attn_output)\n",
    "        output = self.fc(attn_output.mean(dim=1))  \n",
    "\n",
    "        # For plotting during training: store attention weights (optional)\n",
    "        return output, attn_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FinanceDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_commodities,\n",
    "        input_f_and_g,\n",
    "        input_news,\n",
    "        labels,\n",
    "        seq_size,\n",
    "    ):\n",
    "        \n",
    "        self.seq_size = seq_size\n",
    "        self.input_commodities = input_commodities\n",
    "        self.input_f_and_g = input_f_and_g\n",
    "        self.input_news = input_news\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels) - self.seq_size\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        com = self.input_commodities[idx:idx + self.seq_size]\n",
    "        fg = self.input_f_and_g[idx:idx + self.seq_size]\n",
    "        news = self.input_news[idx:idx + self.seq_size]\n",
    "        y = self.labels[idx + self.seq_size]\n",
    "        \n",
    "        com = torch.tensor(com, dtype=torch.float32)\n",
    "        fg = torch.tensor(fg, dtype=torch.float32)\n",
    "        news = torch.tensor(news, dtype=torch.float32)\n",
    "        y = torch.tensor(y, dtype=torch.float32)\n",
    "        \n",
    "        return com, fg, news, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "com = pd.read_pickle(\"../Data/commodities_train.pkl\") \n",
    "com[\"Date\"] = pd.to_datetime(com[\"Date\"])\n",
    "\n",
    "fg = pd.read_pickle(\"../Data/train_f_and_g.pkl\")\n",
    "fg[\"Date\"] = pd.to_datetime(fg[\"Date\"])\n",
    "\n",
    "news = pd.read_pickle(\"../Data/train_news.pkl\")  \n",
    "news[\"Date\"] = pd.to_datetime(news[\"Date\"])\n",
    "\n",
    "# Find the common dates across all datasets\n",
    "common_dates = set(com[\"Date\"]).intersection(set(fg[\"Date\"])).intersection(set(news[\"Date\"]))\n",
    "\n",
    "# Filter each dataset to include only rows with the common dates\n",
    "com = com[com[\"Date\"].isin(common_dates)]\n",
    "fg = fg[fg[\"Date\"].isin(common_dates)]\n",
    "news = news[news[\"Date\"].isin(common_dates)]\n",
    "\n",
    "# Sort by Date\n",
    "com.sort_values([\"Date\"], inplace=True)\n",
    "fg.sort_values([\"Date\"], inplace=True)\n",
    "news.sort_values([\"Date\"], inplace=True)\n",
    "\n",
    "## Commodities and Y\n",
    "\n",
    "com_inputs = com.iloc[:, 1:-1]\n",
    "scaler_com = MinMaxScaler()\n",
    "scaler_y = MinMaxScaler()\n",
    "scaled_com_inputs = scaler_com.fit_transform(com_inputs.values)\n",
    "y = scaler_y.fit_transform(com.iloc[:, -1].values.reshape(-1, 1))\n",
    "\n",
    "## Fear and Greed\n",
    "\n",
    "fg_inputs = fg.iloc[:, 1:-1]  \n",
    "scaler_fg = MinMaxScaler()\n",
    "scaled_fg_inputs = scaler_fg.fit_transform(fg_inputs.values)\n",
    "\n",
    "## News\n",
    "\n",
    "news_inputs = news.iloc[:, 1:-1] \n",
    "scaler_news = MinMaxScaler()\n",
    "scaled_news_inputs = scaler_news.fit_transform(news_inputs.values)\n",
    "\n",
    "# Initialize dataset and dataloader\n",
    "dataset = FinanceDataset(\n",
    "    input_commodities=scaled_com_inputs,\n",
    "    input_f_and_g=scaled_fg_inputs,\n",
    "    input_news=scaled_news_inputs,\n",
    "    labels=y,\n",
    "    seq_size=seq_size,\n",
    ")\n",
    "train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "com_test = pd.read_pickle(\"../Data/commodities_test.pkl\") \n",
    "com_test[\"Date\"] = pd.to_datetime(com_test[\"Date\"])\n",
    "\n",
    "fg_test = pd.read_pickle(\"../Data/test_f_and_g.pkl\")\n",
    "fg_test[\"Date\"] = pd.to_datetime(fg_test[\"Date\"])\n",
    "\n",
    "news_test = pd.read_pickle(\"../Data/test_news.pkl\")  \n",
    "news_test[\"Date\"] = pd.to_datetime(news_test[\"Date\"])\n",
    "\n",
    "# Find the common dates across all datasets\n",
    "common_dates_test = set(com_test[\"Date\"]).intersection(set(fg_test[\"Date\"])).intersection(set(news_test[\"Date\"]))\n",
    "\n",
    "# Filter each dataset to include only rows with the common dates\n",
    "com_test = com_test[com_test[\"Date\"].isin(common_dates_test)]\n",
    "fg_test = fg_test[fg_test[\"Date\"].isin(common_dates_test)]\n",
    "news_test = news_test[news_test[\"Date\"].isin(common_dates_test)]\n",
    "\n",
    "# Sort by Date\n",
    "com_test.sort_values([\"Date\"], inplace=True)\n",
    "fg_test.sort_values([\"Date\"], inplace=True)\n",
    "news_test.sort_values([\"Date\"], inplace=True)\n",
    "\n",
    "## Commodities and Y\n",
    "\n",
    "com_inputs_test = com_test.iloc[:, 1:-1]  # Exclude \"Date\" and target column\n",
    "\n",
    "scaled_com_inputs_test = scaler_com.transform(com_inputs_test.values)\n",
    "y_test = scaler_y.transform(com_test.iloc[:, -1].values.reshape(-1, 1))\n",
    "\n",
    "## Fear and Greed\n",
    "\n",
    "fg_inputs_test = fg_test.iloc[:, 1:-1]  # Exclude \"Date\" and target column\n",
    "scaled_fg_inputs_test = scaler_fg.transform(fg_inputs_test.values)\n",
    "\n",
    "## News\n",
    "\n",
    "news_inputs_test = news_test.iloc[:, 1:-1]  # Exclude \"Date\" and target column\n",
    "scaled_news_inputs_test = scaler_news.transform(news_inputs_test.values)\n",
    "\n",
    "# Initialize dataset and dataloader\n",
    "dataset = FinanceDataset(\n",
    "    input_commodities=scaled_com_inputs_test,\n",
    "    input_f_and_g=scaled_fg_inputs_test,\n",
    "    input_news=scaled_news_inputs_test,\n",
    "    labels=y_test,\n",
    "    seq_size=seq_size,\n",
    ")\n",
    "test_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net(\n",
    "    input_size_commodities=input_size_commodities,\n",
    "    input_size_f_and_g=input_size_f_and_g,\n",
    "    input_size_news=input_size_news,\n",
    "    output_size=output_zize,\n",
    "    hidden_size=hidden_size,\n",
    "    embed_dim=embed_dim,\n",
    "    num_layers=num_layers,\n",
    "    dropout=dropout,\n",
    "    num_heads=num_heads,\n",
    ")\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n",
    "\n",
    "net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directory for saving models if it doesn't exist\n",
    "os.makedirs(\"../Models\", exist_ok=True)\n",
    "\n",
    "# Load model if it exists\n",
    "if os.path.exists(model_path):\n",
    "    net.load_state_dict(torch.load(model_path))\n",
    "    print(f\"Model successfully loaded from {model_path}\")\n",
    "\n",
    "# Early stopping and best model saving parameters\n",
    "patience = 8\n",
    "best_test_loss = float('inf')\n",
    "best_train_loss = float('inf')\n",
    "early_stopping_counter = 0\n",
    "\n",
    "# Loss tracking\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "# Attention weight tracking\n",
    "attention_tracker = {'commodities': [], 'fg': [], 'news': []}\n",
    "\n",
    "# Training loop with early stopping\n",
    "for epoch in range(num_epochs):\n",
    "    # Training phase\n",
    "    net.train()\n",
    "    epoch_train_loss = 0\n",
    "    epoch_attn_commodities = []\n",
    "    epoch_attn_fg = []\n",
    "    epoch_attn_news = []\n",
    "    \n",
    "    for com, fg, news, y in train_loader:\n",
    "        # Move data to the appropriate device\n",
    "        com, fg, news, y = com.to(device), fg.to(device), news.to(device), y.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        outputs, attn_weights = net(com, fg, news)\n",
    "        loss = criterion(outputs.squeeze(-1), y)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Update metrics\n",
    "        epoch_train_loss += loss.item()\n",
    "        epoch_attn_commodities.append(attn_weights[0].item())  \n",
    "        epoch_attn_fg.append(attn_weights[1].item())           \n",
    "        epoch_attn_news.append(attn_weights[2].item())         \n",
    "\n",
    "    avg_train_loss = epoch_train_loss / len(train_loader)\n",
    "    train_losses.append(avg_train_loss)\n",
    "    attention_tracker['commodities'].append(np.mean(epoch_attn_commodities))\n",
    "    attention_tracker['fg'].append(np.mean(epoch_attn_fg))\n",
    "    attention_tracker['news'].append(np.mean(epoch_attn_news))\n",
    "    \n",
    "    # Validation/Test phase\n",
    "    net.eval()\n",
    "    epoch_test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for com, fg, news, y in test_loader:\n",
    "            # Move data to the appropriate device\n",
    "            com, fg, news, y = com.to(device), fg.to(device), news.to(device), y.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            test_outputs, _ = net(com, fg, news)\n",
    "            test_loss = criterion(test_outputs.squeeze(-1), y)\n",
    "            \n",
    "            # Update test loss\n",
    "            epoch_test_loss += test_loss.item()\n",
    "    \n",
    "    avg_test_loss = epoch_test_loss / len(test_loader)\n",
    "    test_losses.append(avg_test_loss)\n",
    "    \n",
    "    # Save the best model\n",
    "    if avg_test_loss < best_test_loss:\n",
    "        best_test_loss = avg_test_loss\n",
    "        best_train_loss = avg_train_loss\n",
    "        best_epoch = epoch\n",
    "        torch.save(net.state_dict(), model_path)\n",
    "        print(f\"Best model saved at epoch {epoch + 1} with Test Loss: {best_test_loss:.4f}\")\n",
    "        early_stopping_counter = 0\n",
    "    else:\n",
    "        early_stopping_counter += 1\n",
    "    \n",
    "    # Print epoch results\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs} - Train Loss: {avg_train_loss:.4f}, Test Loss: {avg_test_loss:.4f}\")\n",
    "    \n",
    "    # Early stopping\n",
    "    if early_stopping_counter >= patience:\n",
    "        print(f\"Early stopping triggered after {epoch + 1} epochs. No improvement in Test Loss for {patience} consecutive epochs.\")\n",
    "        break\n",
    "\n",
    "# Plot Training and Test Loss\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_losses, label='Training Loss', marker='o')\n",
    "plt.plot(test_losses, label='Test Loss', marker='o')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Test Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Plot Attention Weights over Epochs\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(attention_tracker['commodities'], label='Commodities Attention', marker='o')\n",
    "plt.plot(attention_tracker['fg'], label='Fear & Greed Attention', marker='o')\n",
    "plt.plot(attention_tracker['news'], label='News Attention', marker='o')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Average Attention Weight')\n",
    "plt.title('Attention Weight Changes Across Epochs')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Final Summary\n",
    "print(f\"Training completed. Best model saved to {model_path}\")\n",
    "print(f\"Best Test Loss: {best_test_loss} at epoch {best_epoch + 1}\")\n",
    "print(f\"Best Train Loss: {best_train_loss} at epoch {best_epoch + 1}\")\n",
    "print(f\"Final Training Loss: {train_losses[-1]}, Final Test Loss: {test_losses[-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net(\n",
    "    input_size_commodities=input_size_commodities,\n",
    "    input_size_f_and_g=input_size_f_and_g,\n",
    "    input_size_news=input_size_news,\n",
    "    output_size=output_zize,\n",
    "    hidden_size=hidden_size,\n",
    "    embed_dim=embed_dim,\n",
    "    num_layers=num_layers,\n",
    "    dropout=dropout,\n",
    "    num_heads=num_heads,\n",
    ")\n",
    "\n",
    "# Load state_dict only\n",
    "model.load_state_dict(torch.load(model_path)) \n",
    "model.eval()\n",
    "\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for com, fg, news, y in test_loader:\n",
    "        out, _ = model(com, fg, news) \n",
    "        \n",
    "        all_predictions.append(out.numpy())  \n",
    "        all_labels.append(y.numpy())\n",
    "\n",
    "all_predictions = np.concatenate(all_predictions)\n",
    "all_labels = np.concatenate(all_labels)\n",
    "\n",
    "pred = scaler_y.inverse_transform(all_predictions.reshape(-1, 1)).reshape(-1, 1)\n",
    "actual = scaler_y.inverse_transform(all_labels.reshape(-1, 1)).reshape(-1, 1)\n",
    "\n",
    "# DataFrame erstellen\n",
    "df_results = pd.DataFrame({\n",
    "    \"pred\": pred.flatten(),\n",
    "    \"actual\": actual.flatten(),\n",
    "})\n",
    "\n",
    "# DataFrame anzeigen\n",
    "print(df_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Backtesting-Funktion\n",
    "def backtest_model(model, dataloader, scaler_y, seq_size):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for com, fg, news, y in test_loader:\n",
    "            output, _ = model(com, fg, news) \n",
    "            predictions.extend(output.numpy())\n",
    "            actuals.extend(y.numpy())\n",
    "\n",
    "    # Rücktransformation der Vorhersagen und tatsächlichen Werte\n",
    "    predictions = scaler_y.inverse_transform(np.array(predictions).reshape(-1, 1))\n",
    "    actuals = scaler_y.inverse_transform(np.array(actuals).reshape(-1, 1))\n",
    "\n",
    "    return predictions.flatten(), actuals.flatten()\n",
    "\n",
    "# Backtesting starten\n",
    "def run_backtest(model_path, seq_size):\n",
    "    # Modell laden\n",
    "    model = Net(\n",
    "        input_size_commodities=input_size_commodities,\n",
    "        input_size_f_and_g=input_size_f_and_g,\n",
    "        input_size_news=input_size_news,\n",
    "        output_size=output_zize,\n",
    "        hidden_size=hidden_size,\n",
    "        embed_dim=embed_dim,\n",
    "        num_layers=num_layers,\n",
    "        dropout=dropout,\n",
    "        num_heads=num_heads,\n",
    "    )\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.eval()\n",
    "\n",
    "    # Backtesting durchführen\n",
    "    predictions, actuals = backtest_model(model, test_loader, scaler_y, seq_size)\n",
    "\n",
    "    # Ergebnisse visualisieren\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    plt.plot(predictions, label=\"Predicted\", color=\"blue\")\n",
    "    plt.plot(actuals, label=\"Actual\", color=\"orange\")\n",
    "    plt.title(\"Backtesting Results\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Statistiken berechnen\n",
    "    df_results = pd.DataFrame({\"Actual\": actuals, \"Predicted\": predictions})\n",
    "    mse = ((df_results[\"Actual\"] - df_results[\"Predicted\"]) ** 2).mean()\n",
    "    mae = np.abs(df_results[\"Actual\"] - df_results[\"Predicted\"]).mean()\n",
    "    print(f\"Mean Squared Error (MSE): {mse:.2f}\")\n",
    "    print(f\"Mean Absolute Error (MAE): {mae:.2f}\")\n",
    "\n",
    "    return df_results\n",
    "\n",
    "test_loader = DataLoader(dataset, batch_size=batch_size)\n",
    "results = run_backtest(model_path, seq_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"logs\", exist_ok=True)\n",
    "os.makedirs(\"results\", exist_ok=True)\n",
    "\n",
    "# Training\n",
    "com = pd.read_pickle(train_data_path_commodities) \n",
    "com[\"Date\"] = pd.to_datetime(com[\"Date\"])\n",
    "\n",
    "fg = pd.read_pickle(train_data_path_f_and_g)\n",
    "fg[\"Date\"] = pd.to_datetime(fg[\"Date\"])\n",
    "\n",
    "news = pd.read_pickle(train_data_path_news)  \n",
    "news[\"Date\"] = pd.to_datetime(news[\"Date\"])\n",
    "\n",
    "# Find the common dates across all datasets\n",
    "common_dates = set(com[\"Date\"]).intersection(set(fg[\"Date\"])).intersection(set(news[\"Date\"]))\n",
    "\n",
    "# Filter each dataset to include only rows with the common dates\n",
    "com = com[com[\"Date\"].isin(common_dates)]\n",
    "fg = fg[fg[\"Date\"].isin(common_dates)]\n",
    "news = news[news[\"Date\"].isin(common_dates)]\n",
    "\n",
    "scaler_com = MinMaxScaler()\n",
    "scaler_com.fit(com.iloc[:, 1:-1].values)\n",
    "scaler_fg = MinMaxScaler()\n",
    "scaler_fg.fit(fg.iloc[:, 1:-1].values)\n",
    "scaler_news = MinMaxScaler()\n",
    "scaler_news.fit(news.iloc[:, 1:-1].values)\n",
    "scaler_y = MinMaxScaler()\n",
    "scaler_y.fit(com.iloc[:, -1].values.reshape(-1, 1))\n",
    "\n",
    "test_data_commodities = pd.read_pickle(test_data_path_commodities)\n",
    "test_data_fg = pd.read_pickle(test_data_path_f_and_g)\n",
    "test_data_news = pd.read_pickle(test_data_path_news)\n",
    "\n",
    "model = Net(\n",
    "    input_size_commodities=input_size_commodities,\n",
    "    input_size_f_and_g=input_size_f_and_g,\n",
    "    input_size_news=input_size_news,\n",
    "    output_size=output_zize,\n",
    "    hidden_size=hidden_size,\n",
    "    embed_dim=embed_dim,\n",
    "    num_layers=num_layers,\n",
    "    dropout=dropout,\n",
    "    num_heads=num_heads,\n",
    ")\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "model.eval()\n",
    "\n",
    "ALPACA_CREDS = {\n",
    "    \"API_KEY\": os.getenv(\"ALPACA_API_KEY\"),\n",
    "    \"API_SECRET\": os.getenv(\"ALPACA_API_SECRET\"),\n",
    "    \"PAPER\": True,\n",
    "}\n",
    "\n",
    "# Strategy setup\n",
    "start_date = datetime(2023, 1, 1)\n",
    "end_date = datetime(2023, 12, 31)\n",
    "broker = Alpaca(ALPACA_CREDS)\n",
    "\n",
    "strategy = Backtest(\n",
    "    name=backtest_name,\n",
    "    broker=broker,\n",
    "    parameters={\n",
    "        \"symbol\": \"spy\",\n",
    "        \"cash_at_risk\": 0.8,\n",
    "        \"model\": model,\n",
    "        \"num_prior_days\": 30,\n",
    "        \"datase_com\": test_data_commodities,\n",
    "        \"datase_fg\": test_data_fg,\n",
    "        \"datase_news\": test_data_news,\n",
    "        \"scaler_com\": scaler_com,\n",
    "        \"scaler_fg\": scaler_fg,\n",
    "        \"scaler_news\": scaler_news,\n",
    "        \"scaler_y\": scaler_y,\n",
    "    },\n",
    ")\n",
    "\n",
    "# Run backtest\n",
    "backtest_results = strategy.backtest(\n",
    "    YahooDataBacktesting,\n",
    "    start_date,\n",
    "    end_date,\n",
    "    name=backtest_name,\n",
    "    parameters={\n",
    "        \"symbol\": \"spy\",\n",
    "        \"cash_at_risk\": 1,\n",
    "        \"model\": model,\n",
    "        \"num_prior_days\": 30,\n",
    "        \"dataset_com\": test_data_commodities,\n",
    "        \"dataset_fg\": test_data_fg,\n",
    "        \"dataset_news\": test_data_news,\n",
    "        \"scaler_com\": scaler_com,\n",
    "        \"scaler_fg\": scaler_fg,\n",
    "        \"scaler_news\": scaler_news,\n",
    "        \"scaler_y\": scaler_y,\n",
    "    },\n",
    "    benchmark_asset=\"SPY\",\n",
    "    show_plot=True,\n",
    "    show_tearsheet=True,\n",
    ")\n",
    "\n",
    "# Save results\n",
    "pd.DataFrame(backtest_results).to_csv(\"results/backtest_results.csv.gz\", index=False, compression=\"gzip\")\n",
    "\n",
    "print(\"Backtesting complete. Results saved to backtest_results.csv.gz.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
