{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-01T10:24:56.257526Z",
     "start_time": "2024-11-01T10:24:56.223997Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from backtesting import Backtest\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import os\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-01T10:17:07.914668Z",
     "start_time": "2024-11-01T10:17:07.913939Z"
    }
   },
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-01T10:17:07.916204Z",
     "start_time": "2024-11-01T10:17:07.914992Z"
    }
   },
   "outputs": [],
   "source": [
    "# Model parameter\n",
    "input_size = 9\n",
    "output_size = 1\n",
    "hidden_size = 10\n",
    "num_layers = 2\n",
    "dropout = 0.2\n",
    "\n",
    "# Training parameter\n",
    "batch_size = 16\n",
    "num_epochs = 1\n",
    "learning_rate = 0.001\n",
    "seq_size = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-11-01T10:17:07.916045Z"
    }
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, output_size, hidden_size, num_layers):\n",
    "        super(Net, self).__init__()\n",
    "        self.layer_1 = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "        self.layer_2 = nn.LSTM(hidden_size, hidden_size, batch_first=True, dropout=dropout, num_layers=num_layers)\n",
    "        self.output_layer = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, _ = self.layer_1(x)  \n",
    "        x = torch.relu(x)\n",
    "\n",
    "        x, _ = self.layer_2(x)\n",
    "        x = torch.relu(x)\n",
    "\n",
    "        x = self.output_layer(x[:, -1, :])  \n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-01T10:17:07.918741Z",
     "start_time": "2024-11-01T10:17:07.916968Z"
    }
   },
   "outputs": [],
   "source": [
    "class FinanceDataset(Dataset):\n",
    "    def __init__(self, data_path, seq_size):\n",
    "        self.data = pd.read_pickle(data_path)\n",
    "        self.seq_size = seq_size\n",
    "        \n",
    "        self.inputs = self.data.iloc[:, 1:-1].values\n",
    "        self.labels = self.data.iloc[:, -1].values\n",
    "        \n",
    "        scaler = MinMaxScaler()\n",
    "        self.inputs = scaler.fit_transform(self.inputs)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs) - self.seq_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.inputs[idx:idx + self.seq_size]\n",
    "        y = self.labels[idx + self.seq_size] \n",
    "\n",
    "        # Convert to tensors\n",
    "        x = torch.tensor(x, dtype=torch.float32)\n",
    "        y = torch.tensor(y, dtype=torch.float32)\n",
    "        \n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-11-01T10:17:07.918078Z"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize model, loss function, optimizer\n",
    "net = Net(input_size, output_size, hidden_size, num_layers)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-01T10:17:07.919235Z",
     "start_time": "2024-11-01T10:17:07.919001Z"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize dataset and dataloader\n",
    "dataset = FinanceDataset('../Data/train_dax_data.pkl', seq_size=seq_size)\n",
    "train_loader = DataLoader(dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-01T10:17:07.922014Z",
     "start_time": "2024-11-01T10:17:07.920173Z"
    }
   },
   "outputs": [],
   "source": [
    "losses = []\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0\n",
    "    for inputs, labels in train_loader:\n",
    "        print(inputs.shape)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs.float())\n",
    "        loss = criterion(torch.squeeze(outputs), labels.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # Save model after each epoch\n",
    "    model_path = f'../Models/model-{epoch + 1}.pt'\n",
    "    torch.save(net.state_dict(), model_path)\n",
    "\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        learning_rate *= 0.8\n",
    "        optimizer = optim.SGD(net.parameters(), lr=learning_rate)\n",
    "\n",
    "    print(f'Epoch {epoch + 1}, Loss: {running_loss / len(train_loader)}')\n",
    "    losses.append(running_loss / len(train_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backtesting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-11-01T10:17:07.921380Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "\n",
    "net.eval()\n",
    "\n",
    "# Load your DataFrame\n",
    "data = pd.read_pickle('../Data/test_dax_data.pkl')\n",
    "\n",
    "test_data = FinanceDataset('../Data/test_dax_data.pkl', seq_size=seq_size)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size)\n",
    "# Select all rows and columns from index 1 to -1 (exclusive)\n",
    "\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        # Forward pass through the network\n",
    "        out = net(inputs)  # The model expects (batch_size, sequence_length, num_features)\n",
    "        # Store predictions and labels\n",
    "        all_predictions.append(out.numpy())  # Convert to numpy for easier handling\n",
    "        all_labels.append(labels.numpy())\n",
    "\n",
    "# Concatenate results\n",
    "all_predictions = np.concatenate(all_predictions)\n",
    "all_labels = np.concatenate(all_labels)\n",
    "\n",
    "# Print or analyze the predictions\n",
    "print(f'Predicted values: {all_predictions.flatten()}')\n",
    "print(f'Actual values: {all_labels.flatten()}')\n",
    "\n",
    "# Optionally, save predictions to a CSV file\n",
    "output_df = pd.DataFrame({'Predicted': all_predictions.flatten(), 'Actual': all_labels.flatten()})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-01T10:17:07.924515Z",
     "start_time": "2024-11-01T10:17:07.922162Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "threshold = 0.5\n",
    "predicted_classes = (all_predictions.flatten() > threshold).astype(int)\n",
    "\n",
    "accuracy = accuracy_score(all_labels.flatten(), predicted_classes)\n",
    "precision = precision_score(all_labels.flatten(), predicted_classes)\n",
    "recall = recall_score(all_labels.flatten(), predicted_classes)\n",
    "f1 = f1_score(all_labels.flatten(), predicted_classes)\n",
    "\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "print(f'Precision: {precision:.2f}')\n",
    "print(f'Recall: {recall:.2f}')\n",
    "print(f'F1 Score: {f1:.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-11-01T10:17:07.922825Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cm = confusion_matrix(all_labels.flatten(), predicted_classes)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-11-01T10:17:07.923504Z"
    }
   },
   "outputs": [],
   "source": [
    "output_df = pd.DataFrame({\n",
    "    'Predicted': all_predictions.flatten(),\n",
    "    'Predicted_Class': predicted_classes,\n",
    "    'Actual': all_labels.flatten()\n",
    "})\n",
    "correlation_matrix = output_df.corr()\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()\n",
    "display(output_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-01T10:17:07.928733Z",
     "start_time": "2024-11-01T10:17:07.924953Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "models = [f for f in os.listdir(\"../Models/\") if os.path.isfile(os.path.join(\"../Models/\", f))]\n",
    "results = []\n",
    "\n",
    "\n",
    "for model_name in models:\n",
    "\n",
    "    model_path = f\"../Models/{model_name}\"\n",
    "    model = Net(input_size, output_size, hidden_size, num_layers)\n",
    "\n",
    "    # Load state_dict only\n",
    "    model.load_state_dict(torch.load(model_path))  # Do not use weights_only\n",
    "    model.eval()\n",
    "\n",
    "    ALPACA_CREDS = {\n",
    "        \"API_KEY\": os.getenv(\"ALPACA_API_KEY\"), \n",
    "        \"API_SECRET\": os.getenv(\"ALPACA_API_SECRET\"), \n",
    "        \"PAPER\": True\n",
    "    }\n",
    "\n",
    "    # Strategy setup\n",
    "    start_date = datetime(2023, 11, 1)\n",
    "    end_date = datetime(2023, 12, 31)\n",
    "    broker = Alpaca(ALPACA_CREDS)\n",
    "\n",
    "    # Instantiate and run the strategy\n",
    "    strategy = Backtest(\n",
    "        name=model_name,  \n",
    "        broker=broker,\n",
    "        parameters={\n",
    "            \"symbol\": \"DAX\",\n",
    "            \"cash_at_risk\": 0.5,\n",
    "            \"model\": model,\n",
    "            \"num_prior_days\": 5\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Run the backtest\n",
    "    backtest_results = strategy.backtest(\n",
    "        YahooDataBacktesting,\n",
    "        start_date,\n",
    "        end_date,\n",
    "        parameters={\"symbol\": \"DAX\", \"cash_at_risk\": 0.5, \"model\": model}\n",
    "    )\n",
    "    backtest_results = pd.DataFrame(backtest_results)\n",
    "    backtest_results[\"model\"] = model_name\n",
    "    results.append(backtest_results)\n",
    "\n",
    "# Save the results to a CSV file\n",
    "\n",
    "pd.concat(results, ignore_index=True).to_csv(\"results.csv\", index=False)\n",
    "\n",
    "print(\"Backtesting complete. Results saved to backtest_results.csv.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(pd.read_csv(\"results.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
